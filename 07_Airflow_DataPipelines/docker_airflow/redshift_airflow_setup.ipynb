{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redshift Airflow Setup\n",
    "\n",
    "This code:\n",
    "1. Loads AWS credentials\n",
    "2. Creates Redshift serverless instance and retrieves connection details\n",
    "3. Configures Airflow to have connections for AWS + for this Redshift instance\n",
    "\n",
    "This is in support of `redshift_dag1.py` which then uses this setup to run Airflow jobs with Redshift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Module Import\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Loading Config Files + Credentials\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/rambino/dev/DataEngineering_Udacity/04_AWS_DataWarehousing/redshift_credentials.cfg']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#AWS Credentials\n",
    "aws_path = \"/home/rambino/.aws/credentials\"\n",
    "aws_cred = configparser.ConfigParser()\n",
    "aws_cred.read(aws_path)\n",
    "\n",
    "#Redshift Credentials\n",
    "redshift_path = \"/home/rambino/dev/DataEngineering_Udacity/04_AWS_DataWarehousing/redshift_credentials.cfg\"\n",
    "redshift_cred = configparser.ConfigParser()\n",
    "redshift_cred.read(redshift_path)\n",
    "\n",
    "# #ETL Config\n",
    "# cfg_path = \"/home/rambino/dev/DataEngineering_Udacity/Projects/DataWarehouseWithRedshift/dwh.cfg\"\n",
    "# cfg = configparser.ConfigParser()\n",
    "# cfg.read(cfg_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Creating IAM role for Redshift\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam = boto3.client('iam',\n",
    "    region_name             = \"us-west-2\",\n",
    "    aws_access_key_id       = aws_cred['kevin_aws_account']['aws_access_key_id'],\n",
    "    aws_secret_access_key   = aws_cred['kevin_aws_account']['aws_secret_access_key']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "EntityAlreadyExistsException",
     "evalue": "An error occurred (EntityAlreadyExists) when calling the CreateRole operation: Role with name RedShift_Impersonation already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEntityAlreadyExistsException\u001b[0m              Traceback (most recent call last)",
      "\u001b[1;32m/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb Cell 8\u001b[0m line \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#Create IAM role:\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m#This policy is something about allowing Redshift to impersonate a user, but I don't fully understand it yet.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m#Look more into what \"sts:AssumeRole\" really means.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m dwhRole \u001b[39m=\u001b[39m iam\u001b[39m.\u001b[39;49mcreate_role(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     Path \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     RoleName \u001b[39m=\u001b[39;49m  \u001b[39m\"\u001b[39;49m\u001b[39mRedShift_Impersonation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X10sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     Description \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mAllows redshift to access S3\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X10sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     AssumeRolePolicyDocument\u001b[39m=\u001b[39;49mjson\u001b[39m.\u001b[39;49mdumps(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         {\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X10sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mVersion\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39m2012-10-17\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X10sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mStatement\u001b[39;49m\u001b[39m\"\u001b[39;49m: [\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X10sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m                 {\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X10sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m                     \u001b[39m\"\u001b[39;49m\u001b[39mEffect\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mAllow\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X10sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m                     \u001b[39m\"\u001b[39;49m\u001b[39mAction\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m'\u001b[39;49m\u001b[39msts:AssumeRole\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X10sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m                     \u001b[39m\"\u001b[39;49m\u001b[39mPrincipal\u001b[39;49m\u001b[39m\"\u001b[39;49m:{\u001b[39m\"\u001b[39;49m\u001b[39mService\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mredshift.amazonaws.com\u001b[39;49m\u001b[39m\"\u001b[39;49m}\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X10sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m                 }\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X10sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m             ]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X10sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m         }\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X10sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X10sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X10sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m dwhRole\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/botocore/client.py:508\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    505\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpy_operation_name\u001b[39m}\u001b[39;00m\u001b[39m() only accepts keyword arguments.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    506\u001b[0m     )\n\u001b[1;32m    507\u001b[0m \u001b[39m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 508\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_api_call(operation_name, kwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/botocore/client.py:915\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    913\u001b[0m     error_code \u001b[39m=\u001b[39m parsed_response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mError\u001b[39m\u001b[39m\"\u001b[39m, {})\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mCode\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    914\u001b[0m     error_class \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 915\u001b[0m     \u001b[39mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    916\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     \u001b[39mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mEntityAlreadyExistsException\u001b[0m: An error occurred (EntityAlreadyExists) when calling the CreateRole operation: Role with name RedShift_Impersonation already exists."
     ]
    }
   ],
   "source": [
    "#Create IAM role:\n",
    "\n",
    "#This policy is something about allowing Redshift to impersonate a user, but I don't fully understand it yet.\n",
    "#Look more into what \"sts:AssumeRole\" really means.\n",
    "\n",
    "import json\n",
    "\n",
    "dwhRole = iam.create_role(\n",
    "    Path = \"/\",\n",
    "    RoleName =  \"RedShift_Impersonation\",\n",
    "    Description = \"Allows redshift to access S3\",\n",
    "    AssumeRolePolicyDocument=json.dumps(\n",
    "        {\n",
    "            \"Version\": \"2012-10-17\",\n",
    "            \"Statement\": [\n",
    "                {\n",
    "                    \"Effect\": \"Allow\",\n",
    "                    \"Action\": 'sts:AssumeRole',\n",
    "                    \"Principal\":{\"Service\": \"redshift.amazonaws.com\"}\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "dwhRole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::544495716151:role/RedShift_Impersonation'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "role = iam.get_role(RoleName = \"Redshift_Impersonation\")\n",
    "role_arn = role['Role']['Arn']\n",
    "role_arn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '7b1ea781-a7bc-40d2-a77d-e4698bb64e38',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '7b1ea781-a7bc-40d2-a77d-e4698bb64e38',\n",
       "   'content-type': 'text/xml',\n",
       "   'content-length': '212',\n",
       "   'date': 'Mon, 11 Sep 2023 17:09:01 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Attaching IAM policy to the role (which actually gives permissions):\n",
    "\n",
    "attach_response = iam.attach_role_policy(\n",
    "    RoleName = \"RedShift_Impersonation\",\n",
    "    PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3FullAccess\"\n",
    ")\n",
    "\n",
    "attach_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Apply VPC Security Group rules to Redshift\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining PORT for Redshift + VPC security group\n",
    "redshift_port = 5439"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec2 = boto3.client('ec2',\n",
    "    region_name             = \"us-west-2\",\n",
    "    aws_access_key_id       = aws_cred['kevin_aws_account']['aws_access_key_id'],\n",
    "    aws_secret_access_key   = aws_cred['kevin_aws_account']['aws_secret_access_key']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "An error occurred (InvalidGroup.Duplicate) when calling the CreateSecurityGroup operation: The security group 'Redshift_secGroup' already exists for VPC 'vpc-0714907a778e89500'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb Cell 14\u001b[0m line \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m response \u001b[39m=\u001b[39m ec2\u001b[39m.\u001b[39;49mcreate_security_group(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     Description \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mSecurity Group for allowing all access to Redshift cluster\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     GroupName \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mRedshift_secGroup\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m )\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m response\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/botocore/client.py:508\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    505\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpy_operation_name\u001b[39m}\u001b[39;00m\u001b[39m() only accepts keyword arguments.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    506\u001b[0m     )\n\u001b[1;32m    507\u001b[0m \u001b[39m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 508\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_api_call(operation_name, kwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/botocore/client.py:915\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    913\u001b[0m     error_code \u001b[39m=\u001b[39m parsed_response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mError\u001b[39m\u001b[39m\"\u001b[39m, {})\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mCode\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    914\u001b[0m     error_class \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 915\u001b[0m     \u001b[39mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    916\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     \u001b[39mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (InvalidGroup.Duplicate) when calling the CreateSecurityGroup operation: The security group 'Redshift_secGroup' already exists for VPC 'vpc-0714907a778e89500'"
     ]
    }
   ],
   "source": [
    "response = ec2.create_security_group(\n",
    "    Description = \"Security Group for allowing all access to Redshift cluster\",\n",
    "    GroupName = \"Redshift_secGroup\"\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_groups = ec2.describe_security_groups(\n",
    "    GroupNames = [\n",
    "        'Redshift_secGroup'\n",
    "    ]\n",
    ")\n",
    "\n",
    "sec_groups\n",
    "redshift_sg_id = sec_groups['SecurityGroups'][0]['GroupId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "An error occurred (InvalidPermission.Duplicate) when calling the AuthorizeSecurityGroupIngress operation: the specified rule \"peer: 0.0.0.0/0, TCP, from port: 5439, to port: 5439, ALLOW\" already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb Cell 16\u001b[0m line \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m vpc \u001b[39m=\u001b[39m ec2\u001b[39m.\u001b[39;49mauthorize_security_group_ingress(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     CidrIp \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m0.0.0.0/0\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m#Allowing permission to access from any IP\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     FromPort \u001b[39m=\u001b[39;49m redshift_port, \u001b[39m#Default port for Redshift\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     ToPort \u001b[39m=\u001b[39;49m redshift_port,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     IpProtocol \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mTCP\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     GroupId \u001b[39m=\u001b[39;49m redshift_sg_id\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X21sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/botocore/client.py:508\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    505\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpy_operation_name\u001b[39m}\u001b[39;00m\u001b[39m() only accepts keyword arguments.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    506\u001b[0m     )\n\u001b[1;32m    507\u001b[0m \u001b[39m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 508\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_api_call(operation_name, kwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/botocore/client.py:915\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    913\u001b[0m     error_code \u001b[39m=\u001b[39m parsed_response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mError\u001b[39m\u001b[39m\"\u001b[39m, {})\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mCode\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    914\u001b[0m     error_class \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 915\u001b[0m     \u001b[39mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    916\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     \u001b[39mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (InvalidPermission.Duplicate) when calling the AuthorizeSecurityGroupIngress operation: the specified rule \"peer: 0.0.0.0/0, TCP, from port: 5439, to port: 5439, ALLOW\" already exists"
     ]
    }
   ],
   "source": [
    "vpc = ec2.authorize_security_group_ingress(\n",
    "    CidrIp = '0.0.0.0/0', #Allowing permission to access from any IP\n",
    "    FromPort = redshift_port, #Default port for Redshift\n",
    "    ToPort = redshift_port,\n",
    "    IpProtocol = 'TCP',\n",
    "    GroupId = redshift_sg_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Creating Redshift cluster\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "redshiftServerless = boto3.client('redshift-serverless',\n",
    "    region_name             = \"us-west-2\",\n",
    "    aws_access_key_id       = aws_cred['kevin_aws_account']['aws_access_key_id'],\n",
    "    aws_secret_access_key   = aws_cred['kevin_aws_account']['aws_secret_access_key']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameSpace = \"udacity-course-namespace\"\n",
    "dbName = \"default-db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'namespace': {'adminUsername': 'VictorCreme3',\n",
       "  'creationDate': datetime.datetime(2023, 9, 11, 17, 27, 5, 519000, tzinfo=tzutc()),\n",
       "  'dbName': 'default-db',\n",
       "  'iamRoles': ['arn:aws:iam::544495716151:role/RedShift_Impersonation'],\n",
       "  'kmsKeyId': 'AWS_OWNED_KMS_KEY',\n",
       "  'logExports': [],\n",
       "  'namespaceArn': 'arn:aws:redshift-serverless:us-west-2:544495716151:namespace/b69f3367-7c37-4ec3-bb7b-b94d81ca2a95',\n",
       "  'namespaceId': 'b69f3367-7c37-4ec3-bb7b-b94d81ca2a95',\n",
       "  'namespaceName': 'udacity-course-namespace',\n",
       "  'status': 'AVAILABLE'},\n",
       " 'ResponseMetadata': {'RequestId': 'b5d79e5e-6153-48da-91b2-ab348083274d',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'b5d79e5e-6153-48da-91b2-ab348083274d',\n",
       "   'date': 'Mon, 11 Sep 2023 17:27:05 GMT',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '458',\n",
       "   'connection': 'keep-alive'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Create NameSpace (collection of Database objects and users)\n",
    "#   1a. Database name\n",
    "#   1b. Give IAM role to NameSpace\n",
    "# 2. Create Workgroup\n",
    "#   2a.Set capacity (used to process Data Warehouse loads - maybe a cap?)\n",
    "#   2b. Set VPC\n",
    "#   2c. Set VPC security group\n",
    "#   2d. Specify subnets to be used within VPC\n",
    "\n",
    "redshiftServerless.create_namespace(\n",
    "    adminUserPassword=redshift_cred['redshift_credentials']['un'],\n",
    "    adminUsername=redshift_cred['redshift_credentials']['pw'],\n",
    "    namespaceName = nameSpace,\n",
    "    dbName = dbName,\n",
    "    iamRoles = [role_arn],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'workgroup': {'baseCapacity': 128,\n",
       "  'configParameters': [{'parameterKey': 'auto_mv', 'parameterValue': 'true'},\n",
       "   {'parameterKey': 'datestyle', 'parameterValue': 'ISO, MDY'},\n",
       "   {'parameterKey': 'enable_case_sensitive_identifier',\n",
       "    'parameterValue': 'false'},\n",
       "   {'parameterKey': 'enable_user_activity_logging', 'parameterValue': 'true'},\n",
       "   {'parameterKey': 'query_group', 'parameterValue': 'default'},\n",
       "   {'parameterKey': 'search_path', 'parameterValue': '$user, public'},\n",
       "   {'parameterKey': 'max_query_execution_time', 'parameterValue': '14400'}],\n",
       "  'creationDate': datetime.datetime(2023, 9, 11, 17, 27, 9, 576000, tzinfo=tzutc()),\n",
       "  'endpoint': {'address': 'udacity-course.544495716151.us-west-2.redshift-serverless.amazonaws.com',\n",
       "   'port': 5439},\n",
       "  'enhancedVpcRouting': True,\n",
       "  'namespaceName': 'udacity-course-namespace',\n",
       "  'publiclyAccessible': True,\n",
       "  'securityGroupIds': ['sg-0112dfb8ffa5fbfa2'],\n",
       "  'status': 'CREATING',\n",
       "  'subnetIds': ['subnet-0c9dc89be92992e01',\n",
       "   'subnet-080c291f07f1150fb',\n",
       "   'subnet-0eab91f006c2d6cc7',\n",
       "   'subnet-00788cd5aa6e92889'],\n",
       "  'workgroupArn': 'arn:aws:redshift-serverless:us-west-2:544495716151:workgroup/bad39319-bf3b-4a3b-9e65-c61f46b87481',\n",
       "  'workgroupId': 'bad39319-bf3b-4a3b-9e65-c61f46b87481',\n",
       "  'workgroupName': 'udacity-course'},\n",
       " 'ResponseMetadata': {'RequestId': '58a3c66c-42ca-457b-8119-6485e020afc0',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '58a3c66c-42ca-457b-8119-6485e020afc0',\n",
       "   'date': 'Mon, 11 Sep 2023 17:27:10 GMT',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '1152',\n",
       "   'connection': 'keep-alive'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workgroup_response = redshiftServerless.create_workgroup(\n",
    "    workgroupName = \"udacity-course\",\n",
    "    namespaceName = nameSpace,\n",
    "    baseCapacity = 128,\n",
    "    securityGroupIds = [redshift_sg_id],\n",
    "    enhancedVpcRouting=True,\n",
    "    publiclyAccessible=True\n",
    ")\n",
    "workgroup_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'baseCapacity': 128,\n",
       " 'configParameters': [{'parameterKey': 'auto_mv', 'parameterValue': 'true'},\n",
       "  {'parameterKey': 'datestyle', 'parameterValue': 'ISO, MDY'},\n",
       "  {'parameterKey': 'enable_case_sensitive_identifier',\n",
       "   'parameterValue': 'false'},\n",
       "  {'parameterKey': 'enable_user_activity_logging', 'parameterValue': 'true'},\n",
       "  {'parameterKey': 'query_group', 'parameterValue': 'default'},\n",
       "  {'parameterKey': 'search_path', 'parameterValue': '$user, public'},\n",
       "  {'parameterKey': 'max_query_execution_time', 'parameterValue': '14400'}],\n",
       " 'creationDate': datetime.datetime(2023, 9, 11, 17, 27, 9, 576000, tzinfo=tzutc()),\n",
       " 'endpoint': {'address': 'udacity-course.544495716151.us-west-2.redshift-serverless.amazonaws.com',\n",
       "  'port': 5439,\n",
       "  'vpcEndpoints': [{'networkInterfaces': [{'availabilityZone': 'us-west-2c',\n",
       "      'networkInterfaceId': 'eni-0eeccf3c6873a20f4',\n",
       "      'privateIpAddress': '172.31.4.51',\n",
       "      'subnetId': 'subnet-080c291f07f1150fb'}],\n",
       "    'vpcEndpointId': 'vpce-0bcfb5acf0a71a82a',\n",
       "    'vpcId': 'vpc-0714907a778e89500'}]},\n",
       " 'enhancedVpcRouting': True,\n",
       " 'namespaceName': 'udacity-course-namespace',\n",
       " 'publiclyAccessible': True,\n",
       " 'securityGroupIds': ['sg-0112dfb8ffa5fbfa2'],\n",
       " 'status': 'AVAILABLE',\n",
       " 'subnetIds': ['subnet-0c9dc89be92992e01',\n",
       "  'subnet-080c291f07f1150fb',\n",
       "  'subnet-0eab91f006c2d6cc7',\n",
       "  'subnet-00788cd5aa6e92889'],\n",
       " 'workgroupArn': 'arn:aws:redshift-serverless:us-west-2:544495716151:workgroup/bad39319-bf3b-4a3b-9e65-c61f46b87481',\n",
       " 'workgroupId': 'bad39319-bf3b-4a3b-9e65-c61f46b87481',\n",
       " 'workgroupName': 'udacity-course'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import sleep\n",
    "\n",
    "res = redshiftServerless.list_workgroups()\n",
    "while res['workgroups'][0]['status'] == \"CREATING\":\n",
    "    print(\"Creating cluster...\")\n",
    "    sleep(10)\n",
    "    res = redshiftServerless.list_workgroups()\n",
    "\n",
    "res['workgroups'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'udacity-course.544495716151.us-west-2.redshift-serverless.amazonaws.com'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#here is the resource address that we can use from Airflow:\n",
    "redshift_workgroup = res['workgroups'][0]['endpoint']['address']\n",
    "redshift_workgroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD VERSION WHICH CREATES NON-SERVERLESS REDSHIFT:\n",
    "# #Documentation: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/redshift.html#Redshift.Client.create_cluster\n",
    "# redshift_response = redshiftServerless.create_cluster(\n",
    "#     ClusterType = \"multi-node\",\n",
    "#     NodeType = 'dc2.large',\n",
    "#     NumberOfNodes = 2,\n",
    "#     DBName = \"my_redshift_db\",\n",
    "#     ClusterIdentifier = 'redshift-cluster-2',\n",
    "#     MasterUsername = redshift_cred['redshift_credentials']['un'],\n",
    "#     MasterUserPassword = redshift_cred['redshift_credentials']['pw'],\n",
    "#     IamRoles = [role_arn],\n",
    "#     PubliclyAccessible = True,\n",
    "#     VpcSecurityGroupIds = [\n",
    "#         redshift_sg_id\n",
    "#     ],\n",
    "#     Port = redshift_port\n",
    "# )\n",
    "\n",
    "# '''\n",
    "# WARNING! After running this code, you WILL create a Redshift cluster. Be sure to delete it to not incur costs!!\n",
    "# '''\n",
    "\n",
    "# redshift_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from time import sleep\n",
    "\n",
    "# #Cluster takes time to create. This loop iterates until redshift is finished and returns details:\n",
    "# for i in range(20):\n",
    "#     clusters = redshift.describe_clusters()\n",
    "#     if(clusters['Clusters'] == []):\n",
    "#         print(\"cluster still forming...\")\n",
    "#         sleep(5)\n",
    "#         continue\n",
    "#     else:\n",
    "#         try:\n",
    "#             redshift_host = clusters['Clusters'][0]['Endpoint']['Address']\n",
    "#             redshift_port = str(clusters['Clusters'][0]['Endpoint']['Port'])\n",
    "#             redshift_name = clusters['Clusters'][0]['DBName']\n",
    "#             cluster_id = clusters['Clusters'][0]['ClusterIdentifier']\n",
    "\n",
    "#             redshift_user = redshift_cred['redshift_credentials']['UN']\n",
    "#             redshift_pw = redshift_cred['redshift_credentials']['PW']\n",
    "#             print(\"---Variables Loaded Successfully---\")\n",
    "#             print(clusters)\n",
    "#             break\n",
    "#         except:\n",
    "#             print(\"Error in outputting cluster metrics, trying again...\")\n",
    "#             sleep(10)\n",
    "\n",
    "    \n",
    "\n",
    "#     #if(clusters['Clusters'] == []):\n",
    "#     #   print(\"No clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = redshift.delete_cluster(\n",
    "#     ClusterIdentifier = cluster_id,\n",
    "#     SkipFinalClusterSnapshot=True\n",
    "# )\n",
    "# response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Airflow Connnection: AWS credentials\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[sudo] password for rambino: Sorry, try again.\n",
      "[sudo] password for rambino: \n",
      "sudo: no password was provided\n",
      "sudo: 1 incorrect password attempt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Note: Double curly braces ('{{') necessary when using string formatting\n",
    "#Requires Airflow to be running in docker container on local machine\n",
    "\n",
    "command = '''sudo -S docker-compose run airflow-worker connections add 'aws_credentials' \\\n",
    "    --conn-json '{{ \\\n",
    "        \"conn_type\": \"aws\", \\\n",
    "        \"login\":\"{}\", \\\n",
    "        \"password\":\"{}\", \\\n",
    "        \"extra\": {{ \\\n",
    "            \"region_name\": \"us-west-2\" \\\n",
    "        }} \\\n",
    "    }}'\n",
    "'''.format(\n",
    "    aws_cred['airflow_access']['aws_access_key_id']\n",
    "    ,aws_cred['airflow_access']['aws_secret_access_key']\n",
    ")\n",
    "\n",
    "os.system('echo {} | {}'.format(getpass.getpass(),command))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Airflow Connnection: Redshift Credentials\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary:\n",
    "1. ConnID = \"redshift_connection\" (or sth similar)\n",
    "2. ConnType = \"Postgres\"\n",
    "3. Host = redshift_host\n",
    "4. Port = redshift_port\n",
    "5. Schema = redshift_name\n",
    "6. Login = redshift_user\n",
    "7. Password = redshift_pw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[sudo] password for rambino: Starting docker_airflow-airflow-init-1 ... \n",
      "Starting docker_airflow-airflow-init-1 ... done\n",
      "Creating docker_airflow_airflow-worker_run ... \n",
      "Creating docker_airflow_airflow-worker_run ... done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[\u001b[34m2023-09-13T17:28:23.100+0000\u001b[0m] {\u001b[34mcrypto.py:\u001b[0m82} WARNING\u001b[0m - empty cryptography key - values will not be stored encrypted.\u001b[0m\n",
      "Successfully added `conn_id`=redshift_connection : redshift://adminUser1:******@udacity-course.544495716151.us-west-2.redshift-serverless.amazonaws.com:5439/default-db\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Note: Double curly braces ('{{') necessary when using string formatting\n",
    "\n",
    "#Dummy connection with no real data:\n",
    "command = '''sudo -S docker-compose run airflow-worker connections add 'redshift_connection' \\\n",
    "    --conn-json '{{ \\\n",
    "        \"conn_type\": \"redshift\", \\\n",
    "        \"login\": \"{0}\", \\\n",
    "        \"password\": \"{1}\", \\\n",
    "        \"host\": \"{2}\", \\\n",
    "        \"port\": {3}, \\\n",
    "        \"schema\": \"{4}\" \\\n",
    "    }}'\n",
    "'''.format(\n",
    "    redshift_cred['redshift_credentials']['UN'],\n",
    "    redshift_cred['redshift_credentials']['PW'],\n",
    "    redshift_workgroup,\n",
    "    redshift_port,\n",
    "    dbName\n",
    ")\n",
    "\n",
    "os.system('echo {} | {}'.format(getpass.getpass(),command))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
