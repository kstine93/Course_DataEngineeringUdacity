{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redshift Airflow Setup\n",
    "\n",
    "This code:\n",
    "1. Loads AWS credentials\n",
    "2. Creates Redshift instance and retrieves connection details\n",
    "3. Configures Airflow to have connections for AWS + for this Redshift instance\n",
    "\n",
    "This is in support of `redshift_dag1.py` which then uses this setup to run Airflow jobs with Redshift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Module Import\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Loading Config Files + Credentials\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/rambino/dev/DataEngineering_Udacity/04_AWS_DataWarehousing/redshift_credentials.cfg']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#AWS Credentials\n",
    "aws_path = \"/home/rambino/.aws/credentials\"\n",
    "aws_cred = configparser.ConfigParser()\n",
    "aws_cred.read(aws_path)\n",
    "\n",
    "#Redshift Credentials\n",
    "redshift_path = \"/home/rambino/dev/DataEngineering_Udacity/04_AWS_DataWarehousing/redshift_credentials.cfg\"\n",
    "redshift_cred = configparser.ConfigParser()\n",
    "redshift_cred.read(redshift_path)\n",
    "\n",
    "# #ETL Config\n",
    "# cfg_path = \"/home/rambino/dev/DataEngineering_Udacity/Projects/DataWarehouseWithRedshift/dwh.cfg\"\n",
    "# cfg = configparser.ConfigParser()\n",
    "# cfg.read(cfg_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Creating IAM role for Redshift\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam = boto3.client('iam',\n",
    "    region_name             = \"us-west-2\",\n",
    "    aws_access_key_id       = aws_cred['default']['aws_access_key_id'],\n",
    "    aws_secret_access_key   = aws_cred['default']['aws_secret_access_key']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "EntityAlreadyExistsException",
     "evalue": "An error occurred (EntityAlreadyExists) when calling the CreateRole operation: Role with name RedShift_Impersonation already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEntityAlreadyExistsException\u001b[0m              Traceback (most recent call last)",
      "\u001b[1;32m/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#Create IAM role:\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m#This policy is something about allowing Redshift to impersonate a user, but I don't fully understand it yet.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m#Look more into what \"sts:AssumeRole\" really means.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m dwhRole \u001b[39m=\u001b[39m iam\u001b[39m.\u001b[39;49mcreate_role(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     Path \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     RoleName \u001b[39m=\u001b[39;49m  \u001b[39m\"\u001b[39;49m\u001b[39mRedShift_Impersonation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     Description \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mAllows redshift to access S3\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     AssumeRolePolicyDocument\u001b[39m=\u001b[39;49mjson\u001b[39m.\u001b[39;49mdumps(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X11sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         {\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X11sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mVersion\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39m2012-10-17\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X11sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mStatement\u001b[39;49m\u001b[39m\"\u001b[39;49m: [\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m                 {\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X11sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m                     \u001b[39m\"\u001b[39;49m\u001b[39mEffect\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mAllow\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X11sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m                     \u001b[39m\"\u001b[39;49m\u001b[39mAction\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m'\u001b[39;49m\u001b[39msts:AssumeRole\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X11sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m                     \u001b[39m\"\u001b[39;49m\u001b[39mPrincipal\u001b[39;49m\u001b[39m\"\u001b[39;49m:{\u001b[39m\"\u001b[39;49m\u001b[39mService\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mredshift.amazonaws.com\u001b[39;49m\u001b[39m\"\u001b[39;49m}\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X11sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m                 }\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X11sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m             ]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X11sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m         }\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X11sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X11sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X11sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m dwhRole\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/botocore/client.py:508\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    505\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpy_operation_name\u001b[39m}\u001b[39;00m\u001b[39m() only accepts keyword arguments.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    506\u001b[0m     )\n\u001b[1;32m    507\u001b[0m \u001b[39m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 508\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_api_call(operation_name, kwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/botocore/client.py:915\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    913\u001b[0m     error_code \u001b[39m=\u001b[39m parsed_response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mError\u001b[39m\u001b[39m\"\u001b[39m, {})\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mCode\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    914\u001b[0m     error_class \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 915\u001b[0m     \u001b[39mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    916\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     \u001b[39mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mEntityAlreadyExistsException\u001b[0m: An error occurred (EntityAlreadyExists) when calling the CreateRole operation: Role with name RedShift_Impersonation already exists."
     ]
    }
   ],
   "source": [
    "#Create IAM role:\n",
    "\n",
    "#This policy is something about allowing Redshift to impersonate a user, but I don't fully understand it yet.\n",
    "#Look more into what \"sts:AssumeRole\" really means.\n",
    "\n",
    "import json\n",
    "\n",
    "dwhRole = iam.create_role(\n",
    "    Path = \"/\",\n",
    "    RoleName =  \"RedShift_Impersonation\",\n",
    "    Description = \"Allows redshift to access S3\",\n",
    "    AssumeRolePolicyDocument=json.dumps(\n",
    "        {\n",
    "            \"Version\": \"2012-10-17\",\n",
    "            \"Statement\": [\n",
    "                {\n",
    "                    \"Effect\": \"Allow\",\n",
    "                    \"Action\": 'sts:AssumeRole',\n",
    "                    \"Principal\":{\"Service\": \"redshift.amazonaws.com\"}\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "dwhRole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::549653882425:role/RedShift_Impersonation'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "role = iam.get_role(RoleName = \"Redshift_Impersonation\")\n",
    "role_arn = role['Role']['Arn']\n",
    "role_arn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '16b291ef-469f-441f-bed7-bd8e0c83a7a4',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '16b291ef-469f-441f-bed7-bd8e0c83a7a4',\n",
       "   'content-type': 'text/xml',\n",
       "   'content-length': '212',\n",
       "   'date': 'Mon, 17 Oct 2022 17:13:55 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Attaching IAM policy to the role (which actually gives permissions):\n",
    "\n",
    "attach_response = iam.attach_role_policy(\n",
    "    RoleName = \"RedShift_Impersonation\",\n",
    "    PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    ")\n",
    "\n",
    "attach_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Apply VPC Security Group rules to Redshift\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining PORT for Redshift + VPC security group\n",
    "redshift_port = 5439"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec2 = boto3.client('ec2',\n",
    "    region_name             = \"us-west-2\",\n",
    "    aws_access_key_id       = aws_cred['default']['aws_access_key_id'],\n",
    "    aws_secret_access_key   = aws_cred['default']['aws_secret_access_key']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "An error occurred (InvalidGroup.Duplicate) when calling the CreateSecurityGroup operation: The security group 'Redshift_secGroup' already exists for VPC 'vpc-0d64087a33995cf20'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m response \u001b[39m=\u001b[39m ec2\u001b[39m.\u001b[39;49mcreate_security_group(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     Description \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mSecurity Group for allowing all access to Redshift cluster\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     GroupName \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mRedshift_secGroup\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m )\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m response\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/botocore/client.py:508\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    505\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpy_operation_name\u001b[39m}\u001b[39;00m\u001b[39m() only accepts keyword arguments.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    506\u001b[0m     )\n\u001b[1;32m    507\u001b[0m \u001b[39m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 508\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_api_call(operation_name, kwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/botocore/client.py:915\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    913\u001b[0m     error_code \u001b[39m=\u001b[39m parsed_response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mError\u001b[39m\u001b[39m\"\u001b[39m, {})\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mCode\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    914\u001b[0m     error_class \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 915\u001b[0m     \u001b[39mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    916\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     \u001b[39mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (InvalidGroup.Duplicate) when calling the CreateSecurityGroup operation: The security group 'Redshift_secGroup' already exists for VPC 'vpc-0d64087a33995cf20'"
     ]
    }
   ],
   "source": [
    "response = ec2.create_security_group(\n",
    "    Description = \"Security Group for allowing all access to Redshift cluster\",\n",
    "    GroupName = \"Redshift_secGroup\"\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_groups = ec2.describe_security_groups(\n",
    "    GroupNames = [\n",
    "        'Redshift_secGroup'\n",
    "    ]\n",
    ")\n",
    "\n",
    "sec_groups\n",
    "redshift_sg_id = sec_groups['SecurityGroups'][0]['GroupId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "An error occurred (InvalidPermission.Duplicate) when calling the AuthorizeSecurityGroupIngress operation: the specified rule \"peer: 0.0.0.0/0, TCP, from port: 5439, to port: 5439, ALLOW\" already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m vpc \u001b[39m=\u001b[39m ec2\u001b[39m.\u001b[39;49mauthorize_security_group_ingress(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     CidrIp \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m0.0.0.0/0\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m#Allowing permission to access from any IP\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     FromPort \u001b[39m=\u001b[39;49m redshift_port, \u001b[39m#Default port for Redshift\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     ToPort \u001b[39m=\u001b[39;49m redshift_port,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     IpProtocol \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mTCP\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     GroupId \u001b[39m=\u001b[39;49m redshift_sg_id\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/07_Airflow_DataPipelines/docker_airflow/redshift_airflow_setup.ipynb#X22sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/botocore/client.py:508\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    505\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpy_operation_name\u001b[39m}\u001b[39;00m\u001b[39m() only accepts keyword arguments.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    506\u001b[0m     )\n\u001b[1;32m    507\u001b[0m \u001b[39m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 508\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_api_call(operation_name, kwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/botocore/client.py:915\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    913\u001b[0m     error_code \u001b[39m=\u001b[39m parsed_response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mError\u001b[39m\u001b[39m\"\u001b[39m, {})\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mCode\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    914\u001b[0m     error_class \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 915\u001b[0m     \u001b[39mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    916\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     \u001b[39mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (InvalidPermission.Duplicate) when calling the AuthorizeSecurityGroupIngress operation: the specified rule \"peer: 0.0.0.0/0, TCP, from port: 5439, to port: 5439, ALLOW\" already exists"
     ]
    }
   ],
   "source": [
    "vpc = ec2.authorize_security_group_ingress(\n",
    "    CidrIp = '0.0.0.0/0', #Allowing permission to access from any IP\n",
    "    FromPort = redshift_port, #Default port for Redshift\n",
    "    ToPort = redshift_port,\n",
    "    IpProtocol = 'TCP',\n",
    "    GroupId = redshift_sg_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Creating Redshift cluster\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "redshift = boto3.client('redshift',\n",
    "    region_name             = \"us-west-2\",\n",
    "    aws_access_key_id       = aws_cred['default']['aws_access_key_id'],\n",
    "    aws_secret_access_key   = aws_cred['default']['aws_secret_access_key']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Cluster': {'ClusterIdentifier': 'redshift-cluster-2',\n",
       "  'NodeType': 'dc2.large',\n",
       "  'ClusterStatus': 'creating',\n",
       "  'ClusterAvailabilityStatus': 'Modifying',\n",
       "  'MasterUsername': 'dev',\n",
       "  'DBName': 'my_redshift_db',\n",
       "  'AutomatedSnapshotRetentionPeriod': 1,\n",
       "  'ManualSnapshotRetentionPeriod': -1,\n",
       "  'ClusterSecurityGroups': [],\n",
       "  'VpcSecurityGroups': [{'VpcSecurityGroupId': 'sg-0e29a3f1bc12cd56e',\n",
       "    'Status': 'active'}],\n",
       "  'ClusterParameterGroups': [{'ParameterGroupName': 'default.redshift-1.0',\n",
       "    'ParameterApplyStatus': 'in-sync'}],\n",
       "  'ClusterSubnetGroupName': 'default',\n",
       "  'VpcId': 'vpc-0d64087a33995cf20',\n",
       "  'PreferredMaintenanceWindow': 'wed:10:00-wed:10:30',\n",
       "  'PendingModifiedValues': {'MasterUserPassword': '****'},\n",
       "  'ClusterVersion': '1.0',\n",
       "  'AllowVersionUpgrade': True,\n",
       "  'NumberOfNodes': 2,\n",
       "  'PubliclyAccessible': True,\n",
       "  'Encrypted': False,\n",
       "  'Tags': [],\n",
       "  'EnhancedVpcRouting': False,\n",
       "  'IamRoles': [{'IamRoleArn': 'arn:aws:iam::549653882425:role/RedShift_Impersonation',\n",
       "    'ApplyStatus': 'adding'}],\n",
       "  'MaintenanceTrackName': 'current',\n",
       "  'DeferredMaintenanceWindows': [],\n",
       "  'NextMaintenanceWindowStartTime': datetime.datetime(2022, 10, 19, 10, 0, tzinfo=tzutc()),\n",
       "  'AquaConfiguration': {'AquaStatus': 'disabled',\n",
       "   'AquaConfigurationStatus': 'auto'}},\n",
       " 'ResponseMetadata': {'RequestId': '0ea47480-370b-4dda-9b54-da90d13480eb',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '0ea47480-370b-4dda-9b54-da90d13480eb',\n",
       "   'content-type': 'text/xml',\n",
       "   'content-length': '2475',\n",
       "   'vary': 'accept-encoding',\n",
       "   'date': 'Mon, 17 Oct 2022 17:16:42 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Documentation: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/redshift.html#Redshift.Client.create_cluster\n",
    "redshift_response = redshift.create_cluster(\n",
    "    ClusterType = \"multi-node\",\n",
    "    NodeType = 'dc2.large',\n",
    "    NumberOfNodes = 2,\n",
    "    DBName = \"my_redshift_db\",\n",
    "    ClusterIdentifier = 'redshift-cluster-2',\n",
    "    MasterUsername = redshift_cred['redshift_credentials']['un'],\n",
    "    MasterUserPassword = redshift_cred['redshift_credentials']['pw'],\n",
    "    IamRoles = [role_arn],\n",
    "    PubliclyAccessible = True,\n",
    "    VpcSecurityGroupIds = [\n",
    "        redshift_sg_id\n",
    "    ],\n",
    "    Port = redshift_port\n",
    ")\n",
    "\n",
    "'''\n",
    "WARNING! After running this code, you WILL create a Redshift cluster. Be sure to delete it to not incur costs!!\n",
    "'''\n",
    "\n",
    "redshift_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in outputting cluster metrics, trying again...\n",
      "Error in outputting cluster metrics, trying again...\n",
      "Error in outputting cluster metrics, trying again...\n",
      "Error in outputting cluster metrics, trying again...\n",
      "Error in outputting cluster metrics, trying again...\n",
      "Error in outputting cluster metrics, trying again...\n",
      "Error in outputting cluster metrics, trying again...\n",
      "Error in outputting cluster metrics, trying again...\n",
      "Error in outputting cluster metrics, trying again...\n",
      "Error in outputting cluster metrics, trying again...\n",
      "Error in outputting cluster metrics, trying again...\n",
      "---Variables Loaded Successfully---\n",
      "{'Clusters': [{'ClusterIdentifier': 'redshift-cluster-2', 'NodeType': 'dc2.large', 'ClusterStatus': 'available', 'ClusterAvailabilityStatus': 'Available', 'MasterUsername': 'dev', 'DBName': 'my_redshift_db', 'Endpoint': {'Address': 'redshift-cluster-2.ci137bsnqj5n.us-west-2.redshift.amazonaws.com', 'Port': 5439}, 'ClusterCreateTime': datetime.datetime(2022, 10, 17, 17, 18, 36, 633000, tzinfo=tzutc()), 'AutomatedSnapshotRetentionPeriod': 1, 'ManualSnapshotRetentionPeriod': -1, 'ClusterSecurityGroups': [], 'VpcSecurityGroups': [{'VpcSecurityGroupId': 'sg-0e29a3f1bc12cd56e', 'Status': 'active'}], 'ClusterParameterGroups': [{'ParameterGroupName': 'default.redshift-1.0', 'ParameterApplyStatus': 'in-sync'}], 'ClusterSubnetGroupName': 'default', 'VpcId': 'vpc-0d64087a33995cf20', 'AvailabilityZone': 'us-west-2b', 'PreferredMaintenanceWindow': 'wed:10:00-wed:10:30', 'PendingModifiedValues': {}, 'ClusterVersion': '1.0', 'AllowVersionUpgrade': True, 'NumberOfNodes': 2, 'PubliclyAccessible': True, 'Encrypted': False, 'ClusterPublicKey': 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCnccfw7rSgJopmVfUpHa3SU06Ca9HvgYUGQgUN2uXVpJ8Xz1ZgsJi4CtmJkyCkr4kvn5l13lp6Cm6kgeSzokyFByQbmIpErnazv7bl7mhzBbJHuQpAdz8gwvEcrygB14SEHFz9daW5AFCsvteHHw6uqFa3hI6pZsmg9pOs9UXResU10pFb/LzQZ8n9RwbAuQ2lecRAiOzGKeQZOZ5vhcb2gat4Apq5NlX+1ic0/YOTrR7MU4/eWoU2JubWAPyIHYCNEm2cYnKKe6ZMg8voEUfQCd5lZsL4s/YaWIYsepdbLs3EbXOmfYrBx9jvBcc3Sy/dwhlsVlkcejh1n3ma3Ozj Amazon-Redshift\\n', 'ClusterNodes': [{'NodeRole': 'LEADER', 'PrivateIPAddress': '172.31.43.69', 'PublicIPAddress': '52.11.228.148'}, {'NodeRole': 'COMPUTE-0', 'PrivateIPAddress': '172.31.35.142', 'PublicIPAddress': '34.218.136.4'}, {'NodeRole': 'COMPUTE-1', 'PrivateIPAddress': '172.31.43.95', 'PublicIPAddress': '34.208.101.75'}], 'ClusterRevisionNumber': '41881', 'Tags': [], 'EnhancedVpcRouting': False, 'IamRoles': [{'IamRoleArn': 'arn:aws:iam::549653882425:role/RedShift_Impersonation', 'ApplyStatus': 'in-sync'}], 'MaintenanceTrackName': 'current', 'ElasticResizeNumberOfNodeOptions': '[4]', 'DeferredMaintenanceWindows': [], 'NextMaintenanceWindowStartTime': datetime.datetime(2022, 10, 19, 10, 0, tzinfo=tzutc()), 'AvailabilityZoneRelocationStatus': 'disabled', 'ClusterNamespaceArn': 'arn:aws:redshift:us-west-2:549653882425:namespace:4b020ccf-01b7-4f8e-9452-cf2488fb107f', 'AquaConfiguration': {'AquaStatus': 'disabled', 'AquaConfigurationStatus': 'auto'}}], 'ResponseMetadata': {'RequestId': 'e98ee197-31b9-463d-a757-2bf2a31a9bb9', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'e98ee197-31b9-463d-a757-2bf2a31a9bb9', 'content-type': 'text/xml', 'content-length': '4433', 'vary': 'accept-encoding', 'date': 'Mon, 17 Oct 2022 17:18:46 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "\n",
    "#Cluster takes time to create. This loop iterates until redshift is finished and returns details:\n",
    "for i in range(20):\n",
    "    clusters = redshift.describe_clusters()\n",
    "    if(clusters['Clusters'] == []):\n",
    "        print(\"cluster still forming...\")\n",
    "        sleep(5)\n",
    "        continue\n",
    "    else:\n",
    "        try:\n",
    "            redshift_host = clusters['Clusters'][0]['Endpoint']['Address']\n",
    "            redshift_port = str(clusters['Clusters'][0]['Endpoint']['Port'])\n",
    "            redshift_name = clusters['Clusters'][0]['DBName']\n",
    "            cluster_id = clusters['Clusters'][0]['ClusterIdentifier']\n",
    "\n",
    "            redshift_user = redshift_cred['redshift_credentials']['UN']\n",
    "            redshift_pw = redshift_cred['redshift_credentials']['PW']\n",
    "            print(\"---Variables Loaded Successfully---\")\n",
    "            print(clusters)\n",
    "            break\n",
    "        except:\n",
    "            print(\"Error in outputting cluster metrics, trying again...\")\n",
    "            sleep(10)\n",
    "\n",
    "    \n",
    "\n",
    "    #if(clusters['Clusters'] == []):\n",
    "    #   print(\"No clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Cluster': {'ClusterIdentifier': 'redshift-cluster-2',\n",
       "  'NodeType': 'dc2.large',\n",
       "  'ClusterStatus': 'deleting',\n",
       "  'ClusterAvailabilityStatus': 'Modifying',\n",
       "  'MasterUsername': 'dev',\n",
       "  'DBName': 'my_redshift_db',\n",
       "  'Endpoint': {'Address': 'redshift-cluster-2.ci137bsnqj5n.us-west-2.redshift.amazonaws.com',\n",
       "   'Port': 5439},\n",
       "  'ClusterCreateTime': datetime.datetime(2022, 10, 17, 17, 18, 36, 633000, tzinfo=tzutc()),\n",
       "  'AutomatedSnapshotRetentionPeriod': 1,\n",
       "  'ManualSnapshotRetentionPeriod': -1,\n",
       "  'ClusterSecurityGroups': [],\n",
       "  'VpcSecurityGroups': [{'VpcSecurityGroupId': 'sg-0e29a3f1bc12cd56e',\n",
       "    'Status': 'active'}],\n",
       "  'ClusterParameterGroups': [{'ParameterGroupName': 'default.redshift-1.0',\n",
       "    'ParameterApplyStatus': 'in-sync'}],\n",
       "  'ClusterSubnetGroupName': 'default',\n",
       "  'VpcId': 'vpc-0d64087a33995cf20',\n",
       "  'AvailabilityZone': 'us-west-2b',\n",
       "  'PreferredMaintenanceWindow': 'wed:10:00-wed:10:30',\n",
       "  'PendingModifiedValues': {},\n",
       "  'ClusterVersion': '1.0',\n",
       "  'AllowVersionUpgrade': True,\n",
       "  'NumberOfNodes': 2,\n",
       "  'PubliclyAccessible': True,\n",
       "  'Encrypted': False,\n",
       "  'Tags': [],\n",
       "  'EnhancedVpcRouting': False,\n",
       "  'IamRoles': [{'IamRoleArn': 'arn:aws:iam::549653882425:role/RedShift_Impersonation',\n",
       "    'ApplyStatus': 'in-sync'}],\n",
       "  'MaintenanceTrackName': 'current',\n",
       "  'DeferredMaintenanceWindows': [],\n",
       "  'NextMaintenanceWindowStartTime': datetime.datetime(2022, 10, 19, 10, 0, tzinfo=tzutc()),\n",
       "  'AquaConfiguration': {'AquaStatus': 'disabled',\n",
       "   'AquaConfigurationStatus': 'auto'}},\n",
       " 'ResponseMetadata': {'RequestId': 'e9d96da9-9e5b-467d-bab4-cf110d7258f1',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'e9d96da9-9e5b-467d-bab4-cf110d7258f1',\n",
       "   'content-type': 'text/xml',\n",
       "   'content-length': '2669',\n",
       "   'vary': 'accept-encoding',\n",
       "   'date': 'Mon, 17 Oct 2022 17:20:59 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = redshift.delete_cluster(\n",
    "    ClusterIdentifier = cluster_id,\n",
    "    SkipFinalClusterSnapshot=True\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Airflow Connnection: AWS credentials\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[sudo] password for rambino: Starting docker_airflow_airflow-init_1 ... \n",
      "Starting docker_airflow_airflow-init_1 ... done\n",
      "Creating docker_airflow_airflow-worker_run ... \n",
      "Creating docker_airflow_airflow-worker_run ... done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/airflow/.local/lib/python3.7/site-packages/airflow/configuration.py:367: FutureWarning: The auth_backends setting in [api] has had airflow.api.auth.backend.session added in the running config, which is needed by the UI. Please update your config before Apache Airflow 3.0.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2022-10-13 17:03:14,055\u001b[0m] {\u001b[34mcrypto.py:\u001b[0m84} WARNING\u001b[0m - empty cryptography key - values will not be stored encrypted.\u001b[0m\n",
      "Successfully added `conn_id`=aws_credentials : aws://AKIAX76PMUY4U6WL7JEA:******@:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Note: Double curly braces ('{{') necessary when using string formatting\n",
    "#Requires Airflow to be running in docker container on local machine\n",
    "\n",
    "command = '''sudo -S docker-compose run airflow-worker connections add 'aws_credentials' \\\n",
    "    --conn-json '{{ \\\n",
    "        \"conn_type\": \"aws\", \\\n",
    "        \"login\":\"{}\", \\\n",
    "        \"password\":\"{}\", \\\n",
    "        \"extra\": {{ \\\n",
    "            \"region_name\": \"us-west-2\" \\\n",
    "        }} \\\n",
    "    }}'\n",
    "'''.format(\n",
    "    aws_cred['default']['aws_access_key_id']\n",
    "    ,aws_cred['default']['aws_secret_access_key']\n",
    ")\n",
    "\n",
    "os.system('echo {} | {}'.format(getpass.getpass(),command))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Airflow Connnection: Redshift Credentials\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary:\n",
    "1. ConnID = \"redshift_connection\" (or sth similar)\n",
    "2. ConnType = \"Postgres\"\n",
    "3. Host = redshift_host\n",
    "4. Port = redshift_port\n",
    "5. Schema = redshift_name\n",
    "6. Login = redshift_user\n",
    "7. Password = redshift_pw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[sudo] password for rambino: Starting docker_airflow_airflow-init_1 ... \n",
      "Starting docker_airflow_airflow-init_1 ... done\n",
      "Creating docker_airflow_airflow-worker_run ... \n",
      "Creating docker_airflow_airflow-worker_run ... done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/airflow/.local/lib/python3.7/site-packages/airflow/configuration.py:367: FutureWarning: The auth_backends setting in [api] has had airflow.api.auth.backend.session added in the running config, which is needed by the UI. Please update your config before Apache Airflow 3.0.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2022-10-13 17:01:02,888\u001b[0m] {\u001b[34mcrypto.py:\u001b[0m84} WARNING\u001b[0m - empty cryptography key - values will not be stored encrypted.\u001b[0m\n",
      "Successfully added `conn_id`=redshift_connection : Postgres://dev:******@redshift-cluster-2.ci137bsnqj5n.us-west-2.redshift.amazonaws.com:5439/my_redshift_db\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Note: Double curly braces ('{{') necessary when using string formatting\n",
    "\n",
    "#Dummy connection with no real data:\n",
    "command = '''sudo -S docker-compose run airflow-worker connections add 'redshift_connection' \\\n",
    "    --conn-json '{{ \\\n",
    "        \"conn_type\": \"Postgres\", \\\n",
    "        \"login\": \"{0}\", \\\n",
    "        \"password\": \"{1}\", \\\n",
    "        \"host\": \"{2}\", \\\n",
    "        \"port\": {3}, \\\n",
    "        \"schema\": \"{4}\" \\\n",
    "    }}'\n",
    "'''.format(\n",
    "    redshift_user,\n",
    "    redshift_pw,\n",
    "    redshift_host,\n",
    "    redshift_port,\n",
    "    redshift_name\n",
    ")\n",
    "\n",
    "os.system('echo {} | {}'.format(getpass.getpass(),command))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
