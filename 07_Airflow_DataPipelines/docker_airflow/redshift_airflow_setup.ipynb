{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redshift Airflow Setup\n",
    "\n",
    "This code:\n",
    "1. Loads AWS credentials\n",
    "2. Creates Redshift instance and retrieves connection details\n",
    "3. Configures Airflow to have connections for AWS + for this Redshift instance\n",
    "\n",
    "This is in support of `redshift_dag1.py` which then uses this setup to run Airflow jobs with Redshift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Module Import\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Loading Config Files + Credentials\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AWS Credentials\n",
    "aws_path = \"/home/rambino/.aws/credentials\"\n",
    "aws_cred = configparser.ConfigParser()\n",
    "aws_cred.read(aws_path)\n",
    "\n",
    "#Redshift Credentials\n",
    "redshift_path = \"/home/rambino/dev/DataEngineering_Udacity/04_AWS_DataWarehousing/redshift_credentials.cfg\"\n",
    "redshift_cred = configparser.ConfigParser()\n",
    "redshift_cred.read(redshift_path)\n",
    "\n",
    "# #ETL Config\n",
    "# cfg_path = \"/home/rambino/dev/DataEngineering_Udacity/Projects/DataWarehouseWithRedshift/dwh.cfg\"\n",
    "# cfg = configparser.ConfigParser()\n",
    "# cfg.read(cfg_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Creating IAM role for Redshift\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam = boto3.client('iam',\n",
    "    region_name             = \"us-west-2\",\n",
    "    aws_access_key_id       = aws_cred['default']['aws_access_key_id'],\n",
    "    aws_secret_access_key   = aws_cred['default']['aws_secret_access_key']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create IAM role:\n",
    "\n",
    "#This policy is something about allowing Redshift to impersonate a user, but I don't fully understand it yet.\n",
    "#Look more into what \"sts:AssumeRole\" really means.\n",
    "\n",
    "import json\n",
    "\n",
    "dwhRole = iam.create_role(\n",
    "    Path = \"/\",\n",
    "    RoleName =  \"RedShift_Impersonation\",\n",
    "    Description = \"Allows redshift to access S3\",\n",
    "    AssumeRolePolicyDocument=json.dumps(\n",
    "        {\n",
    "            \"Version\": \"2012-10-17\",\n",
    "            \"Statement\": [\n",
    "                {\n",
    "                    \"Effect\": \"Allow\",\n",
    "                    \"Action\": 'sts:AssumeRole',\n",
    "                    \"Principal\":{\"Service\": \"redshift.amazonaws.com\"}\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "dwhRole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = iam.get_role(RoleName = \"Redshift_Impersonation\")\n",
    "role_arn = role['Role']['Arn']\n",
    "role_arn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attaching IAM policy to the role (which actually gives permissions):\n",
    "\n",
    "attach_response = iam.attach_role_policy(\n",
    "    RoleName = \"RedShift_Impersonation\",\n",
    "    PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    ")\n",
    "\n",
    "attach_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Apply VPC Security Group rules to Redshift\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining PORT for Redshift + VPC security group\n",
    "redshift_port = 5439"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec2 = boto3.client('ec2',\n",
    "    region_name             = \"us-west-2\",\n",
    "    aws_access_key_id       = aws_cred['default']['aws_access_key_id'],\n",
    "    aws_secret_access_key   = aws_cred['default']['aws_secret_access_key']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ec2.create_security_group(\n",
    "    Description = \"Security Group for allowing all access to Redshift cluster\",\n",
    "    GroupName = \"Redshift_secGroup\"\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_groups = ec2.describe_security_groups(\n",
    "    GroupNames = [\n",
    "        'Redshift_secGroup'\n",
    "    ]\n",
    ")\n",
    "\n",
    "sec_groups\n",
    "redshift_sg_id = sec_groups['SecurityGroups'][0]['GroupId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vpc = ec2.authorize_security_group_ingress(\n",
    "    CidrIp = '0.0.0.0/0', #Allowing permission to access from any IP\n",
    "    FromPort = redshift_port, #Default port for Redshift\n",
    "    ToPort = redshift_port,\n",
    "    IpProtocol = 'TCP',\n",
    "    GroupId = redshift_sg_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Creating Redshift cluster\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redshift = boto3.client('redshift',\n",
    "    region_name             = \"us-west-2\",\n",
    "    aws_access_key_id       = aws_cred['default']['aws_access_key_id'],\n",
    "    aws_secret_access_key   = aws_cred['default']['aws_secret_access_key']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Documentation: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/redshift.html#Redshift.Client.create_cluster\n",
    "redshift_response = redshift.create_cluster(\n",
    "    ClusterType = \"multi-node\",\n",
    "    NodeType = 'dc2.large',\n",
    "    NumberOfNodes = 2,\n",
    "    DBName = \"my_redshift_db\",\n",
    "    ClusterIdentifier = 'redshift-cluster-2',\n",
    "    MasterUsername = redshift_cred['redshift_credentials']['un'],\n",
    "    MasterUserPassword = redshift_cred['redshift_credentials']['pw'],\n",
    "    IamRoles = [role_arn],\n",
    "    PubliclyAccessible = True,\n",
    "    VpcSecurityGroupIds = [\n",
    "        redshift_sg_id\n",
    "    ],\n",
    "    Port = redshift_port\n",
    ")\n",
    "\n",
    "'''\n",
    "WARNING! After running this code, you WILL create a Redshift cluster. Be sure to delete it to not incur costs!!\n",
    "'''\n",
    "\n",
    "redshift_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "#Cluster takes time to create. This loop iterates until redshift is finished and returns details:\n",
    "for i in range(20):\n",
    "    clusters = redshift.describe_clusters()\n",
    "    if(clusters['Clusters'] == []):\n",
    "        print(\"cluster still forming...\")\n",
    "        sleep(5)\n",
    "        continue\n",
    "    else:\n",
    "        try:\n",
    "            redshift_host = clusters['Clusters'][0]['Endpoint']['Address']\n",
    "            redshift_port = str(clusters['Clusters'][0]['Endpoint']['Port'])\n",
    "            redshift_name = clusters['Clusters'][0]['DBName']\n",
    "            cluster_id = clusters['Clusters'][0]['ClusterIdentifier']\n",
    "\n",
    "            redshift_user = redshift_cred['redshift_credentials']['UN']\n",
    "            redshift_pw = redshift_cred['redshift_credentials']['PW']\n",
    "            print(\"---Variables Loaded Successfully---\")\n",
    "            print(clusters)\n",
    "            break\n",
    "        except:\n",
    "            print(\"Error in outputting cluster metrics, trying again...\")\n",
    "            sleep(10)\n",
    "\n",
    "    \n",
    "\n",
    "    #if(clusters['Clusters'] == []):\n",
    "    #   print(\"No clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = redshift.delete_cluster(\n",
    "    ClusterIdentifier = cluster_id,\n",
    "    SkipFinalClusterSnapshot=True\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Airflow Connnection: AWS credentials\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: Double curly braces ('{{') necessary when using string formatting\n",
    "#Requires Airflow to be running in docker container on local machine\n",
    "\n",
    "command = '''sudo -S docker-compose run airflow-worker connections add 'aws_credentials' \\\n",
    "    --conn-json '{{ \\\n",
    "        \"conn_type\": \"aws\", \\\n",
    "        \"login\":\"{}\", \\\n",
    "        \"password\":\"{}\", \\\n",
    "        \"extra\": {{ \\\n",
    "            \"region_name\": \"us-west-2\" \\\n",
    "        }} \\\n",
    "    }}'\n",
    "'''.format(\n",
    "    aws_cred['default']['aws_access_key_id']\n",
    "    ,aws_cred['default']['aws_secret_access_key']\n",
    ")\n",
    "\n",
    "os.system('echo {} | {}'.format(getpass.getpass(),command))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Airflow Connnection: Redshift Credentials\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary:\n",
    "1. ConnID = \"redshift_connection\" (or sth similar)\n",
    "2. ConnType = \"Postgres\"\n",
    "3. Host = redshift_host\n",
    "4. Port = redshift_port\n",
    "5. Schema = redshift_name\n",
    "6. Login = redshift_user\n",
    "7. Password = redshift_pw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: Double curly braces ('{{') necessary when using string formatting\n",
    "\n",
    "#Dummy connection with no real data:\n",
    "command = '''sudo -S docker-compose run airflow-worker connections add 'redshift_connection' \\\n",
    "    --conn-json '{{ \\\n",
    "        \"conn_type\": \"Postgres\", \\\n",
    "        \"login\": \"{0}\", \\\n",
    "        \"password\": \"{1}\", \\\n",
    "        \"host\": \"{2}\", \\\n",
    "        \"port\": {3}, \\\n",
    "        \"schema\": \"{4}\" \\\n",
    "    }}'\n",
    "'''.format(\n",
    "    redshift_user,\n",
    "    redshift_pw,\n",
    "    redshift_host,\n",
    "    redshift_port,\n",
    "    redshift_name\n",
    ")\n",
    "\n",
    "os.system('echo {} | {}'.format(getpass.getpass(),command))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
