{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Data Cleaning Demo\n",
    "\n",
    "There are a wide variety of ways to manipulate and change data in a Spark-y way. This notebook shows some of them in the context of data cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Importing Modules & Starting Spark Session\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/18 13:00:57 WARN Utils: Your hostname, rambino-AERO-15-XD resolves to a loopback address: 127.0.1.1; using 172.20.10.14 instead (on interface wlp48s0)\n",
      "22/09/18 13:00:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/18 13:00:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "sparkSesh = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Loading Dataset\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Source: http://ita.ee.lbl.gov/traces/NASA_access_log_Jul95.gz\n",
    "logs = sparkSesh.read.text(\"NASA_access_log_Jul95.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Inspecting Data\n",
    "\n",
    "---\n",
    "\n",
    "Overview: Spark has some nifty tools for data inspection, but sometimes it can be easier to convert the Spark df to a Pandas df to take advantage of Pandas increased offering of data inspection and analysis tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1891715"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs.printSchema()\n",
    "logs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------+\n",
      "|value                                                                                                                  |\n",
      "+-----------------------------------------------------------------------------------------------------------------------+\n",
      "|199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] \"GET /history/apollo/ HTTP/1.0\" 200 6245                                 |\n",
      "|unicomp6.unicomp.net - - [01/Jul/1995:00:00:06 -0400] \"GET /shuttle/countdown/ HTTP/1.0\" 200 3985                      |\n",
      "|199.120.110.21 - - [01/Jul/1995:00:00:09 -0400] \"GET /shuttle/missions/sts-73/mission-sts-73.html HTTP/1.0\" 200 4085   |\n",
      "|burger.letters.com - - [01/Jul/1995:00:00:11 -0400] \"GET /shuttle/countdown/liftoff.html HTTP/1.0\" 304 0               |\n",
      "|199.120.110.21 - - [01/Jul/1995:00:00:11 -0400] \"GET /shuttle/missions/sts-73/sts-73-patch-small.gif HTTP/1.0\" 200 4179|\n",
      "+-----------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs.show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] \"GET /history/apollo/ HTTP/1.0\" 200 6245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unicomp6.unicomp.net - - [01/Jul/1995:00:00:06 -0400] \"GET /shuttle/countdown/ HTTP/1.0\" 200 3985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>199.120.110.21 - - [01/Jul/1995:00:00:09 -0400] \"GET /shuttle/missions/sts-73/mission-sts-73.html HTTP/1.0\" 200 4085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>burger.letters.com - - [01/Jul/1995:00:00:11 -0400] \"GET /shuttle/countdown/liftoff.html HTTP/1.0\" 304 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>199.120.110.21 - - [01/Jul/1995:00:00:11 -0400] \"GET /shuttle/missions/sts-73/sts-73-patch-small.gif HTTP/1.0\" 200 4179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                     value\n",
       "0                                   199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] \"GET /history/apollo/ HTTP/1.0\" 200 6245\n",
       "1                        unicomp6.unicomp.net - - [01/Jul/1995:00:00:06 -0400] \"GET /shuttle/countdown/ HTTP/1.0\" 200 3985\n",
       "2     199.120.110.21 - - [01/Jul/1995:00:00:09 -0400] \"GET /shuttle/missions/sts-73/mission-sts-73.html HTTP/1.0\" 200 4085\n",
       "3                 burger.letters.com - - [01/Jul/1995:00:00:11 -0400] \"GET /shuttle/countdown/liftoff.html HTTP/1.0\" 304 0\n",
       "4  199.120.110.21 - - [01/Jul/1995:00:00:11 -0400] \"GET /shuttle/missions/sts-73/sts-73-patch-small.gif HTTP/1.0\" 200 4179"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('max_colwidth',250)\n",
    "#Note: It is almost ALWAYS best to filter data before additional processing. Here we do that by limiting in the Spark df\n",
    "#before passing it to Pandas, rather than passing all the data to Pandas and then limiting there.\n",
    "logs.limit(5).toPandas()\n",
    "#logs.toPandas().head(5) #Inefficient version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From looking at at these data, it appears that we have multiple values simply concatenated as long strings.\n",
    "In order to analyse these data more efficiently, we will need to separate these data into separate columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Trial 1: using 'Split'\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------+\n",
      "|splitVals                                                                                                                         |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[199.72.81.55, -, -, [01/Jul/1995:00:00:01, -0400], \"GET, /history/apollo/, HTTP/1.0\", 200, 6245]                                 |\n",
      "|[unicomp6.unicomp.net, -, -, [01/Jul/1995:00:00:06, -0400], \"GET, /shuttle/countdown/, HTTP/1.0\", 200, 3985]                      |\n",
      "|[199.120.110.21, -, -, [01/Jul/1995:00:00:09, -0400], \"GET, /shuttle/missions/sts-73/mission-sts-73.html, HTTP/1.0\", 200, 4085]   |\n",
      "|[burger.letters.com, -, -, [01/Jul/1995:00:00:11, -0400], \"GET, /shuttle/countdown/liftoff.html, HTTP/1.0\", 304, 0]               |\n",
      "|[199.120.110.21, -, -, [01/Jul/1995:00:00:11, -0400], \"GET, /shuttle/missions/sts-73/sts-73-patch-small.gif, HTTP/1.0\", 200, 4179]|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, col\n",
    "\n",
    "#Split allows us to split strings by a common delimiter. Here's what it looks like for a few rows if we use space:\n",
    "logs \\\n",
    "    .limit(5) \\\n",
    "    .select(\n",
    "        split(col('value'),\" \").alias(\"splitVals\")\n",
    "    ).show(truncate=False)\n",
    "\n",
    "#Looks like some values were separated correctly, but most were not (because spaces are not consistently used as delimiters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Trial 2: Parse using custom UDF\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/18 15:19:40 WARN SimpleFunctionRegistry: The function logparser replaced a previously registered function.\n"
     ]
    }
   ],
   "source": [
    "#Let's build a UDF that can do custom parsing\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import MapType,StringType\n",
    "import re\n",
    "\n",
    "#Note: in the UDFs below, we define 'MapType' as the return type, which allows us to return the data as a JSON.\n",
    "#However, we cannot mix types in these returned map types, so despite having 1 integer type, we are returning all values\n",
    "#as strings.\n",
    "\n",
    "#Note: We can register a UDF to spark using '@' syntax or the function 'register'\n",
    "#Option 1 (define function, convert to UDF, then register):\n",
    "def testFunc(logStr: str):\n",
    "    return logStr\n",
    "\n",
    "sparkSesh.udf.register(\n",
    "    \"logParser\",\n",
    "    udf(\n",
    "        testFunc,\n",
    "        MapType(StringType(),StringType())\n",
    "    )\n",
    ")\n",
    "\n",
    "#Option 2 (add decorator '@udf' and return type, then write function):\n",
    "@udf(returnType=MapType(StringType(),StringType()))\n",
    "def parseLog(logStr: str):\n",
    "    regex = r\"^(?P<client>\\S+?) \\- \\- \\[(?P<datetime>[^\\]]+)\\] +\\\"(?P<request>.*) +(?P<endpoint>\\S+) +(?P<protocol>\\S+)\\\" (?P<status>[0-9]{3}) (?P<size>[0-9]+|\\-)$\"\n",
    "    result = re.search(regex,logStr)\n",
    "\n",
    "    if result is None:\n",
    "        return (logStr,0)\n",
    "\n",
    "    size = result.group('size')\n",
    "    if size == '-':\n",
    "        size = 0\n",
    "\n",
    "    if result is not None:\n",
    "        return {\n",
    "            'client':result.group('client'),\n",
    "            'datetime':result.group('datetime'),\n",
    "            'request':result.group('request'),\n",
    "            'endpoint':result.group('endpoint'),\n",
    "            'protocol':result.group('protocol'),\n",
    "            'status':result.group('status'),\n",
    "            'size':size\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'request': 'GET', 'protocol': 'HTTP/1.0', 'endpoint': '/history/apollo/', 'datetime': '01/Jul/1995:00:00:01 -0400', 'size': '6245', 'client': '199.72.81.55', 'status': '200'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'request': 'GET', 'protocol': 'HTTP/1.0', 'endpoint': '/shuttle/countdown/', 'datetime': '01/Jul/1995:00:00:06 -0400', 'size': '3985', 'client': 'unicomp6.unicomp.net', 'status': '200'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'request': 'GET', 'protocol': 'HTTP/1.0', 'endpoint': '/shuttle/missions/sts-73/mission-sts-73.html', 'datetime': '01/Jul/1995:00:00:09 -0400', 'size': '4085', 'client': '199.120.110.21', 'status': '200'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'request': 'GET', 'protocol': 'HTTP/1.0', 'endpoint': '/shuttle/countdown/liftoff.html', 'datetime': '01/Jul/1995:00:00:11 -0400', 'size': '0', 'client': 'burger.letters.com', 'status': '304'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'request': 'GET', 'protocol': 'HTTP/1.0', 'endpoint': '/shuttle/missions/sts-73/sts-73-patch-small.gif', 'datetime': '01/Jul/1995:00:00:11 -0400', 'size': '4179', 'client': '199.120.110.21', 'status': '200'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'request': 'GET', 'protocol': 'HTTP/1.0', 'endpoint': '/images/NASA-logosmall.gif', 'datetime': '01/Jul/1995:00:00:12 -0400', 'size': '0', 'client': 'burger.letters.com', 'status': '304'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'request': 'GET', 'protocol': 'HTTP/1.0', 'endpoint': '/shuttle/countdown/video/livevideo.gif', 'datetime': '01/Jul/1995:00:00:12 -0400', 'size': '0', 'client': 'burger.letters.com', 'status': '200'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'request': 'GET', 'protocol': 'HTTP/1.0', 'endpoint': '/shuttle/countdown/countdown.html', 'datetime': '01/Jul/1995:00:00:12 -0400', 'size': '3985', 'client': '205.212.115.106', 'status': '200'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'request': 'GET', 'protocol': 'HTTP/1.0', 'endpoint': '/shuttle/countdown/', 'datetime': '01/Jul/1995:00:00:13 -0400', 'size': '3985', 'client': 'd104.aa.net', 'status': '200'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'request': 'GET', 'protocol': 'HTTP/1.0', 'endpoint': '/', 'datetime': '01/Jul/1995:00:00:13 -0400', 'size': '7074', 'client': '129.94.144.152', 'status': '200'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                             parsed\n",
       "0                                   {'request': 'GET', 'protocol': 'HTTP/1.0', 'endpoint': '/history/apollo/', 'datetime': '01/Jul/1995:00:00:01 -0400', 'size': '6245', 'client': '199.72.81.55', 'status': '200'}\n",
       "1                        {'request': 'GET', 'protocol': 'HTTP/1.0', 'endpoint': '/shuttle/countdown/', 'datetime': '01/Jul/1995:00:00:06 -0400', 'size': '3985', 'client': 'unicomp6.unicomp.net', 'status': '200'}\n",
       "2     {'request': 'GET', 'protocol': 'HTTP/1.0', 'endpoint': '/shuttle/missions/sts-73/mission-sts-73.html', 'datetime': '01/Jul/1995:00:00:09 -0400', 'size': '4085', 'client': '199.120.110.21', 'status': '200'}\n",
       "3                 {'request': 'GET', 'protocol': 'HTTP/1.0', 'endpoint': '/shuttle/countdown/liftoff.html', 'datetime': '01/Jul/1995:00:00:11 -0400', 'size': '0', 'client': 'burger.letters.com', 'status': '304'}\n",
       "4  {'request': 'GET', 'protocol': 'HTTP/1.0', 'endpoint': '/shuttle/missions/sts-73/sts-73-patch-small.gif', 'datetime': '01/Jul/1995:00:00:11 -0400', 'size': '4179', 'client': '199.120.110.21', 'status': '200'}\n",
       "5                      {'request': 'GET', 'protocol': 'HTTP/1.0', 'endpoint': '/images/NASA-logosmall.gif', 'datetime': '01/Jul/1995:00:00:12 -0400', 'size': '0', 'client': 'burger.letters.com', 'status': '304'}\n",
       "6          {'request': 'GET', 'protocol': 'HTTP/1.0', 'endpoint': '/shuttle/countdown/video/livevideo.gif', 'datetime': '01/Jul/1995:00:00:12 -0400', 'size': '0', 'client': 'burger.letters.com', 'status': '200'}\n",
       "7               {'request': 'GET', 'protocol': 'HTTP/1.0', 'endpoint': '/shuttle/countdown/countdown.html', 'datetime': '01/Jul/1995:00:00:12 -0400', 'size': '3985', 'client': '205.212.115.106', 'status': '200'}\n",
       "8                                 {'request': 'GET', 'protocol': 'HTTP/1.0', 'endpoint': '/shuttle/countdown/', 'datetime': '01/Jul/1995:00:00:13 -0400', 'size': '3985', 'client': 'd104.aa.net', 'status': '200'}\n",
       "9                                                {'request': 'GET', 'protocol': 'HTTP/1.0', 'endpoint': '/', 'datetime': '01/Jul/1995:00:00:13 -0400', 'size': '7074', 'client': '129.94.144.152', 'status': '200'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfParsed = logs.withColumn('parsed',parseLog('value'))\n",
    "dfParsed.select('parsed').limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Break out map into separate columns\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|              client|\n",
      "+--------------------+\n",
      "|        199.72.81.55|\n",
      "|unicomp6.unicomp.net|\n",
      "|      199.120.110.21|\n",
      "|  burger.letters.com|\n",
      "|      199.120.110.21|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#We can use 'selectExpr' as a way to include dynamic SQL, ex:\n",
    "dfParsed.selectExpr('parsed[\"client\"] as client').limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If we get a little fancy with string interpolation and list comprehensions, we can create these select expressions\n",
    "#dynamically and make a generic expression:\n",
    "\n",
    "from pyspark.sql.functions import map_keys,col\n",
    "import numpy as np\n",
    "\n",
    "#Somewhat verbosely, getting keys of nested Map structure:\n",
    "values = dfParsed \\\n",
    "    .limit(1) \\\n",
    "    .select(\n",
    "        map_keys(col('parsed'))\n",
    "    ) \\\n",
    "    .toPandas() \\\n",
    "    .values \\\n",
    "    .flatten()[0]\n",
    "\n",
    "expressions = [f\"parsed['{key}'] as {key}\" for key in values]\n",
    "\n",
    "#Note: * operator is basically a spread operator, like in JS:\n",
    "dfClean = dfParsed.selectExpr(*expressions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The code above- particularly dynamically making 'selectExpr' expressions - is a bit hacky.\n",
    "I would prefer to find a way to parse this log data that would be simpler and less prone to breaking.\n",
    "\n",
    "**Current Version:**\n",
    "1. Read in data\n",
    "2. Break log string into pieces using regex\n",
    "3. Build JSON from log string pieces\n",
    "4. Store JSON as new column\n",
    "5. Write Spark 'selectExpr' code to convert JSON to new columns\n",
    "6. Convert incorrect value types from JSON (str -> int)\n",
    "\n",
    "**Possibly Better Version:**\n",
    "I wonder if we could do this better by pulling out individual values from the log string incrementally. I don't see the need to get all values from the log string at once if it causes us to STILL have to create new columns from the resulting JSON AND to convert incorrect types.\n",
    "\n",
    "1. Read in data\n",
    "2. Write generic function to apply regex and return matching result\n",
    "3. Write multiple regex to pull out only individual values from log string\n",
    "4. For each individual column needed, create column by running generic function with appropriate regex.\n",
    "\n",
    "This version would require that regex be assessed for every column (a disadvantage), but it would also allow the user to ONLY pull in needed columns instead of importing everything by default (an advantage).\n",
    "I like version #2 in terms of maintainability, but **would require testing to assess performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------------------+--------------------+----+--------------------+------+\n",
      "|request|protocol|            endpoint|            datetime|size|              client|status|\n",
      "+-------+--------+--------------------+--------------------+----+--------------------+------+\n",
      "|    GET|HTTP/1.0|    /history/apollo/|01/Jul/1995:00:00...|6245|        199.72.81.55|   200|\n",
      "|    GET|HTTP/1.0| /shuttle/countdown/|01/Jul/1995:00:00...|3985|unicomp6.unicomp.net|   200|\n",
      "|    GET|HTTP/1.0|/shuttle/missions...|01/Jul/1995:00:00...|4085|      199.120.110.21|   200|\n",
      "|    GET|HTTP/1.0|/shuttle/countdow...|01/Jul/1995:00:00...|   0|  burger.letters.com|   304|\n",
      "|    GET|HTTP/1.0|/shuttle/missions...|01/Jul/1995:00:00...|4179|      199.120.110.21|   200|\n",
      "+-------+--------+--------------------+--------------------+----+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfClean.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------------------+--------------------+----+--------------------+------+--------+\n",
      "|request|protocol|            endpoint|            datetime|size|              client|status|size_int|\n",
      "+-------+--------+--------------------+--------------------+----+--------------------+------+--------+\n",
      "|    GET|HTTP/1.0|    /history/apollo/|01/Jul/1995:00:00...|6245|        199.72.81.55|   200|    6245|\n",
      "|    GET|HTTP/1.0| /shuttle/countdown/|01/Jul/1995:00:00...|3985|unicomp6.unicomp.net|   200|    3985|\n",
      "|    GET|HTTP/1.0|/shuttle/missions...|01/Jul/1995:00:00...|4085|      199.120.110.21|   200|    4085|\n",
      "|    GET|HTTP/1.0|/shuttle/countdow...|01/Jul/1995:00:00...|   0|  burger.letters.com|   304|       0|\n",
      "|    GET|HTTP/1.0|/shuttle/missions...|01/Jul/1995:00:00...|4179|      199.120.110.21|   200|    4179|\n",
      "+-------+--------+--------------------+--------------------+----+--------------------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Now we need to solve the issue we had in the UDF: where all data was returned as strings - since 'size' should be an INT value:\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "dfClean = dfClean.withColumn('size_int',expr('cast(size as int)'))\n",
    "dfClean.limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Analyze\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfClean.createOrReplaceTempView(\"dfClean_sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 117:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|              client|count|\n",
      "+--------------------+-----+\n",
      "|piweba3y.prodigy.com|17572|\n",
      "|piweba4y.prodigy.com|11591|\n",
      "|piweba1y.prodigy.com| 9868|\n",
      "|  alyssa.prodigy.com| 7852|\n",
      "| siltb10.orl.mmc.com| 7573|\n",
      "|piweba2y.prodigy.com| 5922|\n",
      "|  edams.ksc.nasa.gov| 5434|\n",
      "|        163.206.89.4| 4906|\n",
      "|         news.ti.com| 4863|\n",
      "|disarray.demon.co.uk| 4353|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Most common clients\n",
    "sparkSesh.sql('''\n",
    "    SELECT client, COUNT(client) as count\n",
    "    FROM dfClean_sql\n",
    "    GROUP BY client\n",
    "    ORDER BY count DESC\n",
    "    LIMIT 10\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 121:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|            endpoint|size_int|\n",
      "+--------------------+--------+\n",
      "|/shuttle/countdow...| 6823936|\n",
      "|/statistics/1995/...| 3155499|\n",
      "|/statistics/1995/...| 3155499|\n",
      "|/statistics/1995/...| 3155499|\n",
      "|/statistics/1995/...| 3155499|\n",
      "|/statistics/1995/...| 3155499|\n",
      "|/statistics/1995/...| 3155499|\n",
      "|/statistics/1995/...| 3155499|\n",
      "|/statistics/1995/...| 2973350|\n",
      "|/statistics/1995/...| 2973350|\n",
      "+--------------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Biggest files:\n",
    "sparkSesh.sql('''\n",
    "    SELECT endpoint, size_int\n",
    "    FROM dfClean_sql\n",
    "    ORDER BY size_int DESC\n",
    "    LIMIT 10\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of requests per day\n",
    "\n",
    "from pyspark.sql.functions import to_timestamp, lit\n",
    "\n",
    "#first, need to clean 'datetime' column:\n",
    "dfClean = dfClean.withColumn('timestamp',to_timestamp(col('datetime'),\"dd/MMM/yyyy:HH:mm:ss Z\"))\n",
    "dfClean.createOrReplaceTempView(\"dfClean_sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 184:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "| day| count|\n",
      "+----+------+\n",
      "|null|     0|\n",
      "|   1| 45999|\n",
      "|   2| 58994|\n",
      "|   3| 87604|\n",
      "|   4| 74304|\n",
      "|   5| 91426|\n",
      "|   6| 97183|\n",
      "|   7| 95597|\n",
      "|   8| 43702|\n",
      "|   9| 34741|\n",
      "|  10| 66720|\n",
      "|  11| 78467|\n",
      "|  12| 88171|\n",
      "|  13|138096|\n",
      "|  14| 86034|\n",
      "|  15| 49805|\n",
      "|  16| 44324|\n",
      "|  17| 74377|\n",
      "|  18| 66324|\n",
      "|  19| 72283|\n",
      "+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sparkSesh.sql('''\n",
    "    SELECT EXTRACT(day from timestamp) AS day,\n",
    "    COUNT(client) AS count\n",
    "    FROM dfClean_sql\n",
    "    GROUP BY day\n",
    "    ORDER BY day ASC\n",
    "''').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
