{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redshift Setup with Python SDK (boto3)\n",
    "This notebook will show how to set up some AWS resources using the Python SDK for AWS, boto3.\n",
    "\n",
    "Boto3 Documentation: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/redshift.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Package Import\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import re\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Loading Credentials from file\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "\n",
    "config.read_file(open(\"/home/rambino/.aws/credentials\"))\n",
    "aws_key         = config.get('udacity_course','aws_access_key_id')\n",
    "aws_secret      = config.get('udacity_course','aws_secret_access_key')\n",
    "\n",
    "config.read_file(open(\"./redshift_credentials.cfg\"))\n",
    "redshift_user   = config.get('redshift_credentials','UN')\n",
    "redshift_password   = config.get('redshift_credentials','PW')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Creating IAM role for Redshift\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam = boto3.client('iam',\n",
    "    region_name             = \"us-west-2\",\n",
    "    aws_access_key_id       = aws_key,\n",
    "    aws_secret_access_key   = aws_secret\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create IAM role:\n",
    "\n",
    "#This policy is something about allowing Redshift to impersonate a user, but I don't really understand it.\n",
    "#Look more into what \"sts:AssumeRole\" really means.\n",
    "\n",
    "import json\n",
    "\n",
    "dwhRole = iam.create_role(\n",
    "    Path = \"/\",\n",
    "    RoleName =  \"RedShift_Impersonation\",\n",
    "    Description = \"Allows redshift to access S3\",\n",
    "    AssumeRolePolicyDocument=json.dumps(\n",
    "        {\n",
    "            \"Version\": \"2012-10-17\",\n",
    "            \"Statement\": [\n",
    "                {\n",
    "                    \"Effect\": \"Allow\",\n",
    "                    \"Action\": 'sts:AssumeRole',\n",
    "                    \"Principal\":{\"Service\": \"redshift.amazonaws.com\"}\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "dwhRole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = iam.get_role(RoleName = \"Redshift_Impersonation\")\n",
    "role_arn = role['Role']['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attaching IAM policy to the role (which actually gives permissions):\n",
    "\n",
    "attach_response = iam.attach_role_policy(\n",
    "    RoleName = \"RedShift_Impersonation\",\n",
    "    PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    ")\n",
    "\n",
    "attach_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Apply VPC Security Group rules to Redshift\n",
    "\n",
    "---\n",
    "\n",
    "The VPC is currently the AWS component I understand the least. From [what I've read](https://aws.amazon.com/vpc/features/) VPC means that AWS features like Redshift, RDS, and EC2 instances to control how traffic to these services works. For example, can these services talk to each other? Can they be accessed by other applications?\n",
    "It looks like the main form of authentication is IP addresses - where you can specify only certain IP addresses can access the resources you create in AWS.\n",
    "What I don't yet understand is:\n",
    "- Is this equally applicable to S3, Kinesis, SQS, Lambda, and other AWS tools? Or is there something specific about EC2, RDS, and Redshift which means the VPC applies to them? (e.g., these are accessible via regular HTTPS requests / they're potentially publicly accessible?)\n",
    "  - [It looks like no](https://docs.aws.amazon.com/glue/latest/dg/vpc-endpoints-s3.html) - VPC is configurable as a firewall for S3 as well. Maybe instead the question is whether we want to (a) have enhanced security like AWS services only talking to each other WITHIN the private AWS network (no public IPs), and (b) if we do want to expose our resources to the public internet, if we want to only allow some IP addresses to access resources but not others.\n",
    "\n",
    "\n",
    "In any case, it might be that the reason we're using VPC for this current Redshift setup is because the [official AWS documentation](https://aws.amazon.com/premiumsupport/knowledge-center/redshift-cluster-private-public/) says that users should set up a VPC security group in order to expose a Redshift port publicly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec2 = boto3.client('ec2',\n",
    "    region_name             = \"us-west-2\",\n",
    "    aws_access_key_id       = aws_key,\n",
    "    aws_secret_access_key   = aws_secret\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_groups = ec2.describe_security_groups(\n",
    "    GroupNames = [\n",
    "        'Redshift_secGroup'\n",
    "    ]\n",
    ")\n",
    "\n",
    "sec_groups\n",
    "redshift_sg_id = sec_groups['SecurityGroups'][0]['GroupId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ec2.create_security_group(\n",
    "    Description = \"Security Group for allowing all access to Redshift cluster\",\n",
    "    GroupName = \"Redshift_secGroup\"\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vpc = ec2.authorize_security_group_ingress(\n",
    "    CidrIp = '0.0.0.0/0', #Allowing permission to access from any IP\n",
    "    FromPort = 5439, #Default port for Redshift\n",
    "    ToPort = 5439,\n",
    "    IpProtocol = 'TCP',\n",
    "    GroupId = redshift_sg_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Creating Redshift cluster\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "redshift = boto3.client('redshift',\n",
    "    region_name             = \"us-west-2\",\n",
    "    aws_access_key_id       = aws_key,\n",
    "    aws_secret_access_key   = aws_secret\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Documentation: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/redshift.html#Redshift.Client.create_cluster\n",
    "redshift_response = redshift.create_cluster(\n",
    "    ClusterType = \"multi-node\",\n",
    "    NodeType = 'dc2.large',\n",
    "    NumberOfNodes = 4,\n",
    "    DBName = \"my_redshift_db\",\n",
    "    ClusterIdentifier = 'redshift-cluster-2',\n",
    "    MasterUsername = redshift_user,\n",
    "    MasterUserPassword = redshift_password,\n",
    "    IamRoles = [role_arn],\n",
    "    PubliclyAccessible = True,\n",
    "    VpcSecurityGroupIds = [\n",
    "        redshift_sg_id\n",
    "    ]\n",
    ")\n",
    "\n",
    "'''\n",
    "WARNING! After running this code, you WILL create a Redshift cluster. Be sure to delete it to not incur costs!!\n",
    "'''\n",
    "\n",
    "redshift_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = redshift.describe_clusters()\n",
    "redshift_endpoint = clusters['Clusters'][0]['Endpoint']\n",
    "db_name = clusters['Clusters'][0]['DBName']\n",
    "cluster_id = clusters['Clusters'][0]['ClusterIdentifier']\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = redshift.delete_cluster(\n",
    "    ClusterIdentifier = cluster_id,\n",
    "    SkipFinalClusterSnapshot=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Creating S3 Bucket\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3',\n",
    "    region_name             = \"us-west-2\",\n",
    "    aws_access_key_id       = aws_key,\n",
    "    aws_secret_access_key   = aws_secret\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#This command is telling me my bucket name is invalid even though it is not. Not sure why:\n",
    "\n",
    "s3_response = s3.create_bucket(\n",
    "    Bucket = \"whyWontBucketWork-udacitycourse\",\n",
    "    CreateBucketConfiguration = {\n",
    "        'LocationConstraint':'us-west-2'\n",
    "    },\n",
    "    \n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_resource = boto3.resource('s3',\n",
    "    aws_access_key_id       = aws_key,\n",
    "    aws_secret_access_key   = aws_secret\n",
    ")\n",
    "bucket = s3_resource.Bucket(\"udacitybucket17\") #Bucket I made manually previously\n",
    "\n",
    "#Iterate over files in a bucket:\n",
    "bucket_data = bucket.objects.all()\n",
    "for file in bucket_data:\n",
    "    print(file)\n",
    "\n",
    "#Alternatively:\n",
    "bucket_data = bucket.objects.filter(Prefix = \"AWS_\")\n",
    "for file in bucket_data:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Attempt to connect to Redshift cluster:\n",
    "\n",
    "---\n",
    "\n",
    "At this point we have:\n",
    "- Created a redshift cluster, with an IAM role whose sole policy is 'AmazonS3ReadOnlyAccess'\n",
    "- Specified a security group which allows access to port 5439 from any IP address.\n",
    "\n",
    "What I think is missing though is: making sure Redshift is using our security group we set up (and not the default security group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address = redshift_endpoint['Address']\n",
    "port = redshift_endpoint['Port']\n",
    "conn_string = f\"postgresql://{redshift_user}:{redshift_password}@{address}:{port}/{db_name}\"\n",
    "\n",
    "%sql $conn_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dev:***@redshift-cluster-2.cakcgemszurv.us-west-2.redshift.amazonaws.com:5439/my_redshift_db\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "%%sql \n",
    "\n",
    "select oid as database_id,\n",
    "       datname as database_name,\n",
    "       datallowconn as allow_connect\n",
    "from pg_database\n",
    "order by oid;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT current_database();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
