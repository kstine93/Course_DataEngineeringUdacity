{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import isnan, count, when, col, desc, udf, col, sort_array, asc, avg\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "from pyspark.sql import Window\n",
    "\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note that Spark only allows one Spark context and one Spark session to be defined at any time.\n",
    "#In the code below, 'GetOrCreate' will either create the Spark session or modify the existing one.\n",
    "\n",
    "sparkSesh = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"app Name\") \\\n",
    "    .config('config option','config value') \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at parameters of Spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSesh.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing a basic file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_path = \"./sparkify_log_small.json\"\n",
    "log_data = sparkSesh.read.json(read_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log_data.head()\n",
    "#log_data.take(2)\n",
    "\n",
    "#log_data.schema()\n",
    "log_data.describe() #Similar to 'str' function (structure) in R for describing data frames\n",
    "\n",
    "#log_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drilling down into particular columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data.select(\"artist\").show()\n",
    "log_data.select(\"artist\").dropDuplicates().sort('artist').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More advanced drilldown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_data \\\n",
    "#     .where(log_data.artist != \"null\") \\\n",
    "#     .groupBy('artist') \\\n",
    "#     .count() \\\n",
    "#     .orderBy('count', ascending=False) \\\n",
    "#     .show()\n",
    "\n",
    "\n",
    "log_data \\\n",
    "    .select([\"userId\",\"page\",\"song\"]) \\\n",
    "    .where(log_data.userId == \"1046\") \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using custom function to create a new column in data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_hour = udf(lambda x: datetime.datetime.fromtimestamp(float(x) / 1000.0). hour)\n",
    "\n",
    "#Note: this will not evaluate until data is called (lazy evaluation)\n",
    "log_data = log_data.withColumn(\"hour\", get_hour(log_data.ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering sessions events by when users choose 'nextSong', and looking at the hour during which that happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_hour_view = log_data \\\n",
    "    .filter(log_data.page == \"NextSong\") \\\n",
    "    .groupBy(log_data.hour) \\\n",
    "    .count() \\\n",
    "    .orderBy(log_data.hour.cast(\"float\"))\n",
    "\n",
    "song_hour_view.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting to Pandas data frame\n",
    ">(Question: Why use pandas over spark?) Maybe to use matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_song_hour_view = song_hour_view.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pd_song_hour_view['hour'],pd_song_hour_view['count'])\n",
    "plt.xlim(-1,24)\n",
    "plt.ylim(0,1.2 * max(pd_song_hour_view['count']))\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.ylabel(\"# Songs played\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will drop any records where 'userId' or 'sessionId' is missing\n",
    "clean_log_data = log_data.dropna(how = \"any\", subset = [\"userId\",\"sessionId\"])\n",
    "\n",
    "#Clean out any fields where userId is an empty string\n",
    "clean_log_data = clean_log_data.filter(clean_log_data[\"userId\"] != \"\")\n",
    "\n",
    "clean_log_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Analysis: users who have upgraded their service\n",
    "We'll take a look at the events before and after users decided to upgrade their subscriptions to try and find out why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple function for creating a column to flag downgrade events\n",
    "flag_upgrade_event = udf(lambda x: 1 if x == \"Submit Upgrade\" else 0, IntegerType())\n",
    "\n",
    "clean_log_data = clean_log_data.withColumn(\"upgrade\", flag_upgrade_event(\"page\"))\n",
    "clean_log_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definig a window function for partitioning data based on userId and ordering by time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowVal = Window \\\n",
    "    .partitionBy(\"userId\") \\\n",
    "    .orderBy(desc('ts')) \\\n",
    "    .rangeBetween(Window.unboundedPreceding,0) #This including all PREVIOUS rows, but no rows after criteria.\n",
    "\n",
    "clean_log_data = clean_log_data.withColumn(\"phase\", Fsum(\"upgrade\").over(windowVal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_log_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's find a random customer who downgraded to try this on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_log_data \\\n",
    "    .select(['userId']) \\\n",
    "    .where(clean_log_data.page == \"Submit Upgrade\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_log_data \\\n",
    "    .select(['userId','firstname','ts','page','level','phase']) \\\n",
    "    .where(clean_log_data.userId == \"1232\") \\\n",
    "    .sort('ts') \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write data out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is cool - check out how Spark saves the file. For me, it's actually not a CSV at all, it's a FOLDER\n",
    "#which has some metadata files and then multiple partitioned files. This is pretty cool - Spark is automatically\n",
    "#partitioning my files, but keeping the abstraction very basic on my level.\n",
    "\n",
    "write_path = \"./sparkify_log_small.csv\"\n",
    "clean_log_data.write.mode(saveMode=\"overwrite\").csv(write_path)\n",
    "#clean_log_data.write.save(write_path, format = \"csv\", header = True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Spark Commands & Notes:\n",
    "- `Where()` (alias for `filter()`) filters rows given a certain condition\n",
    "- Spark SQL offers aggregation commands like `count()`, `min()`, `max()`, `avg()`, and `countDistinct()`\n",
    "  - You can also use the `agg()` command and specify multiple types of aggregations like this: `agg({\"salary\":\"avg\", \"age\":\"max})`\n",
    "- Window functions are ways of combining the values of *ranges* of rows in a dataframe. When defining the window, we can choose how to sort and group (within the `partitionBy` method) the rows and how wide of a window we'd like to use (described by `rangeBetween` or `rowsBetween`)\n",
    "- [PySpark User Guide](https://spark.apache.org/docs/latest/api/python/user_guide/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which page did user id \"\" (empty string) NOT visit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Get ALL pages\n",
    "pages = log_data.select(\"page\") \\\n",
    "    .dropDuplicates() \\\n",
    "    .toPandas()['page']\n",
    "\n",
    "pages = list(pages)\n",
    "\n",
    "#2. Get pages where criteria\n",
    "emptyId_pages = log_data.select(\"page\") \\\n",
    "    .filter(\"userId = ''\") \\\n",
    "    .dropDuplicates() \\\n",
    "    .toPandas()['page']\n",
    "\n",
    "emptyId_pages = list(emptyId_pages) #Converting to list\n",
    "\n",
    "#3. Get non-matches:\n",
    "list(filter(lambda x: x not in emptyId_pages, pages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What type of user does the empty string user id most likely refer to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data \\\n",
    "    .select([\"firstname\",\"level\",\"page\"]) \\\n",
    "    .filter(\"userId = ''\") \\\n",
    "    .show()\n",
    "\n",
    "#Looks like users who have not yet logged in to the platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many female users do we have in the data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data \\\n",
    "    .select([\"gender\",\"userId\"]) \\\n",
    "    .dropDuplicates() \\\n",
    "    .groupBy(log_data['gender']) \\\n",
    "    .count() \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many songs were played from the most played artist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data \\\n",
    "    .select(\"artist\") \\\n",
    "    .groupBy(log_data['artist']) \\\n",
    "    .count() \\\n",
    "    .orderBy(desc(\"count\")) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many songs do users listen to on average between visiting our home page? Please round your answer to the closest integer.\n",
    ">Note: Code below is primarily from Udacity instructors with some small changes by me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step #1: Creating Ulog_data to mark the boundaries of window function (i.e., is the record \"Home\"?)\n",
    "ishome = udf(lambda ishome : int(ishome == 'Home'), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step #2: Defining window as needing to partition on userId (so we only look at sessions within each user)\n",
    "#then order by timestamp (so we can chronologically see session events)\n",
    "user_window = Window \\\n",
    "    .partitionBy('userID') \\\n",
    "    .orderBy(asc('ts')) \\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "#Note: More information on 'RangeBetween' here:\n",
    "# https://spark.apache.org/docs/3.2.0/api/python/reference/api/pyspark.sql.Window.rangeBetween.html\n",
    "#the window is a 'traveling' window which progresses through the data set according to the variable(s) in the 'orderBy' clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3: Cool! Now we're creating a subset of the data frame with ONLY the events we care about (home visits OR songplays)\n",
    "#This negates the need to somehow mark songplays - if it's not home, we know what it is.\n",
    "#The 'homevisit' column is marking (with 0 or 1) whether the row corresponds to a home visit. The 'period' column then uses\n",
    "#this to create mini session intervals in-between home visits. Every time that 'homevisit' is 1 (and we have reached the homepage\n",
    "# again), 'period' increments by 1. This means that for each user, their entire session will be broken up according to how many\n",
    "# times they visit 'home', with each visit meaning that all subsequent page visits will have a unique ID (1,1,1,HOME,2,2,HOME,3,\n",
    "# HOME, etc...)\n",
    "#This is helpful because it allows us to then group by this new identifier and aggregate within each of these mini-sessions.\n",
    " \n",
    "cusum = log_data.filter((log_data.page == 'NextSong') | (log_data.page == 'Home')) \\\n",
    "    .select('userID', 'page', 'ts') \\\n",
    "    .withColumn('homevisit', ishome(col('page'))) \\\n",
    "    .withColumn('period', Fsum('homevisit').over(user_window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|avg(count(period))|\n",
      "+------------------+\n",
      "| 5.956678700361011|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Step 4: Now we have the 'period' column which has incrementing unique ids (1,1,1,2,2,3,etc.) that show the boundaries of when\n",
    "#users visited 'home' during a session. Since these periods are all unique, it's possible to simply 'group' by these periods,\n",
    "#and then aggregate over them.\n",
    "#For our purposes, we just want to know how many 'nextSong' events took place in-between home visits, so we can do the following:\n",
    "#1. First, filter only for 'nextSong' pages so we don't count 'Home' events\n",
    "#2. Group our data by userID and THEN period (period only unique within userID)\n",
    "#3. Count the number of events within these groupings. Due to our work, this will equate to the number of 'nextSong' events\n",
    "#4. Now we have the count of these events between home visits within each session, for each user. Now we average them all.\n",
    "cusum.filter((cusum.page == 'NextSong')) \\\n",
    "    .groupBy('userID', 'period') \\\n",
    "    .agg({'period':'count'}) \\\n",
    "    .agg({'count(period)':'avg'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------------+---------+------+\n",
      "|userID|    page|           ts|homevisit|period|\n",
      "+------+--------+-------------+---------+------+\n",
      "|  1232|    Home|1513727268284|        1|     1|\n",
      "|  1232|NextSong|1513727589284|        0|     1|\n",
      "|  1232|NextSong|1513727885284|        0|     1|\n",
      "|  1232|NextSong|1513728173284|        0|     1|\n",
      "|  1232|NextSong|1513728346284|        0|     1|\n",
      "|  1232|NextSong|1513728548284|        0|     1|\n",
      "|  1232|NextSong|1513738827284|        0|     1|\n",
      "|  1232|NextSong|1513739064284|        0|     1|\n",
      "|  1232|    Home|1513739156284|        1|     2|\n",
      "|  1232|NextSong|1513739286284|        0|     2|\n",
      "|  1232|NextSong|1513739552284|        0|     2|\n",
      "|  1232|NextSong|1513739756284|        0|     2|\n",
      "|  1232|NextSong|1513740075284|        0|     2|\n",
      "|  1232|NextSong|1513740198284|        0|     2|\n",
      "|  1232|NextSong|1513740448284|        0|     2|\n",
      "|  1232|NextSong|1513740707284|        0|     2|\n",
      "|  1232|NextSong|1513740934284|        0|     2|\n",
      "|  1232|NextSong|1513741251284|        0|     2|\n",
      "|  1232|NextSong|1513741461284|        0|     2|\n",
      "|  1232|NextSong|1513741657284|        0|     2|\n",
      "+------+--------+-------------+---------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cusum.filter(cusum.userID == '1232').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
