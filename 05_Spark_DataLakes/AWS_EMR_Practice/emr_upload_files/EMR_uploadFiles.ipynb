{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script to upload files to EMR cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Module import\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Credential Upload\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/rambino/.aws/credentials']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#AWS Credentials\n",
    "aws_path = \"/home/rambino/.aws/credentials\"\n",
    "aws_cred = configparser.ConfigParser()\n",
    "aws_cred.read(aws_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Uploading data files to S3\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3',\n",
    "    region_name             = \"us-east-1\",\n",
    "    aws_access_key_id       = aws_cred['default']['aws_access_key_id'],\n",
    "    aws_secret_access_key   = aws_cred['default']['aws_secret_access_key']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucketName = 'emr-input-output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'MFZNCTA4V619TW2P',\n",
       "  'HostId': 'xictsv1/1GglA/aH5tCI8WrWM7Xpu1g4XmPcP5Zw11J2xDuA6+O5Lz5M/r1zPJkNQv+IfZ5AoPI=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'xictsv1/1GglA/aH5tCI8WrWM7Xpu1g4XmPcP5Zw11J2xDuA6+O5Lz5M/r1zPJkNQv+IfZ5AoPI=',\n",
       "   'x-amz-request-id': 'MFZNCTA4V619TW2P',\n",
       "   'date': 'Wed, 14 Sep 2022 15:26:36 GMT',\n",
       "   'location': 'http://emr-input-output.s3.amazonaws.com/',\n",
       "   'server': 'AmazonS3',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0},\n",
       " 'Location': 'http://emr-input-output.s3.amazonaws.com/'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = s3.create_bucket(\n",
    "    ACL='public-read-write',\n",
    "    Bucket=bucketName,\n",
    "    CreateBucketConfiguration={\n",
    "        'LocationConstraint': \"us-east-2\"\n",
    "    }\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uploading files to bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuring location of resources:\n",
    "s3_input_path = \"input/cities.csv\"\n",
    "s3_output_path = \"output/\"\n",
    "s3_config_path = \"config/testPaths.cfg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input data\n",
    "\n",
    "with open('cities.csv', 'rb') as data:\n",
    "    response = s3.upload_fileobj(data, bucketName,s3_input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjusting config file to reflect input + output data path:\n",
    "cfg_path = 'testPaths.cfg'\n",
    "\n",
    "path = configparser.ConfigParser()\n",
    "path.read(cfg_path)\n",
    "\n",
    "path['PATHS']['input'] = s3_input_path\n",
    "path['PATHS']['output'] = s3_output_path\n",
    "\n",
    "#Saving Config file:\n",
    "with open(cfg_path,\"w\") as file:\n",
    "    path.write(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Config data\n",
    "with open('testPaths.cfg', 'rb') as data:\n",
    "    response = s3.upload_fileobj(data, bucketName,s3_config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Uploading script to EMR\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all of my resources are in S3, I just need to upload my python script to my EMR cluster and submit the Spark job.\n",
    "\n",
    "I will first need to connect (via SSH) to the EMR master node, and then upload my python script using this command (run from directory with the python file)\n",
    "\n",
    "```\n",
    "scp -i /home/rambino/.aws/spark_keypair.pem /home/rambino/dev/DataEngineering_Udacity/05_Spark_DataLakes/AWS_EMR_Practice/emr_upload_files/emr_sparkTest.py hadoop@[EC2-INSTANCE-DNS-HERE]:/home/hadoop\n",
    "```\n",
    "\n",
    "Next, I'll need to SSH into the EMR master node and:\n",
    "1. First figure out where the `spark-submit` command is by running `which spark-submit`\n",
    "2. Run this command on my new script and see the output (substituting spark-submit path as needed):\n",
    "\n",
    "`/usr/bin/spark-submit --master yarn emr_sparkTest.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Post-exercise Notes\n",
    "1. It is not seemingly possible to configure the master EMR node from within SSH (I tried to install configparser as a Python module, but got 'permission denied' errors)\n",
    "2. If I run this code again, I will need to either:\n",
    "   1. Set up cluster such that configparser is installed in the cluster or:\n",
    "   2. change code so that it does not require this package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Using HDFS instead of S3\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While S3 is very convenient, there might be times when using HDFS is better. EMR clusters always come equipped with HDFS installed, and there is simply some setup required to use this system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Firstly, we'll need to copy files (using 'scp' command) to the EMR master node like we did with the .pem file or the python script we ran on EMR.\n",
    "\n",
    "Next, we'll make a new HDFS directory called `/data` and copy our local files into it:\n",
    "\n",
    "```\n",
    "hdfs dfs -mkdir /data\n",
    "\n",
    "hdfs dfs -copyFromLocal cities.csv /data/\n",
    "```\n",
    "\n",
    "Now the data is within the hadoop file system, we only need to change the URL for the data in our python script and we can access the HDFS files just as easily as we did S3 files! (See accompanying python scripts for syntax)\n",
    "\n",
    ">Don't forget that you can access the UI for HDFS on your cluster by following the port-forwarding commands in the `EMR_boto3Setup` notebook and using **Port 50070** (for an EMR notebook v5.28). See the Python notebook for a list of other ports if using a different EMR version\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
