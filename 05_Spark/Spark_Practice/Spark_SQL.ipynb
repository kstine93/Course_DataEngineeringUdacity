{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark SQL Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import isnan, count, when, col, desc, udf, col, sort_array, asc, avg\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "from pyspark.sql import Window\n",
    "\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: config option\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/27 16:45:51 WARN Utils: Your hostname, rambino-AERO-15-XD resolves to a loopback address: 127.0.1.1; using 192.168.2.54 instead (on interface wlp48s0)\n",
      "23/08/27 16:45:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/27 16:45:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "sparkSesh = (\n",
    "    SparkSession.builder.appName(\"app Name\")\n",
    "    .config(\"config option\", \"config value\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_path = \"./sparkify_log_small.json\"\n",
    "log_data = sparkSesh.read.json(read_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Dataframes by default do not support direct SQL querying (apparently), so we need to create a 'view'\n",
    "\n",
    "log_data.createOrReplaceTempView(\"log_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+---------+------+-------------+---------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "|              artist|     auth|firstName|gender|itemInSession| lastName|   length|level|            location|method|    page| registration|sessionId|                song|status|           ts|           userAgent|userId|\n",
      "+--------------------+---------+---------+------+-------------+---------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "|       Showaddywaddy|Logged In|  Kenneth|     M|          112| Matthews|232.93342| paid|Charlotte-Concord...|   PUT|NextSong|1509380319284|     5132|Christmas Tears W...|   200|1513720872284|\"Mozilla/5.0 (Win...|  1046|\n",
      "|          Lily Allen|Logged In|Elizabeth|     F|            7|    Chase|195.23873| free|Shreveport-Bossie...|   PUT|NextSong|1512718541284|     5027|       Cheryl Tweedy|   200|1513720878284|\"Mozilla/5.0 (Win...|  1000|\n",
      "|Cobra Starship Fe...|Logged In|     Vera|     F|            6|Blackwell|196.20526| paid|          Racine, WI|   PUT|NextSong|1499855749284|     5516|Good Girls Go Bad...|   200|1513720881284|\"Mozilla/5.0 (Mac...|  2219|\n",
      "+--------------------+---------+---------+------+-------------+---------+---------+-----+--------------------+------+--------+-------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkSesh.sql(\n",
    "    \"\"\"\n",
    "    SELECT * FROM log_table limit 3;\n",
    "    \"\"\"\n",
    ").show()\n",
    "\n",
    "# Note: For more information on \"show()\" vs \"collect()\" vs \"take()\" in Spark for returning data:\n",
    "# https://stackoverflow.com/questions/41000273/spark-difference-between-collect-take-and-show-outputs-after-conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(x)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: If you want to use UDFs with Spark SQL, you have a slightly different syntax:\n",
    "sparkSesh.udf.register(\n",
    "    \"hour_from_epoch\", lambda x: int(datetime.datetime.fromtimestamp(x / 1000).hour)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+\n",
      "|hour|plays_per_hour|\n",
      "+----+--------------+\n",
      "|   0|           147|\n",
      "|   1|           225|\n",
      "|   2|           216|\n",
      "|   3|           179|\n",
      "|   4|           141|\n",
      "|   5|           151|\n",
      "|   6|           113|\n",
      "|   7|           180|\n",
      "|   8|            93|\n",
      "|  23|           205|\n",
      "+----+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sparkSesh.sql(\n",
    "    \"\"\"\n",
    "    SELECT hour_from_epoch(ts) AS hour, count(*) as plays_per_hour\n",
    "    FROM log_table\n",
    "    WHERE page = \"NextSong\"\n",
    "    GROUP BY hour\n",
    "    ORDER BY cast(hour as int) ASC\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "It's quite easy to convert Spark dataframes to Pandas dataframes with the `toPandas()` method. Using Pandas or Spark dataframes is mostly a matter of preference.\n",
    "\n",
    "However, the course instructor recommends that **I choose one API (Pandas or Spark / Spark SQL) and practice it consistently** rather than trying to learn a little about all the APIs (i.e., specialize)\n",
    "\n",
    ">It makes most sense to me to specialize in Spark / Spark SQL. I see it used more often for Data Engineering than Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which page did user id \"\" (empty string) NOT visit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|          page|\n",
      "+--------------+\n",
      "|     Downgrade|\n",
      "|        Logout|\n",
      "| Save Settings|\n",
      "|      Settings|\n",
      "|      NextSong|\n",
      "|       Upgrade|\n",
      "|         Error|\n",
      "|Submit Upgrade|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkSesh.sql(\n",
    "    \"\"\"\n",
    "    SELECT DISTINCT page\n",
    "    FROM log_table\n",
    "    WHERE page NOT IN (\n",
    "        SELECT DISTINCT page\n",
    "        FROM log_table\n",
    "        WHERE userID = \"\"\n",
    "    )\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many female users do we have in the data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------+\n",
      "|count(DISTINCT userId)|gender|\n",
      "+----------------------+------+\n",
      "|                   127|     F|\n",
      "|                     1|  null|\n",
      "|                   155|     M|\n",
      "+----------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkSesh.sql(\n",
    "    \"\"\"\n",
    "    SELECT COUNT(DISTINCT userId), gender\n",
    "    FROM log_table\n",
    "    GROUP BY gender\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many songs were played from the most played artist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|plays|              artist|\n",
      "+-----+--------------------+\n",
      "|   17|       Kings Of Leon|\n",
      "|   16|            Coldplay|\n",
      "|   15|Florence + The Ma...|\n",
      "|   13|        Jack Johnson|\n",
      "|   10|            BjÃÂ¶rk|\n",
      "|   10|       Justin Bieber|\n",
      "|   10|      The Black Keys|\n",
      "|    9|          Lily Allen|\n",
      "|    9|           Daft Punk|\n",
      "|    9|            Tub Ring|\n",
      "|    8|         OneRepublic|\n",
      "|    7|           Radiohead|\n",
      "|    7|     Alliance Ethnik|\n",
      "|    6|        Taylor Swift|\n",
      "|    6|          Kanye West|\n",
      "|    6|             Rihanna|\n",
      "|    6|         Miley Cyrus|\n",
      "|    6|     Michael Jackson|\n",
      "|    6|Red Hot Chili Pep...|\n",
      "|    6|      Arctic Monkeys|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkSesh.sql(\n",
    "    \"\"\"\n",
    "    SELECT COUNT(*) AS plays, artist\n",
    "    FROM log_table\n",
    "    WHERE artist IS NOT NULL\n",
    "    GROUP BY artist\n",
    "    ORDER BY plays DESC\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many songs do users listen to on average between visiting our home page? Please round your answer to the closest integer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSesh.sql(\n",
    "    \"\"\"\n",
    "    WITH songs_home AS (\n",
    "        SELECT *\n",
    "        ,SUM(isHome) OVER \n",
    "            (PARTITION BY userID ORDER BY ts DESC ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING) AS period\n",
    "        FROM (\n",
    "            SELECT page\n",
    "            ,userID\n",
    "            ,ts\n",
    "            ,CASE WHEN page = 'Home' THEN 1 ELSE 0 END AS isHome\n",
    "            FROM log_table\n",
    "            WHERE page = \"Home\" OR page = \"NextSong\"\n",
    "        )\n",
    "    )\n",
    "    SELECT COUNT(period) AS count, userID, period, page\n",
    "    FROM songs_home\n",
    "    GROUP BY userID, period, page\n",
    "    HAVING page = 'NextSong'\n",
    "    \"\"\"\n",
    ").createOrReplaceTempView(\"songs_home_counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|       avg(count)|\n",
      "+-----------------+\n",
      "|5.625925925925926|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkSesh.sql(\n",
    "    \"\"\"\n",
    "    SELECT AVG(count)\n",
    "    FROM songs_home_counts\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructor's Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT CASE WHEN 1 > 0 THEN 1 WHEN 2 > 0 THEN 2.0 ELSE 1.2 END;\n",
    "is_home = sparkSesh.sql(\n",
    "    \"\"\"\n",
    "    SELECT userID\n",
    "    ,page\n",
    "    ,ts\n",
    "    ,CASE WHEN page = 'Home' THEN 1 ELSE 0 END AS is_home\n",
    "    FROM log_table\n",
    "    WHERE (page = 'NextSong') or (page = 'Home')\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# keep the results in a new view\n",
    "is_home.createOrReplaceTempView(\"is_home_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the cumulative sum over the is_home column\n",
    "cumulative_sum = sparkSesh.sql(\n",
    "    \"\"\"\n",
    "    SELECT *\n",
    "    ,SUM(is_home) OVER\n",
    "        (PARTITION BY userID ORDER BY ts ASC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS period\n",
    "    FROM is_home_table\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# keep the results in a view\n",
    "cumulative_sum.createOrReplaceTempView(\"period_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|avg(count_results)|\n",
      "+------------------+\n",
      "| 5.625925925925926|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find the average count for NextSong\n",
    "sparkSesh.sql(\n",
    "    \"\"\"\n",
    "    SELECT AVG(count_results)\n",
    "    FROM\n",
    "        (SELECT COUNT(*) AS count_results FROM period_table\n",
    "        GROUP BY userID, period, page HAVING page = 'NextSong') AS counts\n",
    "\"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bug: Different results when sorting differently\n",
    "\n",
    "Note: I get different results if I partition and order data DESCENDING vs. ASCENDING within my window function.\n",
    "I *think* I figured out why this is: it's because some of my timestamps are the same for 'HOME' page visits and 'NextPage' visits.\n",
    "For example, with ID 1079, this is the case. When I order in one direction, the 'NextPage' event is ordered BEFORE the 'Home' visit. When I order in the other way, it is put AFTER the 'Home' visit.\n",
    "\n",
    "This is because SQL decides to resolve the ambiguous sorting by choosing another column to sort by: page.\n",
    "When it sorts page in ascending order, this resolves differently than when it sorts in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-------------+\n",
      "|count|userID|           ts|\n",
      "+-----+------+-------------+\n",
      "|    2|  1955|1513735032284|\n",
      "|    2|  2047|1513755094284|\n",
      "|    2|  2219|1513723220284|\n",
      "|    2|  1079|1513749231284|\n",
      "|    2|  2813|1513754919284|\n",
      "|    2|  1138|1513729066284|\n",
      "|    2|  2089|1513748348284|\n",
      "+-----+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This code shows that there are a lot of user sessions where 2 events share the same timestamp\n",
    "sparkSesh.sql(\n",
    "    \"\"\"\n",
    "    SELECT COUNT(ts) AS count, userID, ts\n",
    "    FROM log_table\n",
    "    GROUP BY userID, ts\n",
    "    HAVING count > 1\n",
    "    ORDER BY count DESC\n",
    "\"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solved\n",
    "If we clean out data from these IDs where the timestamp is exactly the same for 2 or more events, we can see that we now no longer get different answers depending on if we sort ASC or DESC\n",
    ">Conclusion: If these were production data, I would challenge the event-logging system which is producing events with the exact same timestamp. It doesn't make sense to me how two events like visiting 'Home' and 'NextSong' could happen simultaneously. If that event-logging system cannot be changed, then I would need to be more careful with my code so that it acknowledges that two events can happen at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "badIDs = sparkSesh.sql(\n",
    "    \"\"\"\n",
    "    SELECT COUNT(ts) AS count, userID, ts\n",
    "    FROM log_table\n",
    "    GROUP BY userID, ts\n",
    "    HAVING count > 1\n",
    "    ORDER BY count DESC\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "badIDs.createOrReplaceTempView(\"badIDs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reloading data with 'bad IDs' filtered out. Rerun code with this data to compare how sorting 'ts' differently doesn't make\n",
    "# a difference anymore.\n",
    "\n",
    "log_table = sparkSesh.sql(\n",
    "    \"\"\"\n",
    "    SELECT *\n",
    "    FROM log_table\n",
    "    WHERE userID NOT IN (SELECT userID from badIDs)\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "log_table.createOrReplaceTempView(\"log_table\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
