{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMR Setup with Python SDK (boto3)\n",
    "This notebook will show how to set up some AWS resources using the Python SDK for AWS, boto3.\n",
    "\n",
    "Boto3 Documentation: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/redshift.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/04 19:18:20 WARN Utils: Your hostname, rambino-AERO-15-XD resolves to a loopback address: 127.0.1.1; using 192.168.0.234 instead (on interface wlp48s0)\n",
      "22/10/04 19:18:20 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/rambino/.local/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/rambino/.ivy2/cache\n",
      "The jars for the packages stored in: /home/rambino/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-72b3acfb-237f-441a-ae1c-1b7c35cc41a5;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 117ms :: artifacts dl 3ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-72b3acfb-237f-441a-ae1c-1b7c35cc41a5\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/3ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/04 19:18:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "aws_key      = aws_cred['default']['aws_access_key_id']\n",
    "aws_secret   = aws_cred['default']['aws_secret_access_key']\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.jars.packages\",\"com.amazonaws:aws-java-sdk-s3:1.12.311\") \\\n",
    "    .config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:3.3.4\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setSystemProperty('com.amazonaws.services.s3.enableV4','true')\n",
    "\n",
    "sc._jsc.hadoopConfiguration().set('fs.s3a.access.key',aws_key)\n",
    "sc._jsc.hadoopConfiguration().set('fs.s3a.secret.key',aws_secret)\n",
    "sc._jsc.hadoopConfiguration().set('spark.hadoop.fs.s3a.bucket.all.committer.magic.enabled', 'true')\n",
    "sc._jsc.hadoopConfiguration().set('fs.s3a.endpoint','s3-us-west-2.amazonaws.com')\n",
    "sc._jsc.hadoopConfiguration().set('fs.s3a.connection.ssl.enabled','true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, StringType, FloatType, StructType, StructField\n",
    "\n",
    "song_schema = StructType([\n",
    "    StructField('num_songs',IntegerType(),True),\n",
    "    StructField('artist_id',StringType(),True),\n",
    "    StructField('artist_latitude',FloatType(),True),\n",
    "    StructField('artist_longitude',FloatType(),True),\n",
    "    StructField('artist_location',StringType(),True),\n",
    "    StructField('artist_name',StringType(),True),\n",
    "    StructField('song_id',StringType(),True),\n",
    "    StructField('title',StringType(),True),\n",
    "    StructField('duration',FloatType(),True),\n",
    "    StructField('year',IntegerType(),True)\n",
    "])\n",
    "\n",
    "log_schema = StructType([\n",
    "    StructField('artist',StringType(),True),\n",
    "    StructField('auth',StringType(),True),\n",
    "    StructField('firstName',StringType(),True),\n",
    "    StructField('gender',StringType(),True),\n",
    "    StructField('itemInSession',IntegerType(),True),\n",
    "    StructField('lastName',StringType(),True),\n",
    "    StructField('length',IntegerType(),True),\n",
    "    StructField('level',StringType(),True),\n",
    "    StructField('location',StringType(),True),\n",
    "    StructField('method',StringType(),True),\n",
    "    StructField('page',StringType(),True),\n",
    "    StructField('registration',StringType(),True),\n",
    "    StructField('sessionId',IntegerType(),True),\n",
    "    StructField('song',StringType(),True),\n",
    "    StructField('status',IntegerType(),True),\n",
    "    StructField('ts',FloatType(),True),\n",
    "    StructField('userAgent',StringType(),True),\n",
    "    StructField('userId',StringType(),True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df = spark.read.format('json').schema(song_schema).load('s3a://udacity-dend/song_data/*/*/*')#/A/B/C/TRABCEI128F424C983.json')\n",
    "log_df = spark.read.format('json').schema(log_schema).load('s3a://udacity-dend/log_data/*/*')#/2018/11/2018-11-12-events.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df = log_df.where(\"page = 'NextSong'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from datetime import datetime\n",
    "\n",
    "get_hour = udf(lambda x: x.hour)\n",
    "get_day = udf(lambda x: x.day)\n",
    "get_week = udf(lambda x: x.isocalendar().week)\n",
    "get_month = udf(lambda x: x.month)\n",
    "get_year = udf(lambda x: x.year)\n",
    "get_weekday = udf(lambda x: x.weekday())\n",
    "\n",
    "get_datetime = udf(lambda x: datetime.fromtimestamp(x/1000))\n",
    "log_df = log_df.withColumn('timestamp',get_datetime('ts'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract columns to create time table\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "log_df = log_df \\\n",
    "    .withColumn('songplay_id',F.expr(\"uuid()\")) \\\n",
    "    .withColumn('hour',get_hour('timestamp')) \\\n",
    "    .withColumn('day',get_day('timestamp')) \\\n",
    "    .withColumn('week',get_week('timestamp')) \\\n",
    "    .withColumn('month',get_month('timestamp')) \\\n",
    "    .withColumn('year',get_year('timestamp')) \\\n",
    "    .withColumn('weekday',get_weekday('timestamp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplays_table = songplays_table \\\n",
    "    .withColumn('songplay_id',F.expr(\"uuid()\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#data.withColumn('timestamp',ts_to_timestamp('ts')).show()\n",
    "match_condition = ((log_df.song == song_df.title) & (log_df.artist == song_df.artist_name))\n",
    "songplays_table = log_df.join(song_df, match_condition, \"left\") \\\n",
    "    .select(\n",
    "        log_df.ts, log_df.userId, log_df.level, log_df.sessionId,\n",
    "        log_df.location, log_df.userAgent, log_df.month, log_df.year,\n",
    "        song_df.song_id, song_df.artist_id\n",
    "    )                \n",
    "\n",
    "\n",
    "#songplays_table.limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_df = songplays_table.filter(\"song_id != 'None'\").limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplays_table \\\n",
    "    .select(\"songplay_id\",\"ts\",\"userId\",\"level\",\"song_id\",\"artist_id\",\"sessionId\",\"location\",\"userAgent\",\"year\",\"month\") \\\n",
    "    .write \\\n",
    "    .option(\"header\",True) \\\n",
    "    .partitionBy(\"year\",\"month\") \\\n",
    "    .csv(\"./_out/\" + \"songplays\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%md\n",
    "\n",
    "To do (Sept. 30):\n",
    "1. I saved the output of a local ETL to the _out folder. Take a look at it and see if the data looks right\n",
    "   1. ~~Why are so many entries missing 'song_id' and 'artist_id'? (333 / 6280)~~\n",
    "   2. ~~Query the data and see what kind of results I get (Compared to one query for Redshift - I GET THE SAME RESULTS!! Looks like I (probably) did it right)~~\n",
    "   3. ~~take a look at double-checks I did for Redshift project - any I should implement here?~~\n",
    "      1. ~~Yes, need: unique **songs, users and artists** - should implement this check in notebook after running ETL locally.~~\n",
    "2. ~~Run the etl.py again with limited data (Nov. 22 has at least 1 match in song + artist - use that?)~~\n",
    "3. Clean up this notebook - should have EMR creation code + code to pull in data from S3 and inspect it.\n",
    "4. Test writing as parquet\n",
    "5. Cleanup\n",
    "   1. Clean etl.py file to not have any errant comments or code. Docstrings in place?\n",
    "   2. Clean EMR_boto3Setup notebook so that testing code is neatly organized or in separate notebook.\n",
    "   3. Delete _out folder with test data\n",
    "6. Finish rest of this notebook to spin up EMR\n",
    "7. Use built-in notebook to run low-data code once\n",
    "8. Upload .py to EMR via SSH and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplays_schema = StructType([\n",
    "    StructField('songplay_id',StringType(),False),\n",
    "    StructField('ts',FloatType(),True),\n",
    "    StructField('userId',StringType(),True),\n",
    "    StructField('level',StringType(),True),\n",
    "    StructField('song_id',StringType(),True),\n",
    "    StructField('artist_id',StringType(),True),\n",
    "    StructField('sessionId',IntegerType(),True),\n",
    "    StructField('location',StringType(),True),\n",
    "    StructField('userAgent',StringType(),True),\n",
    "    StructField('year',IntegerType(),True),\n",
    "    StructField('month',IntegerType(),True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'songplays' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/rambino/dev/DataEngineering_Udacity/Projects/DataLakeWithEMR/EMR_boto3Setup.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/Projects/DataLakeWithEMR/EMR_boto3Setup.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m songplays\u001b[39m.\u001b[39mfilter(\u001b[39m\"\u001b[39m\u001b[39mlevel = \u001b[39m\u001b[39m'\u001b[39m\u001b[39mfree\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'songplays' is not defined"
     ]
    }
   ],
   "source": [
    "songplays.filter(\"level = 'free'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pyspark.sql.functions' has no attribute 'day'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/rambino/dev/DataEngineering_Udacity/Projects/DataLakeWithEMR/EMR_boto3Setup.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/Projects/DataLakeWithEMR/EMR_boto3Setup.ipynb#Y105sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m#year, month, dayofmonth, hour, weekofyear, date_format, desc, to_timestamp\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/Projects/DataLakeWithEMR/EMR_boto3Setup.ipynb#Y105sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/Projects/DataLakeWithEMR/EMR_boto3Setup.ipynb#Y105sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m#make_timestamp = F.udf(lambda x: F.cast(typ=T.TimestampType(),val=x/1000)) #F.udf(lambda x: F.unix_timestamp(x/1000,'dd-MM-yyyy HH:mm:ss.SSS'))\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/Projects/DataLakeWithEMR/EMR_boto3Setup.ipynb#Y105sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/Projects/DataLakeWithEMR/EMR_boto3Setup.ipynb#Y105sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m#songplays.select(to_timestamp('ts')).show()\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/Projects/DataLakeWithEMR/EMR_boto3Setup.ipynb#Y105sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m songplays \u001b[39m=\u001b[39m songplays\u001b[39m.\u001b[39mwithColumn(\u001b[39m'\u001b[39m\u001b[39mtimestamp\u001b[39m\u001b[39m'\u001b[39m,make_timestamp(\u001b[39m'\u001b[39m\u001b[39mts\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/Projects/DataLakeWithEMR/EMR_boto3Setup.ipynb#Y105sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m songplays \\\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/Projects/DataLakeWithEMR/EMR_boto3Setup.ipynb#Y105sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m.\u001b[39mwithColumn(\u001b[39m'\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m'\u001b[39m,F\u001b[39m.\u001b[39mto_timestamp(songplays[\u001b[39m'\u001b[39m\u001b[39mts\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m/\u001b[39m\u001b[39m1000\u001b[39m)) \\\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/Projects/DataLakeWithEMR/EMR_boto3Setup.ipynb#Y105sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m.\u001b[39mwithColumn(\u001b[39m'\u001b[39m\u001b[39mmonth\u001b[39m\u001b[39m'\u001b[39m,F\u001b[39m.\u001b[39mmonth(\u001b[39m'\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m'\u001b[39m)) \\\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/Projects/DataLakeWithEMR/EMR_boto3Setup.ipynb#Y105sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m.\u001b[39mwithColumn(\u001b[39m'\u001b[39m\u001b[39mhour\u001b[39m\u001b[39m'\u001b[39m,F\u001b[39m.\u001b[39;49mday(\u001b[39m'\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m'\u001b[39m)) \\\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/Projects/DataLakeWithEMR/EMR_boto3Setup.ipynb#Y105sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m.\u001b[39mshow()\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pyspark.sql.functions' has no attribute 'day'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "#from pyspark.sql.functions import cast as choohoh\n",
    "from pyspark.sql import types as T\n",
    "#year, month, dayofmonth, hour, weekofyear, date_format, desc, to_timestamp\n",
    "\n",
    "#make_timestamp = F.udf(lambda x: F.cast(typ=T.TimestampType(),val=x/1000)) #F.udf(lambda x: F.unix_timestamp(x/1000,'dd-MM-yyyy HH:mm:ss.SSS'))\n",
    "\n",
    "#songplays.select(to_timestamp('ts')).show()\n",
    "songplays = songplays.withColumn('timestamp',make_timestamp('ts'))\n",
    "songplays \\\n",
    "    .withColumn('date',F.to_timestamp(songplays['ts']/1000)) \\\n",
    "    .withColumn('month',F.month('date')) \\\n",
    "    .withColumn('hour',F.dayofmonth('date')) \\\n",
    "    .show()\n",
    "#songplays.withColumn(\"timestamp\",F.date_format(songplays.ts.cast(dataType=T.TimestampType()), \"yyyy-MM-dd\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_path_prefix = \"./_out/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplays = spark.read \\\n",
    "    .format('csv') \\\n",
    "    .option('header',True) \\\n",
    "    .load(read_path_prefix + \"songplays\")\n",
    "\n",
    "songplays.createOrReplaceTempView('songplays_tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>songplay_id</th>\n",
       "      <th>ts</th>\n",
       "      <th>userId</th>\n",
       "      <th>level</th>\n",
       "      <th>song_id</th>\n",
       "      <th>artist_id</th>\n",
       "      <th>sessionId</th>\n",
       "      <th>location</th>\n",
       "      <th>userAgent</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7dc063ba-685e-497f-9233-5e0bf7f5e92c</td>\n",
       "      <td>1.54226708E12</td>\n",
       "      <td>49</td>\n",
       "      <td>paid</td>\n",
       "      <td>SOCUITT12AB0187A32</td>\n",
       "      <td>ARKS2FE1187B99325D</td>\n",
       "      <td>606</td>\n",
       "      <td>San Francisco-Oakland-Hayward, CA</td>\n",
       "      <td>Mozilla/5.0 (Windows NT 5.1; rv:31.0) Gecko/20...</td>\n",
       "      <td>2018</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>370b1546-146f-44a8-8692-2b2418a2e3a8</td>\n",
       "      <td>1.54227861E12</td>\n",
       "      <td>80</td>\n",
       "      <td>paid</td>\n",
       "      <td>SOSDZFY12A8C143718</td>\n",
       "      <td>AR748W61187B9B6AB8</td>\n",
       "      <td>611</td>\n",
       "      <td>Portland-South Portland, ME</td>\n",
       "      <td>\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4...</td>\n",
       "      <td>2018</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32b383cc-62c8-4f65-9f5e-2cb92f34afab</td>\n",
       "      <td>1.54227887E12</td>\n",
       "      <td>80</td>\n",
       "      <td>paid</td>\n",
       "      <td>SOTNWCI12AAF3B2028</td>\n",
       "      <td>ARS54I31187FB46721</td>\n",
       "      <td>611</td>\n",
       "      <td>Portland-South Portland, ME</td>\n",
       "      <td>\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4...</td>\n",
       "      <td>2018</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b85d7ea5-71ec-4bad-be9b-37586c957d2b</td>\n",
       "      <td>1.54227914E12</td>\n",
       "      <td>80</td>\n",
       "      <td>paid</td>\n",
       "      <td>SOBONKR12A58A7A7E0</td>\n",
       "      <td>AR5E44Z1187B9A1D74</td>\n",
       "      <td>611</td>\n",
       "      <td>Portland-South Portland, ME</td>\n",
       "      <td>\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4...</td>\n",
       "      <td>2018</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54443a43-6805-4035-af07-7fefe8056041</td>\n",
       "      <td>1.54228031E12</td>\n",
       "      <td>80</td>\n",
       "      <td>paid</td>\n",
       "      <td>SOLZOBD12AB0185720</td>\n",
       "      <td>ARPDVPJ1187B9ADBE9</td>\n",
       "      <td>611</td>\n",
       "      <td>Portland-South Portland, ME</td>\n",
       "      <td>\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4...</td>\n",
       "      <td>2018</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15630d35-df92-4218-878d-4f5b75266d8a</td>\n",
       "      <td>1.54228202E12</td>\n",
       "      <td>30</td>\n",
       "      <td>paid</td>\n",
       "      <td>SOULTKQ12AB018A183</td>\n",
       "      <td>ARKQQZA12086C116FC</td>\n",
       "      <td>324</td>\n",
       "      <td>San Jose-Sunnyvale-Santa Clara, CA</td>\n",
       "      <td>Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) G...</td>\n",
       "      <td>2018</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>a477f6b4-4551-465b-8caa-dacd6afb1eba</td>\n",
       "      <td>1.5422849E12</td>\n",
       "      <td>30</td>\n",
       "      <td>paid</td>\n",
       "      <td>SOIOESO12A6D4F621D</td>\n",
       "      <td>ARVLXWP1187FB5B94A</td>\n",
       "      <td>324</td>\n",
       "      <td>San Jose-Sunnyvale-Santa Clara, CA</td>\n",
       "      <td>Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) G...</td>\n",
       "      <td>2018</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>b036aa59-a542-403e-9051-f1caa1e80556</td>\n",
       "      <td>1.54228621E12</td>\n",
       "      <td>30</td>\n",
       "      <td>paid</td>\n",
       "      <td>SOSQIRI12A8C133897</td>\n",
       "      <td>AR1XIHA1187FB4AED3</td>\n",
       "      <td>324</td>\n",
       "      <td>San Jose-Sunnyvale-Santa Clara, CA</td>\n",
       "      <td>Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) G...</td>\n",
       "      <td>2018</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>01ad1f8b-d918-4d66-bbd5-908359ff05cb</td>\n",
       "      <td>1.54228674E12</td>\n",
       "      <td>30</td>\n",
       "      <td>paid</td>\n",
       "      <td>SONZWDK12A6701F62B</td>\n",
       "      <td>ARL4UQB1187B9B74E3</td>\n",
       "      <td>324</td>\n",
       "      <td>San Jose-Sunnyvale-Santa Clara, CA</td>\n",
       "      <td>Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) G...</td>\n",
       "      <td>2018</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4f1bec2c-87a0-453c-9b84-ca925651d0b8</td>\n",
       "      <td>1.54228779E12</td>\n",
       "      <td>30</td>\n",
       "      <td>paid</td>\n",
       "      <td>SOYUKXG12A58A77837</td>\n",
       "      <td>ARZNLRE1187B99B6C3</td>\n",
       "      <td>324</td>\n",
       "      <td>San Jose-Sunnyvale-Santa Clara, CA</td>\n",
       "      <td>Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) G...</td>\n",
       "      <td>2018</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ccdb71f4-8835-4d5d-95be-8bbc1674f80c</td>\n",
       "      <td>1.54228949E12</td>\n",
       "      <td>30</td>\n",
       "      <td>paid</td>\n",
       "      <td>SOBONKR12A58A7A7E0</td>\n",
       "      <td>AR5E44Z1187B9A1D74</td>\n",
       "      <td>324</td>\n",
       "      <td>San Jose-Sunnyvale-Santa Clara, CA</td>\n",
       "      <td>Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) G...</td>\n",
       "      <td>2018</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>537edd60-8575-4fbe-a9ac-eaa7bb819625</td>\n",
       "      <td>1.54229224E12</td>\n",
       "      <td>97</td>\n",
       "      <td>paid</td>\n",
       "      <td>SOVOZSC12A8C144E73</td>\n",
       "      <td>ART0ETO1187B9AB519</td>\n",
       "      <td>605</td>\n",
       "      <td>Lansing-East Lansing, MI</td>\n",
       "      <td>\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5...</td>\n",
       "      <td>2018</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>b29c71e1-701c-4492-9c0c-666f3181f057</td>\n",
       "      <td>1.54229303E12</td>\n",
       "      <td>97</td>\n",
       "      <td>paid</td>\n",
       "      <td>SOYTFSY12A6D4FD84E</td>\n",
       "      <td>ARRFSMX1187FB39B03</td>\n",
       "      <td>605</td>\n",
       "      <td>Lansing-East Lansing, MI</td>\n",
       "      <td>\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5...</td>\n",
       "      <td>2018</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>05038f8e-7a96-499a-99b6-0c182633cf40</td>\n",
       "      <td>1.54229368E12</td>\n",
       "      <td>97</td>\n",
       "      <td>paid</td>\n",
       "      <td>SOWEFTO12A3F1EB976</td>\n",
       "      <td>ARPN0Y61187B9ABAA0</td>\n",
       "      <td>605</td>\n",
       "      <td>Lansing-East Lansing, MI</td>\n",
       "      <td>\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5...</td>\n",
       "      <td>2018</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ed1c43aa-87c3-4efb-b3c4-2513620d2db1</td>\n",
       "      <td>1.54229421E12</td>\n",
       "      <td>97</td>\n",
       "      <td>paid</td>\n",
       "      <td>SOBONKR12A58A7A7E0</td>\n",
       "      <td>AR5E44Z1187B9A1D74</td>\n",
       "      <td>605</td>\n",
       "      <td>Lansing-East Lansing, MI</td>\n",
       "      <td>\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5...</td>\n",
       "      <td>2018</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>b6179fe5-c28f-4970-b352-06c3ceac19b8</td>\n",
       "      <td>1.5422946E12</td>\n",
       "      <td>97</td>\n",
       "      <td>paid</td>\n",
       "      <td>SOBONKR12A58A7A7E0</td>\n",
       "      <td>AR5E44Z1187B9A1D74</td>\n",
       "      <td>605</td>\n",
       "      <td>Lansing-East Lansing, MI</td>\n",
       "      <td>\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5...</td>\n",
       "      <td>2018</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>a0e11638-8a2b-4c35-a692-a48d67e095d2</td>\n",
       "      <td>1.54229749E12</td>\n",
       "      <td>97</td>\n",
       "      <td>paid</td>\n",
       "      <td>SOJHUQN12A6D4F8EDE</td>\n",
       "      <td>AR4OU721187FB4549D</td>\n",
       "      <td>605</td>\n",
       "      <td>Lansing-East Lansing, MI</td>\n",
       "      <td>\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5...</td>\n",
       "      <td>2018</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>9d2bda8c-2424-4dd4-b1c6-ed8925fc6983</td>\n",
       "      <td>1.54229788E12</td>\n",
       "      <td>32</td>\n",
       "      <td>free</td>\n",
       "      <td>SORTSWB12A58A7DCA3</td>\n",
       "      <td>ARBAW9R1187B98FB6B</td>\n",
       "      <td>554</td>\n",
       "      <td>New York-Newark-Jersey City, NY-NJ-PA</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>2018</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7defcb2e-0983-4820-9ca2-a9c2b8049ec5</td>\n",
       "      <td>1.54229998E12</td>\n",
       "      <td>49</td>\n",
       "      <td>paid</td>\n",
       "      <td>SOLQSYZ12A58A7919B</td>\n",
       "      <td>ARQSM561187FB4A0CF</td>\n",
       "      <td>621</td>\n",
       "      <td>San Francisco-Oakland-Hayward, CA</td>\n",
       "      <td>Mozilla/5.0 (Windows NT 5.1; rv:31.0) Gecko/20...</td>\n",
       "      <td>2018</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>b1844499-d568-4b73-83c5-776b582011cd</td>\n",
       "      <td>1.54230155E12</td>\n",
       "      <td>30</td>\n",
       "      <td>paid</td>\n",
       "      <td>SOTPQFM12AB017AC9E</td>\n",
       "      <td>ARANOZN1187B9B373E</td>\n",
       "      <td>324</td>\n",
       "      <td>San Jose-Sunnyvale-Santa Clara, CA</td>\n",
       "      <td>Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) G...</td>\n",
       "      <td>2018</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             songplay_id             ts userId level  \\\n",
       "0   7dc063ba-685e-497f-9233-5e0bf7f5e92c  1.54226708E12     49  paid   \n",
       "1   370b1546-146f-44a8-8692-2b2418a2e3a8  1.54227861E12     80  paid   \n",
       "2   32b383cc-62c8-4f65-9f5e-2cb92f34afab  1.54227887E12     80  paid   \n",
       "3   b85d7ea5-71ec-4bad-be9b-37586c957d2b  1.54227914E12     80  paid   \n",
       "4   54443a43-6805-4035-af07-7fefe8056041  1.54228031E12     80  paid   \n",
       "5   15630d35-df92-4218-878d-4f5b75266d8a  1.54228202E12     30  paid   \n",
       "6   a477f6b4-4551-465b-8caa-dacd6afb1eba   1.5422849E12     30  paid   \n",
       "7   b036aa59-a542-403e-9051-f1caa1e80556  1.54228621E12     30  paid   \n",
       "8   01ad1f8b-d918-4d66-bbd5-908359ff05cb  1.54228674E12     30  paid   \n",
       "9   4f1bec2c-87a0-453c-9b84-ca925651d0b8  1.54228779E12     30  paid   \n",
       "10  ccdb71f4-8835-4d5d-95be-8bbc1674f80c  1.54228949E12     30  paid   \n",
       "11  537edd60-8575-4fbe-a9ac-eaa7bb819625  1.54229224E12     97  paid   \n",
       "12  b29c71e1-701c-4492-9c0c-666f3181f057  1.54229303E12     97  paid   \n",
       "13  05038f8e-7a96-499a-99b6-0c182633cf40  1.54229368E12     97  paid   \n",
       "14  ed1c43aa-87c3-4efb-b3c4-2513620d2db1  1.54229421E12     97  paid   \n",
       "15  b6179fe5-c28f-4970-b352-06c3ceac19b8   1.5422946E12     97  paid   \n",
       "16  a0e11638-8a2b-4c35-a692-a48d67e095d2  1.54229749E12     97  paid   \n",
       "17  9d2bda8c-2424-4dd4-b1c6-ed8925fc6983  1.54229788E12     32  free   \n",
       "18  7defcb2e-0983-4820-9ca2-a9c2b8049ec5  1.54229998E12     49  paid   \n",
       "19  b1844499-d568-4b73-83c5-776b582011cd  1.54230155E12     30  paid   \n",
       "\n",
       "               song_id           artist_id sessionId  \\\n",
       "0   SOCUITT12AB0187A32  ARKS2FE1187B99325D       606   \n",
       "1   SOSDZFY12A8C143718  AR748W61187B9B6AB8       611   \n",
       "2   SOTNWCI12AAF3B2028  ARS54I31187FB46721       611   \n",
       "3   SOBONKR12A58A7A7E0  AR5E44Z1187B9A1D74       611   \n",
       "4   SOLZOBD12AB0185720  ARPDVPJ1187B9ADBE9       611   \n",
       "5   SOULTKQ12AB018A183  ARKQQZA12086C116FC       324   \n",
       "6   SOIOESO12A6D4F621D  ARVLXWP1187FB5B94A       324   \n",
       "7   SOSQIRI12A8C133897  AR1XIHA1187FB4AED3       324   \n",
       "8   SONZWDK12A6701F62B  ARL4UQB1187B9B74E3       324   \n",
       "9   SOYUKXG12A58A77837  ARZNLRE1187B99B6C3       324   \n",
       "10  SOBONKR12A58A7A7E0  AR5E44Z1187B9A1D74       324   \n",
       "11  SOVOZSC12A8C144E73  ART0ETO1187B9AB519       605   \n",
       "12  SOYTFSY12A6D4FD84E  ARRFSMX1187FB39B03       605   \n",
       "13  SOWEFTO12A3F1EB976  ARPN0Y61187B9ABAA0       605   \n",
       "14  SOBONKR12A58A7A7E0  AR5E44Z1187B9A1D74       605   \n",
       "15  SOBONKR12A58A7A7E0  AR5E44Z1187B9A1D74       605   \n",
       "16  SOJHUQN12A6D4F8EDE  AR4OU721187FB4549D       605   \n",
       "17  SORTSWB12A58A7DCA3  ARBAW9R1187B98FB6B       554   \n",
       "18  SOLQSYZ12A58A7919B  ARQSM561187FB4A0CF       621   \n",
       "19  SOTPQFM12AB017AC9E  ARANOZN1187B9B373E       324   \n",
       "\n",
       "                                 location  \\\n",
       "0       San Francisco-Oakland-Hayward, CA   \n",
       "1             Portland-South Portland, ME   \n",
       "2             Portland-South Portland, ME   \n",
       "3             Portland-South Portland, ME   \n",
       "4             Portland-South Portland, ME   \n",
       "5      San Jose-Sunnyvale-Santa Clara, CA   \n",
       "6      San Jose-Sunnyvale-Santa Clara, CA   \n",
       "7      San Jose-Sunnyvale-Santa Clara, CA   \n",
       "8      San Jose-Sunnyvale-Santa Clara, CA   \n",
       "9      San Jose-Sunnyvale-Santa Clara, CA   \n",
       "10     San Jose-Sunnyvale-Santa Clara, CA   \n",
       "11               Lansing-East Lansing, MI   \n",
       "12               Lansing-East Lansing, MI   \n",
       "13               Lansing-East Lansing, MI   \n",
       "14               Lansing-East Lansing, MI   \n",
       "15               Lansing-East Lansing, MI   \n",
       "16               Lansing-East Lansing, MI   \n",
       "17  New York-Newark-Jersey City, NY-NJ-PA   \n",
       "18      San Francisco-Oakland-Hayward, CA   \n",
       "19     San Jose-Sunnyvale-Santa Clara, CA   \n",
       "\n",
       "                                            userAgent  year  month  \n",
       "0   Mozilla/5.0 (Windows NT 5.1; rv:31.0) Gecko/20...  2018     11  \n",
       "1   \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4...  2018     11  \n",
       "2   \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4...  2018     11  \n",
       "3   \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4...  2018     11  \n",
       "4   \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4...  2018     11  \n",
       "5   Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) G...  2018     11  \n",
       "6   Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) G...  2018     11  \n",
       "7   Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) G...  2018     11  \n",
       "8   Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) G...  2018     11  \n",
       "9   Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) G...  2018     11  \n",
       "10  Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) G...  2018     11  \n",
       "11  \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5...  2018     11  \n",
       "12  \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5...  2018     11  \n",
       "13  \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5...  2018     11  \n",
       "14  \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5...  2018     11  \n",
       "15  \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5...  2018     11  \n",
       "16  \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/5...  2018     11  \n",
       "17  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...  2018     11  \n",
       "18  Mozilla/5.0 (Windows NT 5.1; rv:31.0) Gecko/20...  2018     11  \n",
       "19  Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) G...  2018     11  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do we have any duplicate userIds?\n",
    "spark.sql('''\n",
    "SELECT *\n",
    "FROM songplays_tbl\n",
    "WHERE song_id IS NOT NULL\n",
    "LIMIT 20\n",
    "''').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "songs = spark.read \\\n",
    "    .format('csv') \\\n",
    "    .option('header',True) \\\n",
    "    .load(read_path_prefix + \"songs\")\n",
    "\n",
    "songs.createOrReplaceTempView('songs_tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:======================================================>(421 + 5) / 426]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|           song_id|count|\n",
      "+------------------+-----+\n",
      "|SOPIHUA12A8AE45B1D|    1|\n",
      "|SOAXEBM12AB017E5F9|    1|\n",
      "|SOVJXVJ12A8C13517D|    1|\n",
      "|SOBTWBZ12A6D4FBE3E|    1|\n",
      "|SORJVDO12AF72A1970|    1|\n",
      "+------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Do we have any duplicate song Ids?\n",
    "spark.sql('''\n",
    "SELECT song_id, COUNT(song_id) count\n",
    "FROM songs_tbl\n",
    "GROUP BY song_id\n",
    "ORDER BY count DESC\n",
    "LIMIT 5\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "artists = spark.read \\\n",
    "    .format('csv') \\\n",
    "    .option('header',True) \\\n",
    "    .load(read_path_prefix + \"artists\")\n",
    "\n",
    "artists.createOrReplaceTempView('artists_tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:====================================================> (290 + 9) / 299]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|         artist_id|count|\n",
      "+------------------+-----+\n",
      "|ARLGUTA1187B9B605F|    1|\n",
      "|AR82DJK1187B991107|    1|\n",
      "|ARBS7RY1187FB3B72F|    1|\n",
      "|ARTHJGQ1187FB42F1F|    1|\n",
      "|AR23EC41187FB4805D|    1|\n",
      "+------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Do we have any duplicate artist Ids?\n",
    "spark.sql('''\n",
    "SELECT artist_id, COUNT(artist_id) count\n",
    "FROM artists_tbl\n",
    "GROUP BY artist_id\n",
    "ORDER BY count DESC\n",
    "LIMIT 5\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:========================================================>(59 + 1) / 60]\r"
     ]
    }
   ],
   "source": [
    "songplays = spark.read \\\n",
    "    .format('csv') \\\n",
    "    .schema(songplays_schema) \\\n",
    "    .option('header',True) \\\n",
    "    .load('./_out/songplays')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|freq|            location|\n",
      "+----+--------------------+\n",
      "|   9|San Francisco-Oak...|\n",
      "|   2|Janesville-Beloit...|\n",
      "|   2|       Red Bluff, CA|\n",
      "|   1|Birmingham-Hoover...|\n",
      "|   1|          Eugene, OR|\n",
      "|   1|Houston-The Woodl...|\n",
      "+----+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:========================================================>(59 + 1) / 60]\r"
     ]
    }
   ],
   "source": [
    "#Example analytics: get locations where songs were played on Nov. 11, 2018\n",
    "\n",
    "spark.sql('''\n",
    "SELECT count(*) AS freq, location\n",
    "from play_tbl\n",
    "WHERE song_id IS NOT NULL\n",
    "AND (ts/1000) > 1543532400\n",
    "GROUP BY location\n",
    "ORDER BY freq DESC\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run etl.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df.select('artist').dropDuplicates().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', 800)\n",
    "\n",
    "#Extract data to make songs table:\n",
    "df = data.select('userId','firstName','lastName','gender','level').orderBy(desc('ts')).dropDuplicates(['userId'])\n",
    "df.toPandas()\n",
    "#sc._jsc.hadoopConfiguration().set('fs.s3a.endpoint','s3-us-west-2.amazonaws.com')\n",
    "\n",
    "#writing to S3 as parquet:\n",
    "# df.write \\\n",
    "#     .option(\"header\",True) \\\n",
    "#     .partitionBy(\"year\",\"artist_id\") \\\n",
    "#     .parquet('s3a://rambino-output/test-output-parquet')\n",
    "\n",
    "#This works ^ but it takes FOREVER. I had 23 records in this test output and it still took 26 MINUTES. Absolutely insane.\n",
    "#To do:\n",
    "#1. When testing my code, avoid writing to S3 until the last minute.\n",
    "#2. Try to do some research as to why this is so slow and how to make it work better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Package Import\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from asyncio import as_completed\n",
    "import concurrent.futures\n",
    "\n",
    "def func(x,y,z):\n",
    "    return x+y+z\n",
    "\n",
    "results = []\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = []\n",
    "    for cluster in [2,4]:\n",
    "        futures.append(\n",
    "            executor.submit(func,cluster,2,3)\n",
    "        )\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        results.append(future.result())\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Loading Credentials from file\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/rambino/.aws/credentials']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#AWS Credentials\n",
    "aws_path = \"/home/rambino/.aws/credentials\"\n",
    "aws_cred = configparser.ConfigParser()\n",
    "aws_cred.read(aws_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Create SSH keypair for connecting to EC2 instances\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec2 = boto3.client('ec2',\n",
    "    region_name             = \"us-east-1\",\n",
    "    aws_access_key_id       = aws_cred['default']['aws_access_key_id'],\n",
    "    aws_secret_access_key   = aws_cred['default']['aws_secret_access_key']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ec2.create_key_pair(\n",
    "    KeyName = 'spark_ec2_key',\n",
    "    DryRun=False,\n",
    "    KeyFormat='pem'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/rambino/.aws/spark_keypair.pem',\"w\") as file:\n",
    "    file.writelines(response['KeyMaterial'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Setting up VPC for the EMR cluster\n",
    "\n",
    "---\n",
    "\n",
    "If no VPC is specified for an EMR cluster, then the cluster is launched in the normal AWS cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating default VPC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws ec2 create-default-vpc --profile default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting **first** subnetId for this VPC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vpc_output = ec2.describe_vpcs()\n",
    "\n",
    "#Getting first (and only) VPC:\n",
    "vpcId = vpc_output['Vpcs'][0]['VpcId']\n",
    "\n",
    "subnet_output = ec2.describe_subnets(\n",
    "    Filters=[\n",
    "        {\n",
    "            'Name':'vpc-id',\n",
    "            'Values':[vpcId]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "subnetId = subnet_output['Subnets'][0]['SubnetId']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Creating EMR Cluster\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps needed to set up and connect to EMR:**\n",
    "1. set up cluster with correct specifications\n",
    "2. get 'master public DNS' for the cluster\n",
    "3. edit security group to allow my computer to connect via SSH (add inbound rule to allow SSH connection from my IP)\n",
    "   1. Note: Security group is distinct entity from cluster - why not just set this up beforehand?\n",
    "      1. Note: It IS possible to set up a security group beforehand - and to specify this security group for the master and slave nodes. For a more official setup, it's probably better to do this to ensure that the security group we set up for EMR is custom-defined (and not default).\n",
    "      2. UPDATE: Well, actually when you CREATE a cluster, security groups are created automatically for the cluster on the default VPC. I could go through the trouble to set up custom security groups *beforehand*, or I could just create the cluster and then change the security groups as needed once they are created. Since I can't think of a reason it would be better to create custom security groups beforehand rather than just edit the ones which are created for me, I will just edit the ones created for me in this code.\n",
    "4. Set up proxy to access \"persistent web UI for Spark\"?\n",
    "   1. This looks like it's for being able to view the Spark UI somehow, but the way they're setting up the proxy settings and filtering URLs seems really hacky (e.g., they're filtering urls matching \"http://10.*)\". I'm not sure I want to set this up until I know that it's much better than using AWS' built-in UI viewer.\n",
    "   2. Update: **It turns out that AWS also recommends using FoxyProxy (or other tools) to connect to Spark UIs on EMR**, so I will in fact do this now.\n",
    "      1. [read more here](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-web-interfaces.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emr = boto3.client('emr',\n",
    "    region_name             = \"us-east-1\",\n",
    "    aws_access_key_id       = aws_cred['default']['aws_access_key_id'],\n",
    "    aws_secret_access_key   = aws_cred['default']['aws_secret_access_key']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With boto3\n",
    "emr.run_job_flow(\n",
    "            Name='spark-cluster',\n",
    "            LogUri='s3://emrlogs/',\n",
    "            ReleaseLabel='emr-5.28.0',\n",
    "            Instances={\n",
    "                'MasterInstanceType': 'm5.xlarge',\n",
    "                'SlaveInstanceType': 'm5.xlarge',\n",
    "                'InstanceCount': 4,\n",
    "                'Ec2KeyName':'spark_ec2_key',\n",
    "                'KeepJobFlowAliveWhenNoSteps': True\n",
    "                #'EmrManagedMasterSecurityGroup': security_groups['manager'].id,\n",
    "                #'EmrManagedSlaveSecurityGroup': security_groups['worker'].id,\n",
    "            },\n",
    "            Applications=[\n",
    "                {\n",
    "                    \"Name\":\"Spark\"\n",
    "                },\n",
    "                {\n",
    "                    \"Name\":\"Zeppelin\"\n",
    "                }\n",
    "            ],\n",
    "            JobFlowRole='EMR_EC2_DefaultRole',\n",
    "            ServiceRole='EMR_DefaultRole',\n",
    "            VisibleToAllUsers=True\n",
    "        )\n",
    "\n",
    "#NOTE: Under the 'Applications' specification of the EMR cluster above, you can also load in applications like\n",
    "# Spark, TensorFlow, Presto, and Hadoop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with AWS CLI:\n",
    "\n",
    "!aws emr create-cluster --name test-cluster \\\n",
    "    --use-default-roles \\\n",
    "    --release-label emr-5.28.0 \\\n",
    "    --instance-count 4 \\\n",
    "    --applications Name=Spark Name=Zeppelin \\\n",
    "    --ec2-attributes KeyName='spark_ec2_key',SubnetId='subnet-0b6cc9cfba9463659'\\\n",
    "    --instance-type m5.xlarge \\\n",
    "    --log-uri s3://emrlogs/ \\\n",
    "    --visible-to-all-users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Configuring Cluster\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_list = emr.list_clusters(\n",
    "    ClusterStates=['STARTING','RUNNING']\n",
    ")\n",
    "print(cluster_list)\n",
    "cluster_id = cluster_list['Clusters'][0]['Id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cluster = emr.describe_cluster(\n",
    "    ClusterId = cluster_id\n",
    ")\n",
    "new_cluster\n",
    "secGroup_master = new_cluster['Cluster']['Ec2InstanceAttributes']['EmrManagedMasterSecurityGroup']\n",
    "cluster_dns = new_cluster['Cluster']['MasterPublicDnsName']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure Cluster Security Groups to only accept SSH ingress from my IP address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting my public IP address from config.me website (IP is last element of returned array)\n",
    "myIP = !curl ifconfig.me\n",
    "myIP = myIP[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specifying internal port (arbitrary?)\n",
    "myPort = '32'\n",
    "myCidrIp = myIP + \"/\" + myPort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ec2.authorize_security_group_ingress(\n",
    "    GroupId=secGroup_master,\n",
    "    IpPermissions=[\n",
    "        {\n",
    "            'FromPort': 22,\n",
    "            'IpProtocol': 'tcp',\n",
    "            'IpRanges': [\n",
    "                {\n",
    "                    'CidrIp': myCidrIp,\n",
    "                    'Description': 'SSH access to Spark EMR on AWS from Kevins Computer',\n",
    "                },\n",
    "            ],\n",
    "            'ToPort': 22,\n",
    "        },\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Interacting with Cluster\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#File path where cluster login information is kept on my machine:\n",
    "pem_path = '/home/rambino/.aws/spark_keypair.pem'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to Cluster via SSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Command to use in terminal (interactive):\n",
    "print(f\"ssh hadoop@{cluster_dns} -i {pem_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Proxy connection to allow interaction with Spark UI\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up FoxyProxy to allow connection to Spark UI from localhost\n",
    "[AWS Documentation on Port forwarding for EMR connections](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-ssh-tunnel.html)\n",
    "\n",
    "\n",
    "I needed to install the browser extension FoxyProxy to allow my browser to interface with the EMR cluster. Once I installed it, I then needed to set up a new proxy with these settings:\n",
    "- IP address: `localhost`\n",
    "- Port: `8157` (only needed to match dynamic port forwarding below)\n",
    "\n",
    "Then, in the 'pattern matching' part, I needed to specify which URLs should be forwarded in this way. This was already specified by Udacity. The json file accompanying this notebook named 'foxyproxy...' shows these patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copying credentials file to the master node (not sure why yet)\n",
    "print(f\"scp -i {pem_path} {pem_path} hadoop@{cluster_dns}:/home/hadoop/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This sets up port forwarding (somehow) so that data from our local machine on port 8157 is forwarded to the master node (allowing interactivity)\n",
    "#NOTE: Terminal remains open when this request succeeds - and needs to remain running while accessing Spark UI\n",
    "\n",
    "#Note: Getting this SSH connection to work has been unpredictable at times. Often get 'connection refused' errors, but then it\n",
    "#suddenly works. Should ideally figure out what's going on there...\n",
    "\n",
    "print(f\"ssh -v -i {pem_path} -N -D 127.0.0.1:8157 hadoop@{cluster_dns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing Spark UI:\n",
    "- Base URL:           http://ec2-54-87-42-167.compute-1.amazonaws.com\n",
    "\n",
    "- Spark History:      http://ec2-54-87-42-167.compute-1.amazonaws.com:18080/\n",
    "- YARN Node Manager:  http://ec2-54-87-42-167.compute-1.amazonaws.com:8042/\n",
    "\n",
    "\n",
    "[See more ports here](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-web-interfaces.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Deleting EMR Cluster (Teardown)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emr.terminate_job_flows(\n",
    "    JobFlowIds=[\n",
    "        cluster_id\n",
    "    ]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
