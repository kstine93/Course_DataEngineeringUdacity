{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMR Setup with Python SDK (boto3)\n",
    "This notebook will show how to set up some AWS resources using the Python SDK for AWS, boto3.\n",
    "\n",
    "Boto3 Documentation: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/redshift.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "aws_key      = aws_cred['default']['aws_access_key_id']\n",
    "aws_secret   = aws_cred['default']['aws_secret_access_key']\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.jars.packages\",\"com.amazonaws:aws-java-sdk-s3:1.12.311\") \\\n",
    "    .config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:3.3.4\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setSystemProperty('com.amazonaws.services.s3.enableV4','true')\n",
    "\n",
    "sc._jsc.hadoopConfiguration().set('fs.s3a.access.key',aws_key)\n",
    "sc._jsc.hadoopConfiguration().set('fs.s3a.secret.key',aws_secret)\n",
    "sc._jsc.hadoopConfiguration().set('spark.hadoop.fs.s3a.bucket.all.committer.magic.enabled', 'true')\n",
    "sc._jsc.hadoopConfiguration().set('fs.s3a.endpoint','s3-us-west-2.amazonaws.com')\n",
    "sc._jsc.hadoopConfiguration().set('fs.s3a.connection.ssl.enabled','true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, StringType, FloatType, StructType, StructField\n",
    "\n",
    "song_schema = StructType([\n",
    "    StructField('num_songs',IntegerType(),True),\n",
    "    StructField('artist_id',StringType(),True),\n",
    "    StructField('artist_latitude',FloatType(),True),\n",
    "    StructField('artist_longitude',FloatType(),True),\n",
    "    StructField('artist_location',StringType(),True),\n",
    "    StructField('artist_name',StringType(),True),\n",
    "    StructField('song_id',StringType(),True),\n",
    "    StructField('title',StringType(),True),\n",
    "    StructField('duration',FloatType(),True),\n",
    "    StructField('year',IntegerType(),True)\n",
    "])\n",
    "\n",
    "log_schema = StructType([\n",
    "    StructField('artist',StringType(),True),\n",
    "    StructField('auth',StringType(),True),\n",
    "    StructField('firstName',StringType(),True),\n",
    "    StructField('gender',StringType(),True),\n",
    "    StructField('itemInSession',IntegerType(),True),\n",
    "    StructField('lastName',StringType(),True),\n",
    "    StructField('length',IntegerType(),True),\n",
    "    StructField('level',StringType(),True),\n",
    "    StructField('location',StringType(),True),\n",
    "    StructField('method',StringType(),True),\n",
    "    StructField('page',StringType(),True),\n",
    "    StructField('registration',StringType(),True),\n",
    "    StructField('sessionId',IntegerType(),True),\n",
    "    StructField('song',StringType(),True),\n",
    "    StructField('status',IntegerType(),True),\n",
    "    StructField('ts',FloatType(),True),\n",
    "    StructField('userAgent',StringType(),True),\n",
    "    StructField('userId',StringType(),True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df = spark.read.format('json').schema(song_schema).load('s3a://udacity-dend/song_data/*/*/*')#/A/B/C/TRABCEI128F424C983.json')\n",
    "log_df = spark.read.format('json').schema(log_schema).load('s3a://udacity-dend/log_data/*/*')#/2018/11/2018-11-12-events.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df = log_df.where(\"page = 'NextSong'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from datetime import datetime\n",
    "\n",
    "get_hour = udf(lambda x: x.hour)\n",
    "get_day = udf(lambda x: x.day)\n",
    "get_week = udf(lambda x: x.isocalendar().week)\n",
    "get_month = udf(lambda x: x.month)\n",
    "get_year = udf(lambda x: x.year)\n",
    "get_weekday = udf(lambda x: x.weekday())\n",
    "\n",
    "get_datetime = udf(lambda x: datetime.fromtimestamp(x/1000))\n",
    "log_df = log_df.withColumn('timestamp',get_datetime('ts'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract columns to create time table\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "log_df = log_df \\\n",
    "    .withColumn('songplay_id',F.expr(\"uuid()\")) \\\n",
    "    .withColumn('hour',get_hour('timestamp')) \\\n",
    "    .withColumn('day',get_day('timestamp')) \\\n",
    "    .withColumn('week',get_week('timestamp')) \\\n",
    "    .withColumn('month',get_month('timestamp')) \\\n",
    "    .withColumn('year',get_year('timestamp')) \\\n",
    "    .withColumn('weekday',get_weekday('timestamp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplays_table = songplays_table \\\n",
    "    .withColumn('songplay_id',F.expr(\"uuid()\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#data.withColumn('timestamp',ts_to_timestamp('ts')).show()\n",
    "match_condition = ((log_df.song == song_df.title) & (log_df.artist == song_df.artist_name))\n",
    "songplays_table = log_df.join(song_df, match_condition, \"left\") \\\n",
    "    .select(\n",
    "        log_df.ts, log_df.userId, log_df.level, log_df.sessionId,\n",
    "        log_df.location, log_df.userAgent, log_df.month, log_df.year,\n",
    "        song_df.song_id, song_df.artist_id\n",
    "    )                \n",
    "\n",
    "\n",
    "#songplays_table.limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_df = songplays_table.filter(\"song_id != 'None'\").limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplays_table \\\n",
    "    .select(\"songplay_id\",\"ts\",\"userId\",\"level\",\"song_id\",\"artist_id\",\"sessionId\",\"location\",\"userAgent\",\"year\",\"month\") \\\n",
    "    .write \\\n",
    "    .option(\"header\",True) \\\n",
    "    .partitionBy(\"year\",\"month\") \\\n",
    "    .csv(\"./_out/\" + \"songplays\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%md\n",
    "\n",
    "To do (Sept. 30):\n",
    "1. I saved the output of a local ETL to the _out folder. Take a look at it and see if the data looks right\n",
    "   ~1. Why are so many entries missing 'song_id' and 'artist_id'? (333 / 6280)~\n",
    "   ~2. Query the data and see what kind of results I get (Compared to one query for Redshift - I GET THE SAME RESULTS!! Looks like I (probably) did it right)~\n",
    "   3. take a look at double-checks I did for Redshift project - any I should implement here?\n",
    "      1. Yes, need: unique **songs, users and artists**\n",
    "2. Run the etl.py again with limited data (Nov. 22 has at least 1 match in song + artist - use that?)\n",
    "3. Test writing as parquet\n",
    "4. Cleanup\n",
    "   1. Clean etl.py file to not have any errant comments or code. Docstrings in place?\n",
    "   2. Clean EMR_boto3Setup notebook so that testing code is neatly organized or in separate notebook.\n",
    "   3. Delete _out folder with test data\n",
    "5. Finish rest of this notebook to spin up EMR\n",
    "6. Use built-in notebook to run low-data code once\n",
    "7. Upload .py to EMR via SSH and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplays_schema = StructType([\n",
    "    StructField('ts',FloatType(),True),\n",
    "    StructField('userId',StringType(),True),\n",
    "    StructField('level',StringType(),True),\n",
    "    StructField('song_id',StringType(),True),\n",
    "    StructField('artist_id',StringType(),True),\n",
    "    StructField('sessionId',IntegerType(),True),\n",
    "    StructField('location',StringType(),True),\n",
    "    StructField('userAgent',StringType(),True),\n",
    "    StructField('year',IntegerType(),True),\n",
    "    StructField('month',IntegerType(),True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplays.filter(\"level = 'free'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pyspark.sql.functions' has no attribute 'day'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/rambino/dev/DataEngineering_Udacity/Projects/DataLakeWithEMR/EMR_boto3Setup.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/Projects/DataLakeWithEMR/EMR_boto3Setup.ipynb#Y105sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m#year, month, dayofmonth, hour, weekofyear, date_format, desc, to_timestamp\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/Projects/DataLakeWithEMR/EMR_boto3Setup.ipynb#Y105sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/Projects/DataLakeWithEMR/EMR_boto3Setup.ipynb#Y105sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m#make_timestamp = F.udf(lambda x: F.cast(typ=T.TimestampType(),val=x/1000)) #F.udf(lambda x: F.unix_timestamp(x/1000,'dd-MM-yyyy HH:mm:ss.SSS'))\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/Projects/DataLakeWithEMR/EMR_boto3Setup.ipynb#Y105sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/Projects/DataLakeWithEMR/EMR_boto3Setup.ipynb#Y105sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m#songplays.select(to_timestamp('ts')).show()\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/Projects/DataLakeWithEMR/EMR_boto3Setup.ipynb#Y105sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m songplays \u001b[39m=\u001b[39m songplays\u001b[39m.\u001b[39mwithColumn(\u001b[39m'\u001b[39m\u001b[39mtimestamp\u001b[39m\u001b[39m'\u001b[39m,make_timestamp(\u001b[39m'\u001b[39m\u001b[39mts\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/Projects/DataLakeWithEMR/EMR_boto3Setup.ipynb#Y105sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m songplays \\\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/Projects/DataLakeWithEMR/EMR_boto3Setup.ipynb#Y105sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m.\u001b[39mwithColumn(\u001b[39m'\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m'\u001b[39m,F\u001b[39m.\u001b[39mto_timestamp(songplays[\u001b[39m'\u001b[39m\u001b[39mts\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m/\u001b[39m\u001b[39m1000\u001b[39m)) \\\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/Projects/DataLakeWithEMR/EMR_boto3Setup.ipynb#Y105sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m.\u001b[39mwithColumn(\u001b[39m'\u001b[39m\u001b[39mmonth\u001b[39m\u001b[39m'\u001b[39m,F\u001b[39m.\u001b[39mmonth(\u001b[39m'\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m'\u001b[39m)) \\\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/Projects/DataLakeWithEMR/EMR_boto3Setup.ipynb#Y105sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m.\u001b[39mwithColumn(\u001b[39m'\u001b[39m\u001b[39mhour\u001b[39m\u001b[39m'\u001b[39m,F\u001b[39m.\u001b[39;49mday(\u001b[39m'\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m'\u001b[39m)) \\\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rambino/dev/DataEngineering_Udacity/Projects/DataLakeWithEMR/EMR_boto3Setup.ipynb#Y105sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m.\u001b[39mshow()\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pyspark.sql.functions' has no attribute 'day'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "#from pyspark.sql.functions import cast as choohoh\n",
    "from pyspark.sql import types as T\n",
    "#year, month, dayofmonth, hour, weekofyear, date_format, desc, to_timestamp\n",
    "\n",
    "#make_timestamp = F.udf(lambda x: F.cast(typ=T.TimestampType(),val=x/1000)) #F.udf(lambda x: F.unix_timestamp(x/1000,'dd-MM-yyyy HH:mm:ss.SSS'))\n",
    "\n",
    "#songplays.select(to_timestamp('ts')).show()\n",
    "songplays = songplays.withColumn('timestamp',make_timestamp('ts'))\n",
    "songplays \\\n",
    "    .withColumn('date',F.to_timestamp(songplays['ts']/1000)) \\\n",
    "    .withColumn('month',F.month('date')) \\\n",
    "    .withColumn('hour',F.dayofmonth('date')) \\\n",
    "    .show()\n",
    "#songplays.withColumn(\"timestamp\",F.date_format(songplays.ts.cast(dataType=T.TimestampType()), \"yyyy-MM-dd\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplays = spark.read \\\n",
    "    .format('csv') \\\n",
    "    .schema(songplays_schema) \\\n",
    "    .option('header',True) \\\n",
    "    .load('./_out/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplays.createOrReplaceTempView(\"play_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplays.limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SELECT COUNT(*) AS freq, location\n",
    "FROM songplays\n",
    "JOIN time ON songplays.start_time = time.start_time\n",
    "WHERE time.year = 2018 \n",
    "AND time.month = 11  \n",
    "AND time.day = 30 \n",
    "GROUP BY songplays.location \n",
    "ORDER BY freq DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''\n",
    "SELECT count(*) AS freq, location\n",
    "from play_tbl\n",
    "WHERE song_id IS NOT NULL\n",
    "AND (ts/1000) > 1543532400\n",
    "GROUP BY location\n",
    "ORDER BY freq DESC\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run etl.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df.select('artist').dropDuplicates().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', 800)\n",
    "\n",
    "#Extract data to make songs table:\n",
    "df = data.select('userId','firstName','lastName','gender','level').orderBy(desc('ts')).dropDuplicates(['userId'])\n",
    "df.toPandas()\n",
    "#sc._jsc.hadoopConfiguration().set('fs.s3a.endpoint','s3-us-west-2.amazonaws.com')\n",
    "\n",
    "#writing to S3 as parquet:\n",
    "# df.write \\\n",
    "#     .option(\"header\",True) \\\n",
    "#     .partitionBy(\"year\",\"artist_id\") \\\n",
    "#     .parquet('s3a://rambino-output/test-output-parquet')\n",
    "\n",
    "#This works ^ but it takes FOREVER. I had 23 records in this test output and it still took 26 MINUTES. Absolutely insane.\n",
    "#To do:\n",
    "#1. When testing my code, avoid writing to S3 until the last minute.\n",
    "#2. Try to do some research as to why this is so slow and how to make it work better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Package Import\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from asyncio import as_completed\n",
    "import concurrent.futures\n",
    "\n",
    "def func(x,y,z):\n",
    "    return x+y+z\n",
    "\n",
    "results = []\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = []\n",
    "    for cluster in [2,4]:\n",
    "        futures.append(\n",
    "            executor.submit(func,cluster,2,3)\n",
    "        )\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        results.append(future.result())\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Loading Credentials from file\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AWS Credentials\n",
    "aws_path = \"/home/rambino/.aws/credentials\"\n",
    "aws_cred = configparser.ConfigParser()\n",
    "aws_cred.read(aws_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Create SSH keypair for connecting to EC2 instances\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec2 = boto3.client('ec2',\n",
    "    region_name             = \"us-east-1\",\n",
    "    aws_access_key_id       = aws_cred['default']['aws_access_key_id'],\n",
    "    aws_secret_access_key   = aws_cred['default']['aws_secret_access_key']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ec2.create_key_pair(\n",
    "    KeyName = 'spark_ec2_key',\n",
    "    DryRun=False,\n",
    "    KeyFormat='pem'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/rambino/.aws/spark_keypair.pem',\"w\") as file:\n",
    "    file.writelines(response['KeyMaterial'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Setting up VPC for the EMR cluster\n",
    "\n",
    "---\n",
    "\n",
    "If no VPC is specified for an EMR cluster, then the cluster is launched in the normal AWS cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating default VPC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws ec2 create-default-vpc --profile default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting **first** subnetId for this VPC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vpc_output = ec2.describe_vpcs()\n",
    "\n",
    "#Getting first (and only) VPC:\n",
    "vpcId = vpc_output['Vpcs'][0]['VpcId']\n",
    "\n",
    "subnet_output = ec2.describe_subnets(\n",
    "    Filters=[\n",
    "        {\n",
    "            'Name':'vpc-id',\n",
    "            'Values':[vpcId]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "subnetId = subnet_output['Subnets'][0]['SubnetId']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Creating EMR Cluster\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps needed to set up and connect to EMR:**\n",
    "1. set up cluster with correct specifications\n",
    "2. get 'master public DNS' for the cluster\n",
    "3. edit security group to allow my computer to connect via SSH (add inbound rule to allow SSH connection from my IP)\n",
    "   1. Note: Security group is distinct entity from cluster - why not just set this up beforehand?\n",
    "      1. Note: It IS possible to set up a security group beforehand - and to specify this security group for the master and slave nodes. For a more official setup, it's probably better to do this to ensure that the security group we set up for EMR is custom-defined (and not default).\n",
    "      2. UPDATE: Well, actually when you CREATE a cluster, security groups are created automatically for the cluster on the default VPC. I could go through the trouble to set up custom security groups *beforehand*, or I could just create the cluster and then change the security groups as needed once they are created. Since I can't think of a reason it would be better to create custom security groups beforehand rather than just edit the ones which are created for me, I will just edit the ones created for me in this code.\n",
    "4. Set up proxy to access \"persistent web UI for Spark\"?\n",
    "   1. This looks like it's for being able to view the Spark UI somehow, but the way they're setting up the proxy settings and filtering URLs seems really hacky (e.g., they're filtering urls matching \"http://10.*)\". I'm not sure I want to set this up until I know that it's much better than using AWS' built-in UI viewer.\n",
    "   2. Update: **It turns out that AWS also recommends using FoxyProxy (or other tools) to connect to Spark UIs on EMR**, so I will in fact do this now.\n",
    "      1. [read more here](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-web-interfaces.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emr = boto3.client('emr',\n",
    "    region_name             = \"us-east-1\",\n",
    "    aws_access_key_id       = aws_cred['default']['aws_access_key_id'],\n",
    "    aws_secret_access_key   = aws_cred['default']['aws_secret_access_key']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With boto3\n",
    "emr.run_job_flow(\n",
    "            Name='spark-cluster',\n",
    "            LogUri='s3://emrlogs/',\n",
    "            ReleaseLabel='emr-5.28.0',\n",
    "            Instances={\n",
    "                'MasterInstanceType': 'm5.xlarge',\n",
    "                'SlaveInstanceType': 'm5.xlarge',\n",
    "                'InstanceCount': 4,\n",
    "                'Ec2KeyName':'spark_ec2_key',\n",
    "                'KeepJobFlowAliveWhenNoSteps': True\n",
    "                #'EmrManagedMasterSecurityGroup': security_groups['manager'].id,\n",
    "                #'EmrManagedSlaveSecurityGroup': security_groups['worker'].id,\n",
    "            },\n",
    "            Applications=[\n",
    "                {\n",
    "                    \"Name\":\"Spark\"\n",
    "                },\n",
    "                {\n",
    "                    \"Name\":\"Zeppelin\"\n",
    "                }\n",
    "            ],\n",
    "            JobFlowRole='EMR_EC2_DefaultRole',\n",
    "            ServiceRole='EMR_DefaultRole',\n",
    "            VisibleToAllUsers=True\n",
    "        )\n",
    "\n",
    "#NOTE: Under the 'Applications' specification of the EMR cluster above, you can also load in applications like\n",
    "# Spark, TensorFlow, Presto, and Hadoop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with AWS CLI:\n",
    "\n",
    "!aws emr create-cluster --name test-cluster \\\n",
    "    --use-default-roles \\\n",
    "    --release-label emr-5.28.0 \\\n",
    "    --instance-count 4 \\\n",
    "    --applications Name=Spark Name=Zeppelin \\\n",
    "    --ec2-attributes KeyName='spark_ec2_key',SubnetId='subnet-0b6cc9cfba9463659'\\\n",
    "    --instance-type m5.xlarge \\\n",
    "    --log-uri s3://emrlogs/ \\\n",
    "    --visible-to-all-users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Configuring Cluster\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_list = emr.list_clusters(\n",
    "    ClusterStates=['STARTING','RUNNING']\n",
    ")\n",
    "print(cluster_list)\n",
    "cluster_id = cluster_list['Clusters'][0]['Id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cluster = emr.describe_cluster(\n",
    "    ClusterId = cluster_id\n",
    ")\n",
    "new_cluster\n",
    "secGroup_master = new_cluster['Cluster']['Ec2InstanceAttributes']['EmrManagedMasterSecurityGroup']\n",
    "cluster_dns = new_cluster['Cluster']['MasterPublicDnsName']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure Cluster Security Groups to only accept SSH ingress from my IP address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting my public IP address from config.me website (IP is last element of returned array)\n",
    "myIP = !curl ifconfig.me\n",
    "myIP = myIP[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specifying internal port (arbitrary?)\n",
    "myPort = '32'\n",
    "myCidrIp = myIP + \"/\" + myPort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ec2.authorize_security_group_ingress(\n",
    "    GroupId=secGroup_master,\n",
    "    IpPermissions=[\n",
    "        {\n",
    "            'FromPort': 22,\n",
    "            'IpProtocol': 'tcp',\n",
    "            'IpRanges': [\n",
    "                {\n",
    "                    'CidrIp': myCidrIp,\n",
    "                    'Description': 'SSH access to Spark EMR on AWS from Kevins Computer',\n",
    "                },\n",
    "            ],\n",
    "            'ToPort': 22,\n",
    "        },\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Interacting with Cluster\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#File path where cluster login information is kept on my machine:\n",
    "pem_path = '/home/rambino/.aws/spark_keypair.pem'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to Cluster via SSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Command to use in terminal (interactive):\n",
    "print(f\"ssh hadoop@{cluster_dns} -i {pem_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Proxy connection to allow interaction with Spark UI\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up FoxyProxy to allow connection to Spark UI from localhost\n",
    "[AWS Documentation on Port forwarding for EMR connections](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-ssh-tunnel.html)\n",
    "\n",
    "\n",
    "I needed to install the browser extension FoxyProxy to allow my browser to interface with the EMR cluster. Once I installed it, I then needed to set up a new proxy with these settings:\n",
    "- IP address: `localhost`\n",
    "- Port: `8157` (only needed to match dynamic port forwarding below)\n",
    "\n",
    "Then, in the 'pattern matching' part, I needed to specify which URLs should be forwarded in this way. This was already specified by Udacity. The json file accompanying this notebook named 'foxyproxy...' shows these patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copying credentials file to the master node (not sure why yet)\n",
    "print(f\"scp -i {pem_path} {pem_path} hadoop@{cluster_dns}:/home/hadoop/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This sets up port forwarding (somehow) so that data from our local machine on port 8157 is forwarded to the master node (allowing interactivity)\n",
    "#NOTE: Terminal remains open when this request succeeds - and needs to remain running while accessing Spark UI\n",
    "\n",
    "#Note: Getting this SSH connection to work has been unpredictable at times. Often get 'connection refused' errors, but then it\n",
    "#suddenly works. Should ideally figure out what's going on there...\n",
    "\n",
    "print(f\"ssh -v -i {pem_path} -N -D 127.0.0.1:8157 hadoop@{cluster_dns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing Spark UI:\n",
    "- Base URL:           http://ec2-54-87-42-167.compute-1.amazonaws.com\n",
    "\n",
    "- Spark History:      http://ec2-54-87-42-167.compute-1.amazonaws.com:18080/\n",
    "- YARN Node Manager:  http://ec2-54-87-42-167.compute-1.amazonaws.com:8042/\n",
    "\n",
    "\n",
    "[See more ports here](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-web-interfaces.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Deleting EMR Cluster (Teardown)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emr.terminate_job_flows(\n",
    "    JobFlowIds=[\n",
    "        cluster_id\n",
    "    ]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
