{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMR Setup with Python SDK (boto3)\n",
    "This notebook will show how to set up some AWS resources using the Python SDK for AWS, boto3.\n",
    "\n",
    "Boto3 Documentation: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/redshift.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "To do (Sept. 30):\n",
    "1. I saved the output of a local ETL to the _out folder. Take a look at it and see if the data looks right\n",
    "   1. ~~Why are so many entries missing 'song_id' and 'artist_id'? (333 / 6280)~~\n",
    "   2. ~~Query the data and see what kind of results I get (Compared to one query for Redshift - I GET THE SAME RESULTS!! Looks like I (probably) did it right)~~\n",
    "   3. ~~take a look at double-checks I did for Redshift project - any I should implement here?~~\n",
    "      1. ~~Yes, need: unique **songs, users and artists** - should implement this check in notebook after running ETL locally.~~\n",
    "2. ~~Run the etl.py again with limited data (Nov. 22 has at least 1 match in song + artist - use that?)~~\n",
    "3. ~~Clean up this notebook - should have EMR creation code + code to pull in data from S3 and inspect it.~~\n",
    "4. ~~Test writing as parquet~~\n",
    "5. Cleanup\n",
    "   1. ~~Clean etl.py file to not have any errant comments or code. Docstrings in place?~~\n",
    "   2. ~~Clean EMR_boto3Setup notebook so that testing code is neatly organized or in separate notebook.~~\n",
    "   3. Delete _out folder with test data\n",
    "6. Finish rest of this notebook to spin up EMR\n",
    "7. Use built-in notebook to run low-data code once\n",
    "8. Upload .py to EMR via SSH and run\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMR Setup with Boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Package Import\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Loading Credentials from file\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AWS Credentials\n",
    "aws_path = \"/home/rambino/.aws/credentials\"\n",
    "aws_cred = configparser.ConfigParser()\n",
    "aws_cred.read(aws_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Create SSH keypair for connecting to EC2 instances\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec2 = boto3.client('ec2',\n",
    "    region_name             = \"us-east-1\",\n",
    "    aws_access_key_id       = aws_cred['default']['aws_access_key_id'],\n",
    "    aws_secret_access_key   = aws_cred['default']['aws_secret_access_key']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ec2.create_key_pair(\n",
    "    KeyName = 'spark_ec2_key',\n",
    "    DryRun=False,\n",
    "    KeyFormat='pem'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/rambino/.aws/spark_keypair.pem',\"w\") as file:\n",
    "    file.writelines(response['KeyMaterial'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Setting up VPC for the EMR cluster\n",
    "\n",
    "---\n",
    "\n",
    "If no VPC is specified for an EMR cluster, then the cluster is launched in the normal AWS cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating default VPC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws ec2 create-default-vpc --profile default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting **first** subnetId for this VPC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vpc_output = ec2.describe_vpcs()\n",
    "\n",
    "#Getting first (and only) VPC:\n",
    "vpcId = vpc_output['Vpcs'][0]['VpcId']\n",
    "\n",
    "subnet_output = ec2.describe_subnets(\n",
    "    Filters=[\n",
    "        {\n",
    "            'Name':'vpc-id',\n",
    "            'Values':[vpcId]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "subnetId = subnet_output['Subnets'][0]['SubnetId']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Creating EMR Cluster\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emr = boto3.client('emr',\n",
    "    region_name             = \"us-east-1\",\n",
    "    aws_access_key_id       = aws_cred['default']['aws_access_key_id'],\n",
    "    aws_secret_access_key   = aws_cred['default']['aws_secret_access_key']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With boto3\n",
    "emr.run_job_flow(\n",
    "            Name='spark-cluster',\n",
    "            LogUri='s3://emrlogs-rambino/',\n",
    "            ReleaseLabel='emr-5.36.0',\n",
    "            Instances={\n",
    "                'MasterInstanceType': 'm5.xlarge',\n",
    "                'SlaveInstanceType': 'm5.xlarge',\n",
    "                'InstanceCount': 3,\n",
    "                'Ec2KeyName':'spark_ec2_key',\n",
    "                'KeepJobFlowAliveWhenNoSteps': True,\n",
    "                'Ec2SubnetId': subnetId\n",
    "            },\n",
    "            Applications=[\n",
    "                {\n",
    "                    \"Name\":\"Spark\"\n",
    "                },\n",
    "                {\n",
    "                    \"Name\":\"Zeppelin\"\n",
    "                },\n",
    "                {\n",
    "                    \"Name\":\"Hadoop\"\n",
    "                },\n",
    "                {\n",
    "                    \"Name\":\"Ganglia\"\n",
    "                },\n",
    "                {\n",
    "                    \"Name\":\"Livy\"\n",
    "                },\n",
    "                {\n",
    "                    \"Name\":\"JupyterEnterpriseGateway\"\n",
    "                }\n",
    "            ],\n",
    "            Configurations=[\n",
    "                {\n",
    "                    #Note: setting timeout of 'livy' to be longer to try to fix 'session not active' errors\n",
    "                    'Classification': 'livy-conf',\n",
    "                    'Properties': {'livy.server.session.timeout':'3h'}\n",
    "                }\n",
    "            ],\n",
    "            JobFlowRole='EMR_EC2_DefaultRole',\n",
    "            ServiceRole='EMR_DefaultRole',\n",
    "            VisibleToAllUsers=True,\n",
    "            AutoTerminationPolicy={\n",
    "                'IdleTimeout': 1800\n",
    "            }\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Configuring Cluster\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_list = emr.list_clusters(\n",
    "    ClusterStates=['STARTING','RUNNING','WAITING']\n",
    ")\n",
    "print(cluster_list)\n",
    "cluster_id = cluster_list['Clusters'][0]['Id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cluster = emr.describe_cluster(\n",
    "    ClusterId = cluster_id\n",
    ")\n",
    "print(new_cluster)\n",
    "secGroup_master = new_cluster['Cluster']['Ec2InstanceAttributes']['EmrManagedMasterSecurityGroup']\n",
    "iam_service_role = new_cluster['Cluster']['Ec2InstanceAttributes']['IamInstanceProfile']\n",
    "cluster_dns = new_cluster['Cluster']['MasterPublicDnsName']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure Cluster Security Groups to only accept SSH ingress from my IP address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting my public IP address from config.me website (IP is last element of returned array)\n",
    "myIP = !curl ifconfig.me\n",
    "myIP = myIP[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specifying internal port (arbitrary?)\n",
    "myPort = '32'\n",
    "myCidrIp = myIP + \"/\" + myPort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ec2.authorize_security_group_ingress(\n",
    "    GroupId=secGroup_master,\n",
    "    IpPermissions=[\n",
    "        {\n",
    "            'FromPort': 22,\n",
    "            'IpProtocol': 'tcp',\n",
    "            'IpRanges': [\n",
    "                {\n",
    "                    'CidrIp': myCidrIp,\n",
    "                    'Description': 'SSH access to Spark EMR on AWS from Kevins Computer',\n",
    "                },\n",
    "            ],\n",
    "            'ToPort': 22,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Running notebook EMR cluster\n",
    "\n",
    "---\n",
    "\n",
    "Note: I uploaded notebook to EMR manually, but this could also be achieved via S3 upload with Boto3\n",
    "Similarly, the service role used below was automatically created with this cluster, but I could make my own custom role with IAM and boto3 if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'s3://aws-emr-resources-549653882425-us-east-1/notebooks/e-B41LV1OZ58I8ZG299XTVEG6Y0/emr_spark_code.ipynb',\n",
    "\n",
    "#Starting EMR notebook\n",
    "response = emr.start_notebook_execution(\n",
    "    EditorId='e-B41LV1OZ58I8ZG299XTVEG6Y0',\n",
    "    RelativePath='emr_spark_code.ipynb',\n",
    "    ExecutionEngine={\n",
    "        'Id': cluster_id,\n",
    "        'Type': 'EMR'\n",
    "    },\n",
    "    ServiceRole=\"EMR_Notebooks_DefaultRole\"#iam_service_role\n",
    ")\n",
    "\n",
    "exec_id = response['NotebookExecutionId']\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking execution status:\n",
    "response = emr.describe_notebook_execution(\n",
    "    NotebookExecutionId=exec_id\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Interacting with Cluster\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uploading Python Notebook to S3 for EMR usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to Cluster via SSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#File path where cluster login information is kept on my machine:\n",
    "pem_path = '/home/rambino/.aws/spark_keypair.pem'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Command to use in terminal (interactive):\n",
    "print(f\"ssh hadoop@{cluster_dns} -i {pem_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Proxy connection to allow interaction with Spark UI\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy credentials file to the master node\n",
    "print(f\"scp -i {pem_path} {pem_path} hadoop@{cluster_dns}:/home/hadoop/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up port forwarding (somehow) so that data from our local machine on port 8157 is forwarded to the master node (allowing interactivity)\n",
    "#NOTE: Terminal remains open when this request succeeds - and needs to remain running while accessing Spark UI\n",
    "\n",
    "print(f\"ssh -v -i {pem_path} -N -D 127.0.0.1:8157 hadoop@{cluster_dns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Deleting EMR Cluster (Teardown)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emr.terminate_job_flows(\n",
    "    JobFlowIds=[\n",
    "        cluster_id\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_path_prefix = \"./_out/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Users\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = spark.read \\\n",
    "    .format('csv') \\\n",
    "    .option('header',True) \\\n",
    "    .load(read_path_prefix + \"users\")\n",
    "\n",
    "users.createOrReplaceTempView('users_tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do we have any duplicate userIds?\n",
    "spark.sql('''\n",
    "SELECT userId, COUNT(userId) count\n",
    "FROM users_tbl\n",
    "GROUP BY userId\n",
    "ORDER BY count DESC\n",
    "LIMIT 5\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Songs\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = spark.read \\\n",
    "    .format('csv') \\\n",
    "    .option('header',True) \\\n",
    "    .load(read_path_prefix + \"songs\")\n",
    "\n",
    "songs.createOrReplaceTempView('songs_tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do we have any duplicate song Ids?\n",
    "spark.sql('''\n",
    "SELECT song_id, COUNT(song_id) count\n",
    "FROM songs_tbl\n",
    "GROUP BY song_id\n",
    "ORDER BY count DESC\n",
    "LIMIT 5\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Artists\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists = spark.read \\\n",
    "    .format('csv') \\\n",
    "    .option('header',True) \\\n",
    "    .load(read_path_prefix + \"artists\")\n",
    "\n",
    "artists.createOrReplaceTempView('artists_tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do we have any duplicate artist Ids?\n",
    "spark.sql('''\n",
    "SELECT artist_id, COUNT(artist_id) count\n",
    "FROM artists_tbl\n",
    "GROUP BY artist_id\n",
    "ORDER BY count DESC\n",
    "LIMIT 5\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplays = spark.read \\\n",
    "    .format('csv') \\\n",
    "    .schema(songplays_schema) \\\n",
    "    .option('header',True) \\\n",
    "    .load('./_out/songplays')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example analytics: get locations where songs were played on Nov. 11, 2018\n",
    "\n",
    "spark.sql('''\n",
    "SELECT count(*) AS freq, location\n",
    "from play_tbl\n",
    "WHERE song_id IS NOT NULL\n",
    "AND (ts/1000) > 1543532400\n",
    "GROUP BY location\n",
    "ORDER BY freq DESC\n",
    "''').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
