{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I. ETL Pipeline for Pre-Processing the Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLEASE RUN THE FOLLOWING CODE FOR PRE-PROCESSING THE FILES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Python packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python packages \n",
    "import pandas as pd\n",
    "import cassandra\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import json\n",
    "import csv\n",
    "import sys\n",
    "import ftfy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating list of filepaths to process original event csv data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rambino/dev/DataEngineering_Udacity/Projects/DataModelingWithCassandra\n"
     ]
    }
   ],
   "source": [
    "# checking your current working directory\n",
    "print(os.getcwd())\n",
    "\n",
    "# Get your current folder and subfolder event data\n",
    "filepath = os.getcwd() + '/event_data'\n",
    "\n",
    "# Create a for loop to create a list of files and collect each filepath\n",
    "for root, dirs, files in os.walk(filepath):\n",
    "    \n",
    "# join the file path and roots with the subdirectories using glob\n",
    "    file_path_list = glob.glob(os.path.join(root,'*'))\n",
    "    #print(file_path_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing the files to create the data file csv that will be used for Apache Casssandra tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "\n",
    "The code block below was provided by Udacity.\n",
    "It looks like the CSV files are being read-in and then re-parsed into a new file which (a) clears out missing data and (b) only loads in necessary columns.\n",
    "\n",
    "**Note:** If I were to re-write this, I *really* don't like the use of numerical indexes to write the new data file. I don't see why we don't take advantage of Pandas to read the data into a DataFrame, and then use `itertuples` to iterate over it and write the new file.\n",
    "\n",
    "**To Do:** Come back to this script after completing rest of code & try to transform this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'artist': 0,\n",
       " 'auth': 1,\n",
       " 'firstName': 2,\n",
       " 'gender': 3,\n",
       " 'itemInSession': 4,\n",
       " 'lastName': 5,\n",
       " 'length': 6,\n",
       " 'level': 7,\n",
       " 'location': 8,\n",
       " 'method': 9,\n",
       " 'page': 10,\n",
       " 'registration': 11,\n",
       " 'sessionId': 12,\n",
       " 'song': 13,\n",
       " 'status': 14,\n",
       " 'ts': 15,\n",
       " 'userId': 16}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(file_path_list[0],'r', encoding='utf8',newline='') as file:\n",
    "    csvreader = csv.reader(file)\n",
    "    list = next(csvreader)\n",
    "    column_dict = {key: val for val, key in enumerate(list)}\n",
    "column_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "NEW VERSION\n",
    "Note: This version reads ALL the data into memory and THEN writes it to a new file.\n",
    "This is probably not the best way to do this. What if we have a file with MILLIONS of lines instead of thousands?\n",
    "Let's instead write each line immediately to the new file\n",
    "'''\n",
    "\n",
    "desired_cols = ['artist','firstName','gender','itemInSession','lastName','length',\\\n",
    "                'level','location','sessionId','song','userId']\n",
    "header = None\n",
    "\n",
    "csv.register_dialect('myDialect', quoting=csv.QUOTE_ALL, skipinitialspace=True)\n",
    "\n",
    "# for every filepath in the file path list \n",
    "with open('event_datafile_new.csv', 'w', encoding = 'utf8', newline='') as output_file:\n",
    "    writer = csv.writer(output_file, dialect='myDialect')\n",
    "    writer.writerow(desired_cols)\n",
    "    \n",
    "    for path in file_path_list:\n",
    "        # reading input csv file \n",
    "        with open(path, 'r', encoding = 'utf8', newline='') as input_file: \n",
    "            # creating a csv reader object \n",
    "            csvreader = csv.reader(input_file)\n",
    "            input_header = next(csvreader)\n",
    "\n",
    "            #Checking columns of all files ordered the same:\n",
    "            if header != None:\n",
    "                if header != input_header:\n",
    "                    raise Exception(f\"File {path} has differently-ordered columns\")\n",
    "            else:\n",
    "                header = input_header\n",
    "                #Object for ensuring we order columns consistently:\n",
    "                h_indices = {key: val for val, key in enumerate(header)}\n",
    "\n",
    "            # extracting each data row one by one andlist append it        \n",
    "            for line in csvreader:\n",
    "                if line[0] == '':\n",
    "                    continue\n",
    "                #Fixing corrupted characters\n",
    "                line = [ftfy.fix_text(item) for item in line]\n",
    "                #Writing columns in order specified by desired_cols\n",
    "                writer.writerow([line[h_indices[key]] for key in desired_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    776.0\n",
       "mean       1.0\n",
       "std        0.0\n",
       "min        1.0\n",
       "25%        1.0\n",
       "50%        1.0\n",
       "75%        1.0\n",
       "max        1.0\n",
       "Name: userId, dtype: float64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the number of rows in your csv file\n",
    "#df = pd.read_csv('event_datafile_new.csv')\n",
    "\n",
    "#df.groupby('sessionId').agg({'userId':['nunique']})\n",
    "df.groupby('sessionId')['userId'].nunique().describe()\n",
    "\n",
    "# with open('event_datafile_new.csv', 'r', encoding = 'utf8') as f:\n",
    "#     print(sum(1 for line in f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II. Complete the Apache Cassandra coding portion of your project. \n",
    "\n",
    "## Now you are ready to work with the CSV file titled <font color=red>event_datafile_new.csv</font>, located within the Workspace directory.  The event_datafile_new.csv contains the following columns: \n",
    "- artist \n",
    "- firstName of user\n",
    "- gender of user\n",
    "- item number in session\n",
    "- last name of user\n",
    "- length of the song\n",
    "- level (paid or free song)\n",
    "- location of the user\n",
    "- sessionId\n",
    "- song title\n",
    "- userId\n",
    "\n",
    "The image below is a screenshot of what the denormalized data should appear like in the <font color=red>**event_datafile_new.csv**</font> after the code above is run:<br>\n",
    "\n",
    "<img src=\"images/image_event_datafile_new.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin writing your Apache Cassandra code in the cells below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should make a connection to a Cassandra instance your local machine \n",
    "# (127.0.0.1)\n",
    "contacts = ['127.0.0.1']\n",
    "port = 9042\n",
    "\n",
    "from cassandra.cluster import Cluster\n",
    "cluster = Cluster(contact_points = contacts, port = port)\n",
    "\n",
    "# To establish connection and begin executing queries, need a session\n",
    "session = cluster.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Keyspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO: Create a Keyspace\n",
    "try:\n",
    "    session.execute('''\n",
    "    CREATE KEYSPACE IF NOT EXISTS music_sparkify_cassandra\n",
    "    WITH REPLICATION =\n",
    "    {'class':'SimpleStrategy','replication_factor':'1'}\n",
    "    ''')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Keyspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO: Set KEYSPACE to the keyspace specified above\n",
    "session.set_keyspace('music_sparkify_cassandra')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we need to create tables to run the following queries. Remember, with Apache Cassandra you model the database tables on the queries you want to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create queries to ask the following three questions of the data\n",
    "\n",
    "#### 1. Give me the artist, song title and song's length in the music app history that was heard during  sessionId = 338, and itemInSession  = 4\n",
    "- So filtering on `sessionId` and `itemsInSession`\n",
    "- Note: Using `df.groupby('sessionId')['userId'].nunique().describe()`  with pandas it seems that each sessionId is unique - even across users - so there is no need to use both `userId` and `sessionId` as composite primary keys- because `sessionId` already can only be 1 user\n",
    "- The table below uses `sessionId` as the partitioning key so that each partition represents a unique session. Within those sessions, items are ordered by `itemsInSession` which shows the order in which songs were played (I think).\n",
    "\n",
    "```\n",
    "CREATE TABLE IF NOT EXISTS songs_by_session\n",
    "(sessionId int,\n",
    "itemInSession int,\n",
    "artist text,\n",
    "song text,\n",
    "length float,\n",
    "PRIMARY KEY (sessionId, itemsInSession))\n",
    "\n",
    "```\n",
    "\n",
    "#### 2. Give me only the following: name of artist, song (sorted by itemInSession) and user (first and last name) for userid = 10, sessionid = 182\n",
    "- So filtering on both `userId` AND `sessionId`. This to me doesn't make any sense. As I wrote above, `sessionId` is already uniquely-identifying users, so there is no reason to also filter by `userId` that I can see.\n",
    "- The table below uses `userId` and `sessionId` as a composite partitioning key so that each partition again represents a unique session, each of which is again sorted by the clustering key `itemInSession` (since we need to sort based on this value).\n",
    "- Since `userId` is probably not needed, it would be a good idea to probably combine this table with the one above, but since the query above might *not* specify `userId`, we can't necessarily include it as a partitioning key for the purpose of this project\n",
    "\n",
    "```\n",
    "CREATE TABLE IF NOT EXISTS songsUser_by_sessionUser\n",
    "(sessionId int,\n",
    "userId int,\n",
    "itemInSession int,\n",
    "artist text,\n",
    "song text,\n",
    "firstName text,\n",
    "lastName text\n",
    "PRIMARY KEY ((sessionId, userId), itemInSession))\n",
    "```\n",
    "\n",
    "#### 3. Give me every user name (first and last) in my music app history who listened to the song 'All Hands Against His Own'\n",
    "- So we need data organized by individual songs. It might be kind of inefficient to partition by each song, since that would mean millions of partitions at scale, and each partition could grow in an unbounded way (no limit to # of song plays).\n",
    "- The basics of what I need are `firstName`, `lastName` and probably `userId` (since names could repeat). Then of course `song`. The partition key would then be `song` and the clustering key `userId` (or jointly `firstName` and `lastName`). This model would NOT record unique listens (which would be overwritten), but that seems to be o.k. based on the query.\n",
    "- **NOTE:** The analyst using this query probably is talking about a specific song (from a particular artist) - partitioning by only `song` would group song data *as long as the song name is the same* - which is quite misleading. Therefore, we probably need to **ENFORCE** using artist in this query - so I'm including it as a partition key\n",
    "\n",
    "```\n",
    "CREATE TABLE IF NOT EXISTS users_by_song\n",
    "(userId int,\n",
    "firstName text,\n",
    "lastName text,\n",
    "song text,\n",
    "artist text,\n",
    "PRIMARY KEY ((song, artist), userId))\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Note: Re-designed this code so that it's more reusable for other insert queries / tables.\n",
    "Includes re-typing values in data stream according to attributes_dict values\n",
    "'''\n",
    "\n",
    "def data_insert(table,attributes_dict):\n",
    "    '''reads in file below and inserts into user-given Cassandra table with user-specified attributes'''\n",
    "    file = 'event_datafile_new.csv'\n",
    "\n",
    "    query = make_insert_query_string(table,attributes_dict)\n",
    "\n",
    "    with open(file, encoding = 'utf8') as f:\n",
    "        csvreader = csv.reader(f)\n",
    "        columns = next(csvreader) # record & skip col names\n",
    "        for line in csvreader:\n",
    "            values = get_insert_values(columns, attributes_dict,line)\n",
    "            session.execute(query, tuple(values))\n",
    "\n",
    "def make_insert_query_string(table, attributes):\n",
    "    '''Creates basic insert query string for Cassandra given table name and list of values'''\n",
    "    attributes_str = \", \".join(attributes)\n",
    "    values_str = \", \".join([\"%s\"]*len(attributes))\n",
    "    query = f\"INSERT INTO {table} ({attributes_str}) VALUES ({values_str})\"\n",
    "    return query\n",
    "\n",
    "def get_insert_values(columns, attribute_dict, data):\n",
    "    '''\n",
    "    Casts data values as type defined by 'attribute_dict'.\n",
    "    Uses 'columns' to ensure that the correct index of 'data' is being used.\n",
    "    Requires that all values in attribute_dict.keys() match an element in 'columns'\n",
    "    '''\n",
    "    return [func(data[columns.index(attr)]) for attr,func in attribute_dict.items()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x7f4cd2b77f40>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TO-DO: Query 1:  Give me the artist, song title and song's length in the music app history that was heard during \\\n",
    "## sessionId = 338, and itemInSession = 4\n",
    "\n",
    "session.execute(\n",
    "    '''\n",
    "    CREATE TABLE IF NOT EXISTS songs_by_session\n",
    "    (sessionId int,\n",
    "    itemInSession int,\n",
    "    artist text,\n",
    "    song text,\n",
    "    length float,\n",
    "    PRIMARY KEY (sessionId, itemInSession))\n",
    "    '''\n",
    ")\n",
    "\n",
    "#session.execute(\"DROP TABLE songs_by_session\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query 1: Inserting Data\n",
    "attributes_dict = {\n",
    "    'sessionId':int,\n",
    "    'itemInSession':int,\n",
    "    'artist':str,\n",
    "    'song':str,\n",
    "    'length':float\n",
    "}\n",
    "table = 'songs_by_session'\n",
    "\n",
    "data_insert(table,attributes_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do a SELECT to verify that the data have been inserted into each table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(artist='Faithless', song='Music Matters (Mark Knight Dub)', length=495.30731201171875)\n"
     ]
    }
   ],
   "source": [
    "#Query 1: Give me the artist, song title and song's length in the music app history that was heard during \\\n",
    "## sessionId = 338, and itemInSession = 4\n",
    "result = session.execute(\n",
    "    '''SELECT artist,song,length FROM songs_by_session WHERE sessionId = 338 AND itemInSession = 4'''\n",
    ")\n",
    "for row in result:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COPY AND REPEAT THE ABOVE THREE CELLS FOR EACH OF THE THREE QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x7f4cd2f2f340>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TO-DO: Query 2: Give me only the following: name of artist, song (sorted by itemInSession) and user (first and last name)\\\n",
    "## for userid = 10, sessionid = 182\n",
    "session.execute(\n",
    "    '''\n",
    "    CREATE TABLE IF NOT EXISTS songsUser_by_sessionUser\n",
    "    (sessionId int,\n",
    "    userId int,\n",
    "    itemInSession int,\n",
    "    artist text,\n",
    "    song text,\n",
    "    firstName text,\n",
    "    lastName text,\n",
    "    PRIMARY KEY ((sessionId, userId), itemInSession))\n",
    "    '''\n",
    ")         \n",
    "\n",
    "#session.execute(\"DROP TABLE songsUser_by_sessionUser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query 2:\n",
    "attributes_dict = {\n",
    "    'sessionId':int,\n",
    "    'userId':int,\n",
    "    'itemInSession':int,\n",
    "    'artist':str,\n",
    "    'song':str,\n",
    "    'firstName':str,\n",
    "    'lastName':str\n",
    "}\n",
    "table = 'songsUser_by_sessionUser'\n",
    "\n",
    "data_insert(table,attributes_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(artist='Down To The Bone', song=\"Keep On Keepin' On\", firstname='Sylvie', lastname='Cruz')\n",
      "Row(artist='Three Drives', song='Greece 2000', firstname='Sylvie', lastname='Cruz')\n",
      "Row(artist='Sebastien Tellier', song='Kilometer', firstname='Sylvie', lastname='Cruz')\n",
      "Row(artist='Lonnie Gordon', song='Catch You Baby (Steve Pitron & Max Sanna Radio Edit)', firstname='Sylvie', lastname='Cruz')\n"
     ]
    }
   ],
   "source": [
    "#Query 2: Give me only the following: name of artist, song (sorted by itemInSession) and user (first and last name)\\\n",
    "## for userid = 10, sessionid = 182\n",
    "result = session.execute(\n",
    "    '''SELECT artist,song,firstName,lastName FROM songsUser_by_sessionUser WHERE userId = 10 AND sessionId = 182 ORDER BY itemInSession'''\n",
    ")\n",
    "for row in result:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x7f4c9c239f30>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TO-DO: Query 3: Give me every user name (first and last) in my music app history who listened to the song 'All Hands Against His Own'\n",
    "session.execute(\n",
    "    '''\n",
    "    CREATE TABLE IF NOT EXISTS users_by_song\n",
    "    (userId int,\n",
    "    firstName text,\n",
    "    lastName text,\n",
    "    song text,\n",
    "    artist text,\n",
    "    PRIMARY KEY ((song, artist), userId))\n",
    "    '''\n",
    ")         \n",
    "\n",
    "#session.execute(\"DROP TABLE users_by_song\")\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query 3:\n",
    "attributes_dict = {\n",
    "    'userId':int,\n",
    "    'firstName':str,\n",
    "    'lastName':str,\n",
    "    'artist':str,\n",
    "    'song':str\n",
    "}\n",
    "\n",
    "table = 'users_by_song'\n",
    "\n",
    "data_insert(table,attributes_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(firstname='Jacqueline', lastname='Lynch')\n",
      "Row(firstname='Tegan', lastname='Levine')\n",
      "Row(firstname='Sara', lastname='Johnson')\n"
     ]
    }
   ],
   "source": [
    "#Query 3: Give me every user name (first and last) in my music app history who listened to the song 'All Hands Against His Own'\n",
    "result = session.execute(\n",
    "    \"SELECT firstName,lastName FROM users_by_song WHERE song = 'All Hands Against His Own' AND artist = 'The Black Keys'\"\n",
    ")\n",
    "\n",
    "for row in result:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop the tables before closing out the sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x7ff64c39c580>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TO-DO: Drop the table before closing out the sessions\n",
    "\n",
    "session.execute(\"DROP TABLE songs_by_session\")\n",
    "session.execute(\"DROP TABLE songsUser_by_sessionUser\")\n",
    "session.execute(\"DROP TABLE users_by_song\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close the session and cluster connection¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.shutdown()\n",
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changes needed:\n",
    "'''\n",
    "DONE 1. Implement my change to data file manipulation, so that we work with tuples + pandas\n",
    "DONE 2. Change parsing of file to be unicode rather than UTF-8 (some foreign characters mis-interpreted)\n",
    "    e.g., \"La Boulette (GÃÂ©nÃÂ©ration Nan Nan)\" instead of \"La Boulette (Génération Nan Nan)\"\n",
    "    Solved with ftfy package for fixing corrupted characters\n",
    "3. Re-write the rationale of my solutions and put in markdown file\n",
    "'''\n",
    "\n",
    "#Other Changes I made:\n",
    "'''\n",
    "1. using artist as another partitioning key for 3rd table\n",
    "2. re-creating data insert functions to be re-usable\n",
    "3. sessionId being unique (table 2 does not need to use userId as a partitioning key)\n",
    "4. use of Docker image to host and expose Cassandra (specific port being used)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
